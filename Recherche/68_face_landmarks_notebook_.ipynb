{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75ef5501",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D,BatchNormalization,Dense,Flatten,MaxPooling2D,Activation,Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import cv2\n",
    "import math\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from skopt import gp_minimize\n",
    "from skopt.utils import use_named_args\n",
    "from tensorflow.keras.backend import clear_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97b49e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_shape=136\n",
    "input_shape=(172,172,1,)\n",
    "split=0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ffa225c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    datasetPath=r\"D:\\Desktop\\cours\\3I\\DetectionVisage\\68_face_landmarks/dataset\"\n",
    "    X_train=[]\n",
    "    X_test=[]\n",
    "    y_train=[]\n",
    "    y_test=[]\n",
    "    files=[]\n",
    "    for root,dirs,allfiles in os.walk(datasetPath):\n",
    "        for file in allfiles:\n",
    "            if 'jpg' in file or 'png' in file or \"txt\" in file:\n",
    "                files.append(root+'/'+file)\n",
    "    files.sort()\n",
    "    for file in files:\n",
    "        landmarks=[]\n",
    "        if \"trainset\" in file:\n",
    "            if '.jpg' in file or \".png\" in file:\n",
    "                X_train+=[getImage(file)]\n",
    "            elif '.txt' in file:\n",
    "                with open(file,'r',encoding='utf8') as f:\n",
    "                    lines=f.readlines()\n",
    "                for line in lines:\n",
    "                    if \"n_points\" not in line and \"version\" not in line and \"{\" not in line and \"}\" not in line:\n",
    "                        tmp=line.split(\" \")\n",
    "                        landmarks+=[float(tmp[0]),float(tmp[1])]\n",
    "                y_train.append(landmarks)\n",
    "        else:\n",
    "            if '.jpg' in file or \".png\" in file:\n",
    "                X_test+=[getImage(file)]\n",
    "            elif '.txt' in file:\n",
    "                with open(file,'r',encoding=\"utf8\") as f:\n",
    "                    lines=f.readlines()\n",
    "                for line in lines:\n",
    "                    if \"n_points\" not in line and \"version\" not in line and \"{\" not in line and \"}\" not in line:\n",
    "                        tmp=line.split(\" \")\n",
    "                        landmarks+=[float(tmp[0]),float(tmp[1])]\n",
    "                y_test.append(landmarks)\n",
    "    print(len(X_train))\n",
    "    print(len(y_train))\n",
    "    print(len(X_test))\n",
    "    print(len(y_test))\n",
    "    X_train=np.array(X_train)\n",
    "    X_test=np.array(X_test)\n",
    "    y_test=np.array(y_test)\n",
    "    y_train=np.array(y_train)\n",
    "    y_train = np.reshape( y_train , ( -1 , 1 , 1 , output_shape ))\n",
    "    y_test = np.reshape( y_test , ( -1 , 1 , 1 , output_shape ))\n",
    "    return X_train,y_train,X_test,y_test\n",
    "def load_dataset_json(split=0.8, output_shape=1):\n",
    "    offset=172/512\n",
    "    datasetPath = os.path.join(\"D:\", \"\\\\Desktop\", \"cours\", \"3I\", \"Dataset\")\n",
    "    X = []\n",
    "    y = []\n",
    "    with open(os.path.join(datasetPath, \"all_data_augmented.json\")) as file:\n",
    "        data = json.load(file)\n",
    "    for key in data:\n",
    "        item = data[key]\n",
    "        file_path = os.path.join(datasetPath, \"images\", item[\"file_name\"])\n",
    "        X.append(getImage(file_path))\n",
    "        array=np.array(item[\"face_landmarks\"])\n",
    "        y.append(list(array.flat))\n",
    "    X = np.array(X) / 255\n",
    "    y = np.array(y)*offset\n",
    "    X= np.expand_dims(X, axis=-1)\n",
    "    y=np.expand_dims(y,axis=(1,2))\n",
    "    return  train_test_split(X, y, train_size=split)\n",
    "\n",
    "def getImage(path):\n",
    "    img=cv2.imread(path,cv2.IMREAD_GRAYSCALE)\n",
    "    img=cv2.resize(img,(172,172))\n",
    "    return img\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ff48257",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=load_dataset_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3b417e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 172, 172, 1)\n",
      "(8000, 1, 1, 136)\n",
      "(2000, 172, 172, 1)\n",
      "(2000, 1, 1, 136)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b08a104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 12384x12384 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=( 172 , 172 ))\n",
    "for i in range( 1 , 6 ):\n",
    "    sample_image = X_test[i]\n",
    "    landmarks=y_test[i][0][0]\n",
    "    fig.add_subplot( 1 , 10 , i )\n",
    "    plt.imshow( sample_image , cmap='gray' )\n",
    "    for j in range(0,136,2):\n",
    "        plt.scatter( landmarks[j],landmarks[j+1], c='yellow' )\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0530dc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "Callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    '../model_best_loss.h5', monitor='val_loss', verbose=0, save_best_only=True,\n",
    "    save_weights_only=False, mode='auto', save_freq='epoch'\n",
    ")\n",
    "early_stop=tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0,\n",
    "    patience=5,\n",
    "    verbose=0,\n",
    "    mode='auto',\n",
    "    baseline=None,\n",
    "    restore_best_weights=False\n",
    ")\n",
    "def getCoeff(x):\n",
    "    return int(34.667*pow(x,4) - 432*pow(x,3) + 1853.3*pow(x,2) - 3184*x + 1984)\n",
    "\n",
    "def getNeurons(x,max_neurons):\n",
    "    return int((max_neurons*pow(math.sin(x*0.25),2)+max_neurons*pow(math.cos(x*0.75),2))/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c165ec6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_layers = [\n",
    "                \n",
    "    Conv2D( 256 , input_shape=input_shape , kernel_size=( 3 , 3 ) ,  activation='relu' ),\n",
    "    Conv2D( 256 , kernel_size=( 3 , 3 ) , activation='relu' ),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Conv2D( 128 , kernel_size=( 3 , 3 ) ,  activation='relu' ),\n",
    "    Conv2D( 128 , kernel_size=( 3 , 3 ) ,  activation='relu' ),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Conv2D( 256 , kernel_size=( 3 , 3 ) , activation='relu' ),\n",
    "    Conv2D( 256 , kernel_size=( 3 , 3 ) , activation='relu' ),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Conv2D( 128 , kernel_size=( 3 , 3 ) , activation='relu' ),\n",
    "    Conv2D( 128 , kernel_size=( 3 , 3 ) , activation='relu' ),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Conv2D( 64 , kernel_size=( 3 , 3 ) , activation='relu' ),\n",
    "    Conv2D( 64 , kernel_size=( 3 , 3 ) , activation='relu' ),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Conv2D( output_shape, kernel_size=( 3 , 3 ) , activation='relu' ),\n",
    "    Conv2D( output_shape , kernel_size=( 3 , 3 ) , activation='relu' ),\n",
    "    Conv2D( output_shape , kernel_size=( 3 , 3 ) ),\n",
    "\n",
    "]\n",
    "model = tf.keras.Sequential( model_layers )\n",
    "model.compile( loss=tf.keras.losses.mean_squared_error,optimizer='Adam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6a4fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(X_train,y_train,batch_size=16,epochs=80,validation_data=(X_test,y_test),callbacks=[Callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea7833f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(layer_number=1,max_neurons=128,kernel_size=1,dropout=0.1):\n",
    "    kernel_size=(kernel_size,kernel_size)\n",
    "    model=Sequential()\n",
    "    model.add(BatchNormalization(input_shape=input_shape))\n",
    "    for i in range(layer_number):\n",
    "        try:\n",
    "            model.add(Conv2D(getNeurons(i,max_neurons),kernel_size,activation='relu'))\n",
    "            model.add(Conv2D(getNeurons(i,max_neurons),kernel_size,activation='relu'))\n",
    "            model.add(BatchNormalization())\n",
    "            model.add(Dropout(dropout))\n",
    "        except:\n",
    "            pass\n",
    "    model.add(Conv2D( output_shape, kernel_size=kernel_size , activation='relu' ))\n",
    "    model.add(Conv2D( output_shape , kernel_size=kernel_size , activation='relu' ))\n",
    "    model.add(Conv2D( output_shape , kernel_size=kernel_size))\n",
    "    model.compile(loss=\"mse\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14f11bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# best_loss=tf.keras.models.load_model('./best_model_loss.h5').evaluate(X_test,y_test)\n",
    "# tf.keras.models.load_model('./best_model_loss.h5').summary()\n",
    "dim_num_conv_layers = Integer(low=1, high=7, name='layer_number')\n",
    "dim_num_kernel_size=Integer(low=1,high=3,name=\"kernel_size\")\n",
    "dim_num_conv_nodes = Integer(low=64, high=256, name='max_neurons')\n",
    "dim_num_dropout = Real(low=0.1, high=0.5, name='dropout')\n",
    "\n",
    "dimensions = [dim_num_conv_layers,\n",
    "              dim_num_conv_nodes,\n",
    "              dim_num_kernel_size,\n",
    "              dim_num_dropout,\n",
    "             ]\n",
    "default_parameters = [1, 64, 1, 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e8a27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class change_best_loss_callback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        global best_loss\n",
    "        if logs!=None:\n",
    "            if best_loss>logs['val_loss']:\n",
    "                best_loss=logs['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0267e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss=-1\n",
    "@use_named_args(dimensions=dimensions)\n",
    "def fitness(layer_number=1,max_neurons=128,kernel_size=(1,1),dropout=0.1):\n",
    "\n",
    "    # Print the hyper-parameters.\n",
    "    print('layer_number: ',layer_number)\n",
    "    print('max_neurons: ', max_neurons)\n",
    "    print('kernel_size: ', kernel_size)\n",
    "    print('dropout: ', dropout)\n",
    "    print()\n",
    "    \n",
    "    # Create the neural network with these hyper-parameters.\n",
    "    model = create_model(layer_number=layer_number,\n",
    "                         max_neurons=max_neurons,\n",
    "                         kernel_size=kernel_size,\n",
    "                         dropout=dropout\n",
    "                        )\n",
    "    # Use Keras to train the model.\n",
    "    history = model.fit(x= X_train,\n",
    "                        y= y_train,\n",
    "                        epochs=50,\n",
    "                        batch_size=16,\n",
    "                        validation_data=(X_test,y_test),\n",
    "                        callbacks=[Callback,early_stop]\n",
    "                       )\n",
    "\n",
    "    # Get the classification accuracy on the validation-set\n",
    "    # after the last training-epoch.\n",
    "    loss = history.history['val_loss'][-1]\n",
    "    # Print the classification accuracy.\n",
    "    print()\n",
    "    print(\"MSE: {0:.2%}\".format(loss))\n",
    "    print()\n",
    "\n",
    "    # Save the model if it improves on the best-found performance.\n",
    "    # We use the global keyword so we update the variable outside\n",
    "    # of this function.\n",
    "    global best_loss\n",
    "\n",
    "    # If the classification accuracy of the saved model is improved ...\n",
    "    if loss < best_loss or best_loss==-1:\n",
    "        # Save the new model to harddisk.\n",
    "        model.save('./best_model_loss.h5')\n",
    "        print(\"model saved\")\n",
    "        # Update the classification accuracy.\n",
    "        best_loss = loss\n",
    "    # Delete the Keras model with these hyper-parameters from memory.\n",
    "    del model\n",
    "    \n",
    "    # Clear the Keras session, otherwise it will keep adding new\n",
    "    # models to the same TensorFlow graph each time we create\n",
    "    # a model with a different set of hyper-parameters.\n",
    "    clear_session()\n",
    "    \n",
    "    # NOTE: Scikit-optimize does minimization so it tries to\n",
    "    # find a set of hyper-parameters with the LOWEST fitness-value.\n",
    "    # Because we are interested in the HIGHEST classification\n",
    "    # accuracy, we need to negate this number so it can be minimized.\n",
    "    return loss\n",
    "# This function exactly comes from :Hvass-Labs, TensorFlow-Tutorials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d8c9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitness(default_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f932432c",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_result = gp_minimize(func=fitness,\n",
    "                            dimensions=dimensions,\n",
    "                            acq_func='EI', # Expected Improvement.\n",
    "                            n_calls=50,\n",
    "                            x0=default_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f1cbbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_2(neurons_1=256,has_layer_1=True, has_2_conv_layer_1=True,neurons_2=16,has_layer_2=True, has_2_conv_layer_2=True,neurons_3=256,has_layer_3=True, has_2_conv_layer_3=True,neurons_4=64,has_layer_4=True, has_2_conv_layer_4=True,neurons_5=16,has_layer_5=True, has_2_conv_layer_5=True,drop_1=0.5,drop_2=0.4,drop_3=0.3,drop_4=0.2,drop_5=0.1,dense_1=0.7*output_shape,dense_2=0.5*output_shape):\n",
    "    model_2=Sequential()\n",
    "    model_2.add(BatchNormalization(input_shape=input_shape))\n",
    "    if has_layer_1:\n",
    "        model_2.add(Dropout(drop_1))\n",
    "        model_2.add(Conv2D(neurons_1,kernel_size=(3,3),activation='relu'))\n",
    "        if has_2_conv_layer_1:\n",
    "            model_2.add(Conv2D(neurons_1,kernel_size=(3,3),activation='relu'))\n",
    "        model_2.add(BatchNormalization())\n",
    "\n",
    "    if has_layer_2:\n",
    "        model_2.add(Dropout(drop_2))\n",
    "        model_2.add(Conv2D(neurons_2,kernel_size=(3,3),activation='relu'))\n",
    "        if has_2_conv_layer_2:\n",
    "            model_2.add(Conv2D(neurons_2,kernel_size=(3,3),activation='relu'))\n",
    "        model_2.add(BatchNormalization())\n",
    "\n",
    "    if has_layer_3:\n",
    "        model_2.add(Dropout(drop_3))\n",
    "        model_2.add(Conv2D(neurons_3,kernel_size=(3,3),activation='relu'))\n",
    "        if has_2_conv_layer_3:\n",
    "            model_2.add(Conv2D(neurons_3,kernel_size=(3,3),activation='relu'))\n",
    "        model_2.add(BatchNormalization())\n",
    "\n",
    "    if has_layer_4:\n",
    "        model_2.add(Dropout(drop_4))\n",
    "        model_2.add(Conv2D(neurons_4,kernel_size=(3,3),activation='relu'))\n",
    "        if has_2_conv_layer_4:\n",
    "            model_2.add(Conv2D(neurons_4,kernel_size=(3,3),activation='relu'))\n",
    "        model_2.add(BatchNormalization())\n",
    "\n",
    "    if has_layer_5:\n",
    "        model_2.add(Dropout(drop_5))\n",
    "        model_2.add(Conv2D(neurons_5,kernel_size=(3,3),activation='relu'))\n",
    "        if has_2_conv_layer_5:\n",
    "            model_2.add(Conv2D(neurons_5,kernel_size=(3,3),activation='relu'))\n",
    "        model_2.add(BatchNormalization())\n",
    "    \n",
    "    model_2.add(Conv2D(dense_1,kernel_size=(3,3),activation='relu'))\n",
    "    model_2.add(BatchNormalization())\n",
    "    model_2.add(Conv2D(dense_2,kernel_size=(3,3),activation='relu'))\n",
    "    model_2.add(BatchNormalization())\n",
    "    model_2.add(Conv2D(output_shape,kernel_size=(3,3)))\n",
    "\n",
    "    model_2.compile(loss=tf.keras.losses.mean_squared_error,optimizer='Adam')\n",
    "    \n",
    "    return model_2\n",
    "mode=create_model_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d541f0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    '../model_best_loss.h5',\n",
    "    monitor='val_loss', verbose=0, save_best_only=True,\n",
    "    save_weights_only=False, mode='auto', save_freq='epoch'\n",
    ")\n",
    "early_stop=tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0,\n",
    "    patience=10,\n",
    "    verbose=0,\n",
    "    mode='auto',\n",
    "    baseline=None,\n",
    "    restore_best_weights=False\n",
    ")\n",
    "reduce_lr =tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.9,\n",
    "                              patience=20, min_lr=0.000000000000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4efa657d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_8 (Batc  (None, 96, 96, 1)        4         \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 94, 94, 8)         80        \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 94, 94, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          (None, 92, 92, 8)         584       \n",
      "                                                                 \n",
      " batch_normalization_10 (Bat  (None, 92, 92, 8)        32        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_15 (Conv2D)          (None, 90, 90, 8)         584       \n",
      "                                                                 \n",
      " batch_normalization_11 (Bat  (None, 90, 90, 8)        32        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_16 (Conv2D)          (None, 88, 88, 136)       9928      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,276\n",
      "Trainable params: 11,226\n",
      "Non-trainable params: 50\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 5s 17ms/step - loss: 2737.8591 - val_loss: 2166.0835\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 1201.1661 - val_loss: 626.4495\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 218.5966 - val_loss: 92.3290\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 33.9095 - val_loss: 20.2295\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 12.7916 - val_loss: 11.8212\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 10.3409 - val_loss: 10.1460\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 10.0393 - val_loss: 10.0562\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9926 - val_loss: 9.9599\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9773 - val_loss: 9.7915\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9691 - val_loss: 9.7299\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9664 - val_loss: 9.8771\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9729 - val_loss: 9.9241\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9582 - val_loss: 9.7463\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9617 - val_loss: 9.7626\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9516 - val_loss: 9.8548\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9633 - val_loss: 9.8404\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9670 - val_loss: 9.7430\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9516 - val_loss: 9.7983\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9567 - val_loss: 10.0002\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9696 - val_loss: 9.8703\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9567 - val_loss: 9.7922\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9537 - val_loss: 9.7646\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9513 - val_loss: 9.8304\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9572 - val_loss: 10.0504\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9603 - val_loss: 9.8365\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9626 - val_loss: 9.7918\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9558 - val_loss: 9.7008\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9565 - val_loss: 9.7146\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9584 - val_loss: 9.7932\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9621 - val_loss: 9.7973\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9584 - val_loss: 9.7212\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9574 - val_loss: 9.7300\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9564 - val_loss: 9.7861\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9630 - val_loss: 9.7959\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9646 - val_loss: 9.7524\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9507 - val_loss: 9.7633\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9604 - val_loss: 9.7329\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9527 - val_loss: 9.7128\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9608 - val_loss: 9.7778\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9612 - val_loss: 9.7137\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9607 - val_loss: 9.7852\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9543 - val_loss: 9.7187\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9531 - val_loss: 9.9317\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9575 - val_loss: 9.7688\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9517 - val_loss: 9.6912\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9563 - val_loss: 9.6838\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9516 - val_loss: 9.7186\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9590 - val_loss: 9.7535\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9559 - val_loss: 9.7584\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9635 - val_loss: 9.7551\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9598 - val_loss: 9.7296\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9559 - val_loss: 9.7354\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9471 - val_loss: 9.7593\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9457 - val_loss: 9.7590\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9431 - val_loss: 9.7344\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9519 - val_loss: 9.7302\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9406 - val_loss: 9.7838\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9548 - val_loss: 9.7782\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9507 - val_loss: 9.7401\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9521 - val_loss: 9.7660\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9473 - val_loss: 9.7718\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9520 - val_loss: 9.7146\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9688 - val_loss: 9.7646\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9578 - val_loss: 9.7115\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9515 - val_loss: 9.6996\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9490 - val_loss: 9.6828\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9416 - val_loss: 9.6855\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9446 - val_loss: 9.7701\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9464 - val_loss: 9.7327\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9505 - val_loss: 9.7118\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9557 - val_loss: 9.6973\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9485 - val_loss: 9.8054\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9446 - val_loss: 9.6933\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9479 - val_loss: 9.7507\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9542 - val_loss: 9.7135\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9486 - val_loss: 9.8144\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9606 - val_loss: 9.7516\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9495 - val_loss: 9.7509\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9522 - val_loss: 9.6985\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9473 - val_loss: 9.7164\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9520 - val_loss: 9.7641\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9457 - val_loss: 9.7864\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9498 - val_loss: 9.7420\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9498 - val_loss: 9.7799\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9487 - val_loss: 9.7518\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9553 - val_loss: 9.7188\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9424 - val_loss: 9.7499\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9501 - val_loss: 9.7137\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9475 - val_loss: 9.7805\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9469 - val_loss: 9.6738\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9485 - val_loss: 9.6948\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9412 - val_loss: 9.7298\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9560 - val_loss: 9.7352\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9543 - val_loss: 9.7584\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9484 - val_loss: 9.6994\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9547 - val_loss: 9.6932\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9334 - val_loss: 9.8093\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9527 - val_loss: 9.7448\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9424 - val_loss: 9.7334\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9420 - val_loss: 9.7282\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9415 - val_loss: 9.8478\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9469 - val_loss: 9.7139\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9416 - val_loss: 9.6983\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9460 - val_loss: 9.7582\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9374 - val_loss: 9.7525\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9528 - val_loss: 9.6827\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9512 - val_loss: 9.7446\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9361 - val_loss: 9.7178\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9429 - val_loss: 9.7276\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9493 - val_loss: 9.7178\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9392 - val_loss: 9.6920\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9451 - val_loss: 9.7715\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9539 - val_loss: 9.7802\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9479 - val_loss: 9.7050\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9456 - val_loss: 9.7569\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9454 - val_loss: 9.7342\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9431 - val_loss: 9.7490\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9417 - val_loss: 9.6769\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9449 - val_loss: 9.7767\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9387 - val_loss: 9.7840\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9433 - val_loss: 9.7891\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9402 - val_loss: 9.7691\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9532 - val_loss: 9.7312\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9461 - val_loss: 9.7467\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9469 - val_loss: 9.7354\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9407 - val_loss: 9.7697\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9402 - val_loss: 9.7273\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9414 - val_loss: 9.7081\n",
      "\n",
      "Loss: 970.81%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 232)       2320      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 232)       484648    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 92, 92, 232)      928       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 92, 92, 232)       0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 160)       334240    \n",
      "                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch_normalization_2 (Batc  (None, 90, 90, 160)      640       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 90, 90, 160)       0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 193)       278113    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 88, 88, 193)      772       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 88, 88, 193)       0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 139)       241582    \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 86, 86, 139)      556       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 84, 84, 108)       135216    \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 84, 84, 108)      432       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 82, 82, 133)       129409    \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 82, 82, 133)      532       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 80, 80, 136)       162928    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,772,320\n",
      "Trainable params: 1,770,388\n",
      "Non-trainable params: 1,932\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 60s 227ms/step - loss: 1197.1582 - val_loss: 56.6755\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 11.1968 - val_loss: 12.3923\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 10.3175 - val_loss: 10.6385\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 10.2469 - val_loss: 10.4424\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 10.1781 - val_loss: 10.3027\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 10.1283 - val_loss: 14.6238\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 10.1852 - val_loss: 10.8813\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 10.1354 - val_loss: 10.3073\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 10.0944 - val_loss: 10.3678\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 10.0985 - val_loss: 9.9474\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 10.0804 - val_loss: 11.4642\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 10.0841 - val_loss: 14.6132\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 10.0708 - val_loss: 12.0123\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 10.0939 - val_loss: 11.3784\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 10.1343 - val_loss: 12.5009\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 10.0879 - val_loss: 12.7507\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 10.0788 - val_loss: 11.5127\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 10.1119 - val_loss: 10.7142\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 10.1253 - val_loss: 10.3935\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 10.0729 - val_loss: 10.8462\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 10.1201 - val_loss: 13.3140\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 10.1039 - val_loss: 13.7076\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 10.0839 - val_loss: 10.8049\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 10.0862 - val_loss: 13.3656\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 10.1121 - val_loss: 10.3616\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 10.1441 - val_loss: 9.9210\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 10.0724 - val_loss: 10.0179\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 10.0756 - val_loss: 9.8755\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 10.0890 - val_loss: 10.2613\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 10.0560 - val_loss: 12.2976\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 10.0734 - val_loss: 10.0337\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 10.1082 - val_loss: 11.6381\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 10.0920 - val_loss: 10.3397\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 10.0559 - val_loss: 10.2253\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 10.0271 - val_loss: 10.4913\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 10.0772 - val_loss: 10.9228\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 10.0776 - val_loss: 10.2448\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 10.0655 - val_loss: 10.2719\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 10.0533 - val_loss: 10.7998\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 10.0932 - val_loss: 10.6959\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 10.0522 - val_loss: 10.1490\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 10.0628 - val_loss: 10.4893\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 10.0539 - val_loss: 9.9895\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 10.0250 - val_loss: 10.5587\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 10.0268 - val_loss: 9.7980\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 10.0442 - val_loss: 10.3732\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 10.0238 - val_loss: 9.8268\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 10.0191 - val_loss: 9.8608\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 10.0301 - val_loss: 9.9920\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 10.0250 - val_loss: 9.9603\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 10.0003 - val_loss: 12.1489\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 10.0160 - val_loss: 10.0542\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 9.9955 - val_loss: 9.7089\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 10.0222 - val_loss: 10.0432\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 10.0187 - val_loss: 10.7762\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 9.9791 - val_loss: 10.1584\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 9.9694 - val_loss: 10.1108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 9.9722 - val_loss: 11.1725\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 9.9960 - val_loss: 9.8783\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 9.9607 - val_loss: 9.9634\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 9.9722 - val_loss: 10.3245\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 9.9573 - val_loss: 9.8571\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 9.9351 - val_loss: 9.9318\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 9.9397 - val_loss: 9.9293\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 9.9253 - val_loss: 10.0798\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 54s 218ms/step - loss: 9.9611 - val_loss: 9.9852\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 9.9441 - val_loss: 10.1080\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 9.9216 - val_loss: 10.5515\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 54s 218ms/step - loss: 9.9152 - val_loss: 9.7604\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 9.9100 - val_loss: 9.9166\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 9.9164 - val_loss: 9.8484\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 54s 218ms/step - loss: 9.9000 - val_loss: 10.6722\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 54s 218ms/step - loss: 9.8856 - val_loss: 10.0115\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 9.8892 - val_loss: 9.8338\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 9.8791 - val_loss: 9.9533\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 9.8690 - val_loss: 9.7257\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 9.8431 - val_loss: 9.8124\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 54s 218ms/step - loss: 9.8501 - val_loss: 9.8345\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 9.8726 - val_loss: 9.7584\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 54s 218ms/step - loss: 9.8427 - val_loss: 9.8782\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 9.8494 - val_loss: 10.2296\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 54s 218ms/step - loss: 9.8136 - val_loss: 9.8311\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 54s 218ms/step - loss: 9.8188 - val_loss: 9.7125\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 9.8091 - val_loss: 9.7944\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 54s 218ms/step - loss: 9.8144 - val_loss: 9.8395\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 54s 218ms/step - loss: 9.7826 - val_loss: 9.7221\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 9.8002 - val_loss: 9.8097\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 54s 218ms/step - loss: 9.7754 - val_loss: 9.8517\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 9.7754 - val_loss: 9.7242\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 54s 218ms/step - loss: 9.7546 - val_loss: 9.7518\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 54s 218ms/step - loss: 9.7354 - val_loss: 9.8455\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 54s 218ms/step - loss: 9.7362 - val_loss: 9.7295\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 54s 218ms/step - loss: 9.7252 - val_loss: 10.0986\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 54s 218ms/step - loss: 9.7489 - val_loss: 9.6397\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 54s 218ms/step - loss: 9.7500 - val_loss: 9.7370\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 54s 218ms/step - loss: 9.7178 - val_loss: 9.7930\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 9.6932 - val_loss: 9.7920\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 54s 218ms/step - loss: 9.7030 - val_loss: 10.2765\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 54s 218ms/step - loss: 9.6776 - val_loss: 9.6903\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 54s 218ms/step - loss: 9.6823 - val_loss: 9.9603\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 54s 218ms/step - loss: 9.7109 - val_loss: 9.7941\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 54s 218ms/step - loss: 9.6597 - val_loss: 10.0454\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 54s 218ms/step - loss: 9.6551 - val_loss: 9.7870\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 54s 218ms/step - loss: 9.6758 - val_loss: 9.7117\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 54s 218ms/step - loss: 9.6481 - val_loss: 10.6449\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 54s 218ms/step - loss: 9.6387 - val_loss: 10.6127\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 54s 218ms/step - loss: 9.6326 - val_loss: 9.7978\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 54s 218ms/step - loss: 9.6402 - val_loss: 9.9496\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 54s 218ms/step - loss: 9.6129 - val_loss: 9.7352\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 54s 218ms/step - loss: 9.6184 - val_loss: 9.9422\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 54s 218ms/step - loss: 9.6008 - val_loss: 10.2036\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 54s 218ms/step - loss: 9.5913 - val_loss: 9.8039\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 54s 218ms/step - loss: 9.6486 - val_loss: 9.7707\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 54s 218ms/step - loss: 9.5751 - val_loss: 9.9001\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 54s 218ms/step - loss: 9.5812 - val_loss: 9.8447\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 54s 218ms/step - loss: 9.5919 - val_loss: 9.9636\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 54s 218ms/step - loss: 9.5758 - val_loss: 9.9966\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 54s 218ms/step - loss: 9.5795 - val_loss: 9.7651\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 54s 218ms/step - loss: 9.5711 - val_loss: 9.9066\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 54s 218ms/step - loss: 9.5793 - val_loss: 10.0497\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 54s 218ms/step - loss: 9.5162 - val_loss: 9.8095\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 54s 218ms/step - loss: 9.5194 - val_loss: 9.9490\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 54s 218ms/step - loss: 9.5302 - val_loss: 9.7634\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 54s 218ms/step - loss: 9.5104 - val_loss: 10.0360\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 54s 218ms/step - loss: 9.5113 - val_loss: 9.7085\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 54s 218ms/step - loss: 9.5395 - val_loss: 10.2924\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 54s 218ms/step - loss: 9.4918 - val_loss: 10.0689\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 9.4941 - val_loss: 9.9091\n",
      "\n",
      "Loss: 990.91%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 33)        330       \n",
      "                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch_normalization_1 (Batc  (None, 94, 94, 33)       132       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 94, 94, 33)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 204)       60792     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 92, 92, 204)      816       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 113)       207581    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 90, 90, 113)      452       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 51)        51918     \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 88, 88, 51)       204       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 136)       62560     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 384,789\n",
      "Trainable params: 383,985\n",
      "Non-trainable params: 804\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 21s 77ms/step - loss: 1764.4890 - val_loss: 299.3759\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 39.9759 - val_loss: 12.2971\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 10.2692 - val_loss: 10.4101\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 10.1403 - val_loss: 10.0363\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 10.0971 - val_loss: 9.7695\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 10.0920 - val_loss: 10.1932\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 10.0764 - val_loss: 9.8383\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 10.0648 - val_loss: 9.8752\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 10.0506 - val_loss: 9.7342\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 10.0640 - val_loss: 9.8496\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 10.0572 - val_loss: 9.7717\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 10.0473 - val_loss: 9.7796\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 10.0655 - val_loss: 9.8913\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 10.0420 - val_loss: 9.8446\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 10.0517 - val_loss: 9.8684\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 10.0510 - val_loss: 9.8356\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 10.0637 - val_loss: 9.9292\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 10.0304 - val_loss: 9.8962\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 10.0645 - val_loss: 9.7549\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 10.0289 - val_loss: 9.8313\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 10.0434 - val_loss: 9.8361\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 10.0143 - val_loss: 9.7121\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 10.0221 - val_loss: 9.9112\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 10.0435 - val_loss: 9.7937\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 10.0355 - val_loss: 9.8444\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 10.0207 - val_loss: 9.8194\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 10.0278 - val_loss: 9.7067\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 10.0265 - val_loss: 9.8488\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 10.0300 - val_loss: 9.8670\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 10.0411 - val_loss: 9.7536\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 10.0232 - val_loss: 9.7967\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 10.0318 - val_loss: 9.8943\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 10.0243 - val_loss: 9.8415\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 10.0165 - val_loss: 9.9853\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 10.0348 - val_loss: 9.9175\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 10.0189 - val_loss: 9.8740\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 10.0016 - val_loss: 9.8199\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 10.0125 - val_loss: 9.7376\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 10.0390 - val_loss: 9.9643\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 10.0369 - val_loss: 9.8677\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 10.0208 - val_loss: 9.7359\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 10.0151 - val_loss: 9.7874\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 10.0207 - val_loss: 9.8484\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9859 - val_loss: 9.8041\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 10.0004 - val_loss: 9.7267\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 10.0071 - val_loss: 9.9262\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9933 - val_loss: 9.8352\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 10.0187 - val_loss: 9.8288\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 10.0194 - val_loss: 9.6981\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9819 - val_loss: 9.7714\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 10.0090 - val_loss: 9.8168\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 10.0010 - val_loss: 9.7988\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9949 - val_loss: 9.9509\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 10.0052 - val_loss: 9.7633\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9849 - val_loss: 9.7196\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 10.0122 - val_loss: 9.8402\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.9942 - val_loss: 9.8402\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9843 - val_loss: 9.8362\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 10.0005 - val_loss: 9.9247\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9782 - val_loss: 9.8990\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 10.0049 - val_loss: 9.7719\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9737 - val_loss: 9.9612\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9716 - val_loss: 9.6916\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9736 - val_loss: 9.7156\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9670 - val_loss: 9.6792\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9775 - val_loss: 9.7901\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.9821 - val_loss: 9.7976\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9836 - val_loss: 9.7304\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9794 - val_loss: 9.7783\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9663 - val_loss: 9.9251\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9895 - val_loss: 9.6661\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.9653 - val_loss: 9.6843\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.9828 - val_loss: 9.7191\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9583 - val_loss: 9.7210\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9956 - val_loss: 9.8353\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9822 - val_loss: 9.7059\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9580 - val_loss: 9.7268\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9631 - val_loss: 9.8942\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9494 - val_loss: 9.7026\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.9761 - val_loss: 9.6339\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9422 - val_loss: 9.7921\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.9573 - val_loss: 9.6966\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9786 - val_loss: 9.7671\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9664 - val_loss: 9.6600\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9446 - val_loss: 9.6810\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9532 - val_loss: 9.7657\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9469 - val_loss: 9.7007\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9463 - val_loss: 9.8485\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9371 - val_loss: 9.6723\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9336 - val_loss: 9.7252\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9351 - val_loss: 9.7980\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9505 - val_loss: 9.6910\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9478 - val_loss: 9.9474\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9395 - val_loss: 9.7057\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9322 - val_loss: 9.6493\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9269 - val_loss: 9.6328\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9544 - val_loss: 9.7015\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9459 - val_loss: 9.6564\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.9411 - val_loss: 9.6482\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9386 - val_loss: 9.6464\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9436 - val_loss: 9.6699\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9344 - val_loss: 9.6862\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9452 - val_loss: 9.7467\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9254 - val_loss: 9.7097\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9144 - val_loss: 9.7523\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9190 - val_loss: 9.6565\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9245 - val_loss: 9.6876\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9254 - val_loss: 9.6912\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9423 - val_loss: 9.6750\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9242 - val_loss: 9.6405\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9157 - val_loss: 9.8188\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9133 - val_loss: 9.8552\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9158 - val_loss: 10.0207\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9305 - val_loss: 9.7482\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9161 - val_loss: 9.6683\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9271 - val_loss: 9.7046\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9159 - val_loss: 9.7292\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9327 - val_loss: 9.9545\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9213 - val_loss: 9.6762\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9154 - val_loss: 9.6988\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9168 - val_loss: 9.7785\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9201 - val_loss: 9.5932\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9048 - val_loss: 9.7308\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9168 - val_loss: 9.5976\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9255 - val_loss: 9.6988\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9100 - val_loss: 9.6733\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.8990 - val_loss: 9.6183\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9087 - val_loss: 9.6896\n",
      "\n",
      "Loss: 968.96%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 117)       1170      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 117)       123318    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 92, 92, 117)      468       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 92, 92, 117)       0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 120)       126480    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 90, 90, 120)      480       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 20)        21620     \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 88, 88, 20)       80        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 47)        8507      \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 86, 86, 47)       188       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 84, 84, 136)       57664     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 339,979\n",
      "Trainable params: 339,369\n",
      "Non-trainable params: 610\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 21s 73ms/step - loss: 1810.6581 - val_loss: 222.9772\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 49.0331 - val_loss: 10.9275\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 10.1524 - val_loss: 9.9684\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.9690 - val_loss: 9.7147\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.9619 - val_loss: 9.7037\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.9617 - val_loss: 9.6905\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.9452 - val_loss: 9.7297\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.9497 - val_loss: 9.8223\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.9468 - val_loss: 9.7909\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.9510 - val_loss: 9.7689\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.9634 - val_loss: 9.7029\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.9583 - val_loss: 9.7932\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.9471 - val_loss: 9.7711\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.9572 - val_loss: 9.7083\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.9554 - val_loss: 9.7631\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.9618 - val_loss: 9.8087\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.9641 - val_loss: 9.7934\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.9712 - val_loss: 9.7073\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.9507 - val_loss: 9.7608\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.9931 - val_loss: 9.9476\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.9536 - val_loss: 9.8194\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.9556 - val_loss: 9.8783\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.9414 - val_loss: 9.8944\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.9435 - val_loss: 9.8364\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.9449 - val_loss: 9.8097\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.9604 - val_loss: 9.7907\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.9343 - val_loss: 10.1223\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 17s 69ms/step - loss: 9.9493 - val_loss: 9.7023\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.9286 - val_loss: 10.0501\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 17s 69ms/step - loss: 9.9472 - val_loss: 9.8744\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.9416 - val_loss: 9.8139\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.9336 - val_loss: 9.6814\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.9299 - val_loss: 9.6567\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.9334 - val_loss: 9.7311\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.9195 - val_loss: 10.0350\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.9141 - val_loss: 9.6809\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.9173 - val_loss: 9.8427\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.9167 - val_loss: 9.6186\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.8967 - val_loss: 9.7599\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.9046 - val_loss: 9.6250\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.8857 - val_loss: 9.7455\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.8691 - val_loss: 9.7424\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.8734 - val_loss: 9.6673\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.8577 - val_loss: 9.6955\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.8590 - val_loss: 9.7220\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.8664 - val_loss: 9.6517\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.8450 - val_loss: 9.7371\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.8665 - val_loss: 9.6521\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.8332 - val_loss: 9.6596\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.8278 - val_loss: 9.6235\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.8280 - val_loss: 9.7443\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.8433 - val_loss: 9.6104\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.8068 - val_loss: 9.8002\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.8131 - val_loss: 9.6511\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.8131 - val_loss: 9.7822\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.7938 - val_loss: 9.6994\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.8107 - val_loss: 9.5694\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.8224 - val_loss: 9.5134\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.7909 - val_loss: 9.5529\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.7750 - val_loss: 9.6882\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.7953 - val_loss: 9.5552\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.7909 - val_loss: 9.5806\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.7908 - val_loss: 9.5849\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.7956 - val_loss: 9.6090\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.7775 - val_loss: 9.6184\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.7745 - val_loss: 9.6771\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.7912 - val_loss: 9.6193\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.7661 - val_loss: 9.6670\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.7739 - val_loss: 9.5869\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.7749 - val_loss: 9.6453\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.7631 - val_loss: 9.4941\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.7620 - val_loss: 9.5535\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.7666 - val_loss: 9.5806\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.7517 - val_loss: 9.5892\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.7411 - val_loss: 9.5938\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.7517 - val_loss: 9.5690\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.7491 - val_loss: 9.7467\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.7434 - val_loss: 9.5207\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.7404 - val_loss: 9.6870\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.7266 - val_loss: 9.6975\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.7203 - val_loss: 9.6071\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.7421 - val_loss: 9.5651\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.7307 - val_loss: 9.5670\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.7258 - val_loss: 9.4974\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.7314 - val_loss: 9.8365\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.7317 - val_loss: 10.3495\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.7301 - val_loss: 9.6423\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.7144 - val_loss: 9.5980\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.7144 - val_loss: 9.5323\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.7043 - val_loss: 9.5392\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.7053 - val_loss: 9.5153\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.7003 - val_loss: 9.5362\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.6867 - val_loss: 9.5252\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.7161 - val_loss: 9.6316\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.7119 - val_loss: 9.4706\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.6949 - val_loss: 9.4447\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.6871 - val_loss: 9.6138\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.7029 - val_loss: 9.7065\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.7046 - val_loss: 9.8269\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.6847 - val_loss: 9.5410\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.6991 - val_loss: 9.4453\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.6980 - val_loss: 9.9002\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.6762 - val_loss: 9.5318\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.6874 - val_loss: 9.5677\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.6874 - val_loss: 9.4602\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.6793 - val_loss: 9.4960\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.6986 - val_loss: 9.4922\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.6721 - val_loss: 9.5968\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.6571 - val_loss: 9.6379\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.6836 - val_loss: 9.4331\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.6685 - val_loss: 9.6434\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.6698 - val_loss: 9.5210\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.6658 - val_loss: 9.4480\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.6604 - val_loss: 9.4304\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.6861 - val_loss: 9.5728\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.6964 - val_loss: 9.7475\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.6611 - val_loss: 9.5575\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.6658 - val_loss: 9.5920\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.6676 - val_loss: 9.4782\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.6684 - val_loss: 9.6427\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.6638 - val_loss: 9.4834\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.6372 - val_loss: 9.6150\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.6551 - val_loss: 9.5368\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.6492 - val_loss: 9.4790\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.6374 - val_loss: 9.4356\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.6344 - val_loss: 9.4433\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.6454 - val_loss: 9.4392\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 9.6520 - val_loss: 9.4578\n",
      "\n",
      "Loss: 945.78%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 196)       1960      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 196)       345940    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 92, 92, 196)      784       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 92, 92, 196)       0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 232)       409480    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 90, 90, 232)      928       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 90, 90, 232)       0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 107)       223523    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 88, 88, 107)      428       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " conv2d_4 (Conv2D)           (None, 86, 86, 10)        9640      \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 86, 86, 10)       40        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 84, 84, 95)        8645      \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 84, 84, 95)       380       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 82, 82, 136)       116416    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,118,168\n",
      "Trainable params: 1,116,886\n",
      "Non-trainable params: 1,282\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 43s 158ms/step - loss: 1369.3894 - val_loss: 110.3148\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 13.1189 - val_loss: 16.2395\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 10.3364 - val_loss: 12.4682\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 10.3141 - val_loss: 10.7325\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 10.2759 - val_loss: 10.5771\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 10.1564 - val_loss: 10.8489\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 10.2193 - val_loss: 11.4740\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 10.2289 - val_loss: 10.9319\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 10.1580 - val_loss: 10.1042\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 10.1227 - val_loss: 10.4177\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 10.0870 - val_loss: 10.0749\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 10.0880 - val_loss: 10.0087\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 10.1313 - val_loss: 10.0597\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 10.1027 - val_loss: 10.0696\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 10.1075 - val_loss: 10.2799\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 10.1056 - val_loss: 10.1120\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 10.0876 - val_loss: 10.1518\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 10.1101 - val_loss: 10.0428\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 10.1027 - val_loss: 9.9090\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 10.0971 - val_loss: 9.9792\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 10.0747 - val_loss: 10.0920\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 10.1204 - val_loss: 10.1903\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 10.0971 - val_loss: 9.9941\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 10.0902 - val_loss: 10.1759\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 10.1004 - val_loss: 10.1253\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 10.1034 - val_loss: 9.8939\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 10.1046 - val_loss: 10.2083\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 10.0814 - val_loss: 10.2913\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 10.0981 - val_loss: 9.9815\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 10.0767 - val_loss: 10.0994\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 10.0801 - val_loss: 10.1907\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 10.0805 - val_loss: 9.9224\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 10.0966 - val_loss: 9.9772\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 10.0672 - val_loss: 9.8830\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 10.0747 - val_loss: 9.8345\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 10.0925 - val_loss: 9.9434\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 10.0816 - val_loss: 9.9651\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 10.0695 - val_loss: 9.8646\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 10.0713 - val_loss: 9.8863\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 10.0845 - val_loss: 9.9376\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 10.0847 - val_loss: 10.3438\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 10.0898 - val_loss: 10.0477\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 10.0507 - val_loss: 9.8263\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 10.0397 - val_loss: 9.7691\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 10.0619 - val_loss: 9.9860\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 10.0249 - val_loss: 10.0072\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 10.0279 - val_loss: 10.1074\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 10.0202 - val_loss: 9.7408\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 10.0169 - val_loss: 9.9062\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 10.0087 - val_loss: 9.7968\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 10.0201 - val_loss: 9.9347\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 10.0135 - val_loss: 9.7564\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 10.0133 - val_loss: 9.7359\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 10.0015 - val_loss: 9.8317\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 9.9947 - val_loss: 9.7594\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 9.9905 - val_loss: 9.8037\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 10.0020 - val_loss: 9.8672\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 9.9825 - val_loss: 9.7903\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 10.0024 - val_loss: 9.9058\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 9.9760 - val_loss: 10.0029\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 9.9723 - val_loss: 9.8274\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 9.9717 - val_loss: 9.7404\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 9.9778 - val_loss: 9.8169\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 9.9548 - val_loss: 10.5014\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 9.9564 - val_loss: 10.2234\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 9.9586 - val_loss: 9.9760\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 9.9546 - val_loss: 9.7234\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 9.9712 - val_loss: 9.9848\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 9.9540 - val_loss: 9.8395\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 9.9567 - val_loss: 9.6719\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 9.9596 - val_loss: 9.9677\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 9.9660 - val_loss: 9.6782\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 9.9480 - val_loss: 9.8497\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 9.9252 - val_loss: 9.6712\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 9.9382 - val_loss: 9.7117\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 9.9237 - val_loss: 9.6405\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 9.9143 - val_loss: 9.8830\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 9.9092 - val_loss: 9.6688\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 9.9517 - val_loss: 9.8476\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 9.8943 - val_loss: 9.8547\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 9.9143 - val_loss: 9.8258\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 9.8927 - val_loss: 9.7937\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 9.9019 - val_loss: 9.6931\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 9.8933 - val_loss: 9.9883\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 9.8944 - val_loss: 10.1169\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 9.8906 - val_loss: 9.6325\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 9.8896 - val_loss: 10.0884\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 9.8962 - val_loss: 9.6925\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 9.8913 - val_loss: 9.7619\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 9.8644 - val_loss: 9.7356\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 9.8810 - val_loss: 9.6201\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 9.8637 - val_loss: 9.7519\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 9.8634 - val_loss: 9.7194\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 9.8654 - val_loss: 9.7631\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 9.8685 - val_loss: 9.6729\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 9.8663 - val_loss: 9.5981\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 9.8488 - val_loss: 9.6494\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 9.8558 - val_loss: 9.6290\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 9.8169 - val_loss: 9.5722\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 9.8338 - val_loss: 9.5403\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 9.8548 - val_loss: 9.5375\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 9.8470 - val_loss: 9.7372\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 9.8356 - val_loss: 9.7097\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 9.8457 - val_loss: 9.5086\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 9.8113 - val_loss: 9.6627\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 9.8218 - val_loss: 9.7900\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 9.8311 - val_loss: 9.6004\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 9.7952 - val_loss: 9.7687\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 9.8267 - val_loss: 9.5212\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 9.7794 - val_loss: 9.5042\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 9.7873 - val_loss: 9.6884\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 9.8384 - val_loss: 9.4981\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 9.7994 - val_loss: 9.8036\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 9.7945 - val_loss: 9.5415\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 9.7810 - val_loss: 9.7421\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 9.7933 - val_loss: 9.6027\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 9.7908 - val_loss: 9.6222\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 9.7574 - val_loss: 9.6201\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 9.7695 - val_loss: 9.5955\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 9.7834 - val_loss: 9.6586\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 9.7709 - val_loss: 9.5189\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 9.7677 - val_loss: 9.7754\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 9.7818 - val_loss: 9.5159\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 9.8058 - val_loss: 9.7870\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 9.8185 - val_loss: 9.6014\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 9.7636 - val_loss: 9.6277\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 9.7628 - val_loss: 9.6341\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 9.7748 - val_loss: 9.6553\n",
      "\n",
      "Loss: 965.53%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 44)        440       \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 94, 94, 44)       176       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 94, 94, 44)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 130)       51610     \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 130)       152230    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 90, 90, 130)      520       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 90, 90, 130)       0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 203)       237713    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 88, 88, 203)      812       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 82)        149896    \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 86, 86, 82)       328       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 84, 84, 87)        64293     \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 84, 84, 87)       348       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 82, 82, 136)       106624    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 764,994\n",
      "Trainable params: 763,900\n",
      "Non-trainable params: 1,094\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 36s 133ms/step - loss: 1419.6592 - val_loss: 274.4026\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 14.2001 - val_loss: 20.9394\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.2982 - val_loss: 14.7254\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.2126 - val_loss: 11.2444\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.2631 - val_loss: 14.2689\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.2413 - val_loss: 12.3647\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.1819 - val_loss: 10.5287\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.1657 - val_loss: 11.7261\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.1669 - val_loss: 10.4956\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.1325 - val_loss: 11.2859\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.1264 - val_loss: 10.6987\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.1443 - val_loss: 10.5798\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.1810 - val_loss: 11.4491\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.1045 - val_loss: 10.8033\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.1078 - val_loss: 10.7988\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.1196 - val_loss: 9.8627\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.1215 - val_loss: 10.7032\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.1105 - val_loss: 10.0289\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0868 - val_loss: 10.1261\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0736 - val_loss: 10.0308\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.1190 - val_loss: 10.3952\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.1106 - val_loss: 10.1682\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.1132 - val_loss: 10.5607\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0926 - val_loss: 10.0745\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.1237 - val_loss: 10.1873\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.1173 - val_loss: 10.6536\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0768 - val_loss: 9.9035\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.1125 - val_loss: 9.9173\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0819 - val_loss: 10.1295\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0879 - val_loss: 10.1153\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0979 - val_loss: 10.0954\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.1279 - val_loss: 9.9041\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0810 - val_loss: 9.8385\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0894 - val_loss: 9.7762\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0850 - val_loss: 10.4656\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0793 - val_loss: 10.5881\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0783 - val_loss: 9.9504\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0761 - val_loss: 10.2956\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0904 - val_loss: 10.3273\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0615 - val_loss: 10.0368\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0526 - val_loss: 9.7819\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0878 - val_loss: 9.8288\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0735 - val_loss: 10.2363\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0749 - val_loss: 9.9001\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0637 - val_loss: 9.9086\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0490 - val_loss: 9.9574\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0715 - val_loss: 10.6036\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0795 - val_loss: 9.8850\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0617 - val_loss: 10.7247\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0700 - val_loss: 10.0268\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0464 - val_loss: 9.9048\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0777 - val_loss: 10.0634\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0626 - val_loss: 10.0269\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0578 - val_loss: 9.9161\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0418 - val_loss: 9.8831\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0523 - val_loss: 10.0109\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0682 - val_loss: 9.7826\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0523 - val_loss: 9.8361\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0721 - val_loss: 9.8743\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0608 - val_loss: 9.9915\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0334 - val_loss: 9.7091\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0504 - val_loss: 9.8172\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0532 - val_loss: 9.8517\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0532 - val_loss: 9.7692\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0296 - val_loss: 9.7550\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0502 - val_loss: 9.8758\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0369 - val_loss: 9.9048\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0512 - val_loss: 10.0129\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0483 - val_loss: 10.0592\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0375 - val_loss: 9.9620\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0396 - val_loss: 9.9809\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0716 - val_loss: 9.8560\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0492 - val_loss: 10.0176\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0629 - val_loss: 9.8887\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0481 - val_loss: 9.7739\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0613 - val_loss: 9.8147\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0413 - val_loss: 10.0050\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0290 - val_loss: 9.8761\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0243 - val_loss: 9.9166\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0312 - val_loss: 9.8054\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0187 - val_loss: 9.9286\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0377 - val_loss: 9.9256\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0310 - val_loss: 9.9536\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0159 - val_loss: 10.0717\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0516 - val_loss: 9.7952\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0265 - val_loss: 9.8227\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0320 - val_loss: 9.8176\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0268 - val_loss: 9.7732\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0327 - val_loss: 9.9680\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0351 - val_loss: 9.9546\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0093 - val_loss: 9.7032\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0207 - val_loss: 9.9943\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0272 - val_loss: 9.8419\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0311 - val_loss: 9.7701\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0003 - val_loss: 9.7250\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0013 - val_loss: 9.9738\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0260 - val_loss: 9.8999\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0295 - val_loss: 9.9398\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0236 - val_loss: 9.7653\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0187 - val_loss: 9.7675\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9974 - val_loss: 10.0359\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9919 - val_loss: 10.1107\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0126 - val_loss: 9.8044\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9964 - val_loss: 9.8683\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0126 - val_loss: 9.7968\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0110 - val_loss: 10.0740\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9858 - val_loss: 9.8267\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0143 - val_loss: 10.0299\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9696 - val_loss: 9.8265\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0143 - val_loss: 9.8850\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9845 - val_loss: 9.8311\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9939 - val_loss: 9.6935\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9971 - val_loss: 9.6693\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9667 - val_loss: 10.0081\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9880 - val_loss: 9.9868\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9872 - val_loss: 9.8382\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9910 - val_loss: 9.7630\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9916 - val_loss: 10.0497\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9785 - val_loss: 9.8260\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9752 - val_loss: 9.8003\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9642 - val_loss: 9.9850\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9618 - val_loss: 9.7301\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9868 - val_loss: 9.9698\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9861 - val_loss: 9.8250\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9441 - val_loss: 9.8837\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9499 - val_loss: 9.8458\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9782 - val_loss: 9.7547\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9591 - val_loss: 9.9621\n",
      "\n",
      "Loss: 996.21%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 17)        170       \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 94, 94, 17)       68        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 94, 94, 17)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 114)       17556     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 92, 92, 114)      456       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 89)        91403     \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 90, 90, 89)       356       \n",
      " hNormalization)                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 65)        52130     \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 88, 88, 65)       260       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 136)       79696     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 242,099\n",
      "Trainable params: 241,527\n",
      "Non-trainable params: 572\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 17s 60ms/step - loss: 1604.2803 - val_loss: 169.0611\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 22.7435 - val_loss: 12.1436\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.1746 - val_loss: 10.3618\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.1302 - val_loss: 10.3258\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0822 - val_loss: 9.8802\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0878 - val_loss: 9.8915\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0481 - val_loss: 9.7881\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0591 - val_loss: 9.8904\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0473 - val_loss: 9.8031\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0453 - val_loss: 9.8190\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0471 - val_loss: 9.7875\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0376 - val_loss: 10.1667\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0346 - val_loss: 9.7860\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0301 - val_loss: 9.7921\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0350 - val_loss: 9.7988\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0308 - val_loss: 9.9211\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0587 - val_loss: 9.8869\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0375 - val_loss: 9.8350\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0492 - val_loss: 9.8872\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0388 - val_loss: 9.7985\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0405 - val_loss: 9.7895\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0374 - val_loss: 9.8287\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0319 - val_loss: 9.9082\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0647 - val_loss: 9.9018\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0389 - val_loss: 10.0385\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0634 - val_loss: 9.9057\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0489 - val_loss: 9.7450\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0457 - val_loss: 9.8109\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0334 - val_loss: 9.7883\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0482 - val_loss: 9.7895\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0442 - val_loss: 9.8610\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0410 - val_loss: 9.8691\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0490 - val_loss: 9.9083\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0491 - val_loss: 9.8564\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0164 - val_loss: 9.7962\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0378 - val_loss: 9.9316\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0434 - val_loss: 9.9689\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0231 - val_loss: 9.8335\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0561 - val_loss: 9.8427\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0457 - val_loss: 9.9783\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0178 - val_loss: 9.9685\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0401 - val_loss: 9.7401\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0417 - val_loss: 9.9278\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0460 - val_loss: 9.7911\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0228 - val_loss: 9.7614\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0492 - val_loss: 9.7444\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0237 - val_loss: 10.0220\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0288 - val_loss: 9.7535\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0045 - val_loss: 9.8218\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0215 - val_loss: 9.9108\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0271 - val_loss: 9.7799\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0334 - val_loss: 9.8123\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0124 - val_loss: 9.8750\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0425 - val_loss: 9.8805\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0405 - val_loss: 9.7617\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0189 - val_loss: 9.8524\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0116 - val_loss: 9.7880\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9990 - val_loss: 9.8496\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0040 - val_loss: 9.9252\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0219 - val_loss: 9.7632\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0203 - val_loss: 9.8106\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0222 - val_loss: 9.8623\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0219 - val_loss: 9.7798\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9995 - val_loss: 9.9151\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0298 - val_loss: 9.8932\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9930 - val_loss: 9.8396\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9982 - val_loss: 9.8802\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0449 - val_loss: 9.8516\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0118 - val_loss: 9.8018\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0116 - val_loss: 9.7490\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0062 - val_loss: 9.7753\n",
      "Epoch 72/128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0077 - val_loss: 9.7104\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9939 - val_loss: 9.8099\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0242 - val_loss: 9.8552\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0259 - val_loss: 10.0107\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0053 - val_loss: 9.8188\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9990 - val_loss: 9.7739\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0205 - val_loss: 9.7711\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0024 - val_loss: 9.8599\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9899 - val_loss: 9.7689\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0016 - val_loss: 9.8450\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0085 - val_loss: 9.8054\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9789 - val_loss: 9.8591\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0134 - val_loss: 9.7144\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0014 - val_loss: 9.9028\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0081 - val_loss: 9.7221\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9934 - val_loss: 9.7737\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9846 - val_loss: 9.8525\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9824 - val_loss: 9.7331\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9948 - val_loss: 9.8340\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9990 - val_loss: 9.7712\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9732 - val_loss: 9.7347\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0009 - val_loss: 9.8470\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9905 - val_loss: 9.8244\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0107 - val_loss: 9.8282\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0003 - val_loss: 9.7050\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0297 - val_loss: 9.8674\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9793 - val_loss: 9.7205\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9880 - val_loss: 9.7757\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0046 - val_loss: 9.8323\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9990 - val_loss: 9.7146\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9951 - val_loss: 9.7568\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9821 - val_loss: 9.7155\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9925 - val_loss: 9.7077\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9794 - val_loss: 9.7772\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9793 - val_loss: 9.7132\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9974 - val_loss: 9.7284\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9987 - val_loss: 9.7214\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9734 - val_loss: 9.7290\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0103 - val_loss: 9.7583\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0041 - val_loss: 9.6886\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0121 - val_loss: 9.7486\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9879 - val_loss: 9.8449\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9923 - val_loss: 9.7573\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9890 - val_loss: 9.6900\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9834 - val_loss: 9.8164\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9872 - val_loss: 9.6990\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9869 - val_loss: 9.9241\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9927 - val_loss: 9.8273\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9913 - val_loss: 9.8224\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9869 - val_loss: 9.7253\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9790 - val_loss: 9.8364\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9929 - val_loss: 9.7302\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9709 - val_loss: 9.7542\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9681 - val_loss: 9.8412\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9860 - val_loss: 9.8194\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9707 - val_loss: 9.7002\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9949 - val_loss: 9.7849\n",
      "\n",
      "Loss: 978.49%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 168)       1680      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 168)       254184    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 92, 92, 168)      672       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 92, 92, 168)       0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 8)         12104     \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 8)         584       \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 88, 88, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 88, 88, 8)         0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 235)       17155     \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 84, 84, 235)       497260    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 84, 84, 235)      940       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 82, 82, 61)        129076    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 82, 82, 61)       244       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 80, 80, 82)        45100     \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 80, 80, 82)       328       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 78, 78, 136)       100504    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,059,867\n",
      "Trainable params: 1,058,757\n",
      "Non-trainable params: 1,110\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 38s 140ms/step - loss: 1455.6581 - val_loss: 68.0961\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 15.1641 - val_loss: 11.4946\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 10.3592 - val_loss: 13.4208\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 10.1678 - val_loss: 9.9783\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 10.1109 - val_loss: 9.9521\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 10.0770 - val_loss: 10.4063\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 10.0486 - val_loss: 9.8463\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 10.0792 - val_loss: 9.8668\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 10.1214 - val_loss: 10.5259\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 10.1167 - val_loss: 10.0056\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 10.0607 - val_loss: 9.8179\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 10.0608 - val_loss: 9.9267\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 10.0654 - val_loss: 9.7817\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 10.0694 - val_loss: 9.8448\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 10.0371 - val_loss: 10.2087\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 10.0573 - val_loss: 9.9423\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 10.0891 - val_loss: 9.9566\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 10.0964 - val_loss: 9.7162\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 10.0419 - val_loss: 9.9933\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 10.0675 - val_loss: 9.9388\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 10.0770 - val_loss: 9.9757\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 10.0475 - val_loss: 9.7998\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 10.0781 - val_loss: 9.8105\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 10.0546 - val_loss: 10.0197\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 10.0616 - val_loss: 9.7648\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 10.0485 - val_loss: 9.8930\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 10.0753 - val_loss: 9.7984\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 10.0687 - val_loss: 9.7251\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 10.0360 - val_loss: 9.7052\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 10.0408 - val_loss: 9.8275\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 10.0720 - val_loss: 9.9592\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 10.0450 - val_loss: 9.9120\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 10.0233 - val_loss: 9.8269\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 10.0332 - val_loss: 9.7741\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 10.0453 - val_loss: 9.7761\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 10.0533 - val_loss: 9.8492\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 10.0138 - val_loss: 9.9807\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 10.0569 - val_loss: 9.8583\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 10.0332 - val_loss: 9.9754\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 10.0420 - val_loss: 9.7116\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 10.0370 - val_loss: 9.7937\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 10.0131 - val_loss: 9.7407\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.9935 - val_loss: 9.8005\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 10.0056 - val_loss: 10.0586\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 10.0257 - val_loss: 10.1378\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 10.0393 - val_loss: 10.1765\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 10.0262 - val_loss: 9.8822\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 10.0089 - val_loss: 9.8713\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 10.0121 - val_loss: 9.9393\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 10.0111 - val_loss: 9.7705\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.9801 - val_loss: 9.9812\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.9781 - val_loss: 10.3498\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 10.0015 - val_loss: 9.9548\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.9700 - val_loss: 9.8143\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.9802 - val_loss: 10.0433\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.9760 - val_loss: 9.8634\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.9774 - val_loss: 9.8626\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.9809 - val_loss: 9.7868\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.9864 - val_loss: 9.7921\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.9809 - val_loss: 9.9957\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.9822 - val_loss: 9.8030\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.9893 - val_loss: 10.0439\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.9560 - val_loss: 9.7559\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.9364 - val_loss: 9.8407\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.9799 - val_loss: 9.7768\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.9720 - val_loss: 9.8148\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.9588 - val_loss: 9.9160\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.9438 - val_loss: 9.7089\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.9498 - val_loss: 9.8092\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.9348 - val_loss: 9.8933\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.9509 - val_loss: 9.7972\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.9166 - val_loss: 10.2687\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.9149 - val_loss: 9.8308\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.9254 - val_loss: 9.7766\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.9158 - val_loss: 9.7992\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.9321 - val_loss: 9.8500\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.9399 - val_loss: 9.7763\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.9111 - val_loss: 9.8851\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.9105 - val_loss: 9.8449\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.9003 - val_loss: 10.0685\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.8856 - val_loss: 9.8847\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.8829 - val_loss: 10.0304\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.8799 - val_loss: 9.8333\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.8780 - val_loss: 9.7121\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.8618 - val_loss: 9.9689\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.8403 - val_loss: 9.7546\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.8317 - val_loss: 9.9862\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.8252 - val_loss: 9.9621\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.8520 - val_loss: 9.7950\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.8375 - val_loss: 9.9205\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.8350 - val_loss: 9.8060\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.8203 - val_loss: 10.0586\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.8174 - val_loss: 9.9881\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.7753 - val_loss: 9.8407\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.7994 - val_loss: 10.0427\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.7817 - val_loss: 10.0684\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.7654 - val_loss: 9.8702\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.7718 - val_loss: 10.0895\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.7733 - val_loss: 9.7808\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.7605 - val_loss: 9.9078\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.7497 - val_loss: 9.9066\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.7601 - val_loss: 9.9482\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.7632 - val_loss: 9.9847\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.7403 - val_loss: 9.8464\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.7189 - val_loss: 9.7299\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.7037 - val_loss: 10.4226\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.7067 - val_loss: 9.7939\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.7035 - val_loss: 10.1096\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.6790 - val_loss: 9.8601\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.6758 - val_loss: 12.4545\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.6684 - val_loss: 11.3605\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.6771 - val_loss: 11.7188\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.6610 - val_loss: 11.8480\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.6460 - val_loss: 10.3632\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.6482 - val_loss: 10.9589\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.6667 - val_loss: 10.5009\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.6279 - val_loss: 10.1173\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.6330 - val_loss: 10.0304\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.6145 - val_loss: 10.1015\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.6202 - val_loss: 9.7774\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.6163 - val_loss: 9.8388\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.5868 - val_loss: 10.0936\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.5999 - val_loss: 10.0859\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.5543 - val_loss: 10.2622\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.5756 - val_loss: 9.6807\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.5640 - val_loss: 10.0235\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.5608 - val_loss: 10.0192\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 9.5509 - val_loss: 9.8345\n",
      "\n",
      "Loss: 983.45%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 230)       2300      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 94, 94, 230)      920       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 94, 94, 230)       0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 51)        105621    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 92, 92, 51)       204       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 92, 92, 51)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 159)       73140     \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 90, 90, 159)      636       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 90, 90, 159)       0         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 63)        90216     \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 63)        35784     \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 86, 86, 63)       252       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 86, 86, 63)        0         \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 84, 84, 82)        46576     \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 84, 84, 82)       328       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 82, 82, 26)        19214     \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 82, 82, 26)       104       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 80, 80, 77)        18095     \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 80, 80, 77)       308       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 78, 78, 136)       94384     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 488,086\n",
      "Trainable params: 486,708\n",
      "Non-trainable params: 1,378\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 34s 126ms/step - loss: 1489.8829 - val_loss: 111.2477\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 16.7214 - val_loss: 11.9680\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 10.2564 - val_loss: 10.8673\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 10.2464 - val_loss: 10.4791\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 10.1336 - val_loss: 10.6323\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 10.1330 - val_loss: 10.0264\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 10.0623 - val_loss: 10.0329\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 10.0469 - val_loss: 10.1785\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 10.0501 - val_loss: 9.9060\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 10.0648 - val_loss: 10.2523\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 10.0294 - val_loss: 10.3664\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 10.0515 - val_loss: 10.3445\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 10.0378 - val_loss: 9.9770\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 10.0672 - val_loss: 16.8521\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 10.0581 - val_loss: 9.9566\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 30s 121ms/step - loss: 10.0629 - val_loss: 9.9725\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 30s 121ms/step - loss: 10.0333 - val_loss: 10.7839\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 30s 121ms/step - loss: 10.0607 - val_loss: 9.8616\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 30s 121ms/step - loss: 10.0640 - val_loss: 9.9806\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 30s 121ms/step - loss: 10.0883 - val_loss: 9.8678\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 30s 121ms/step - loss: 10.0561 - val_loss: 9.8926\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 30s 121ms/step - loss: 10.0412 - val_loss: 10.4991\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 10.0221 - val_loss: 10.7882\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 10.0903 - val_loss: 10.0597\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 10.0591 - val_loss: 11.8490\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 10.0487 - val_loss: 10.0886\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 30s 121ms/step - loss: 10.0631 - val_loss: 10.6957\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 30s 121ms/step - loss: 10.0364 - val_loss: 11.8704\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 30s 121ms/step - loss: 10.0588 - val_loss: 10.1705\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 30s 121ms/step - loss: 10.0418 - val_loss: 10.1216\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 30s 121ms/step - loss: 10.0379 - val_loss: 10.5697\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 10.0426 - val_loss: 10.0699\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 30s 121ms/step - loss: 10.0241 - val_loss: 10.1702\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 10.0507 - val_loss: 10.2463\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 10.0726 - val_loss: 9.7967\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 30s 121ms/step - loss: 10.0611 - val_loss: 9.9015\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 30s 121ms/step - loss: 10.0481 - val_loss: 10.1404\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 30s 121ms/step - loss: 10.0588 - val_loss: 9.9712\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 10.0189 - val_loss: 10.1389\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 30s 121ms/step - loss: 10.0507 - val_loss: 10.1247\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 30s 121ms/step - loss: 10.0520 - val_loss: 10.0422\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 30s 121ms/step - loss: 10.0369 - val_loss: 9.8079\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 30s 121ms/step - loss: 10.0317 - val_loss: 9.9066\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 30s 121ms/step - loss: 10.0173 - val_loss: 10.2742\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 30s 121ms/step - loss: 10.0473 - val_loss: 9.8057\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 30s 121ms/step - loss: 10.0472 - val_loss: 10.8341\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 10.0176 - val_loss: 9.9869\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 10.0158 - val_loss: 9.9528\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 10.0260 - val_loss: 9.9759\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 10.0171 - val_loss: 9.7727\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 10.0347 - val_loss: 10.2694\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 10.0034 - val_loss: 9.7317\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 30s 121ms/step - loss: 9.9953 - val_loss: 10.1304\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 30s 121ms/step - loss: 9.9965 - val_loss: 9.7683\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 10.0021 - val_loss: 10.6205\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.9798 - val_loss: 9.9638\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 30s 121ms/step - loss: 10.0077 - val_loss: 9.7315\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 30s 121ms/step - loss: 10.0126 - val_loss: 9.9523\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 10.0043 - val_loss: 9.8770\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 30s 121ms/step - loss: 9.9691 - val_loss: 10.3953\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.9874 - val_loss: 10.2493\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.9943 - val_loss: 10.1982\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 30s 121ms/step - loss: 9.9925 - val_loss: 10.3306\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 30s 121ms/step - loss: 9.9800 - val_loss: 10.0219\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 30s 121ms/step - loss: 9.9879 - val_loss: 9.8991\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.9592 - val_loss: 10.7122\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 30s 121ms/step - loss: 9.9843 - val_loss: 11.1137\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.9623 - val_loss: 10.4149\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 30s 121ms/step - loss: 9.9467 - val_loss: 10.1256\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 30s 121ms/step - loss: 9.9458 - val_loss: 10.2707\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 30s 121ms/step - loss: 9.9393 - val_loss: 10.6688\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.9374 - val_loss: 9.9353\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 30s 121ms/step - loss: 9.9203 - val_loss: 10.3148\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.9141 - val_loss: 9.9602\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 30s 121ms/step - loss: 9.8818 - val_loss: 9.9614\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 30s 121ms/step - loss: 9.8924 - val_loss: 10.4922\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.8922 - val_loss: 10.2120\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.8877 - val_loss: 10.1212\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.8596 - val_loss: 10.0487\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 30s 121ms/step - loss: 9.8330 - val_loss: 10.6122\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 30s 121ms/step - loss: 9.8337 - val_loss: 10.0510\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.8233 - val_loss: 9.6448\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.8086 - val_loss: 9.8144\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.7915 - val_loss: 9.9822\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.7807 - val_loss: 10.0179\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 30s 121ms/step - loss: 9.7652 - val_loss: 9.7990\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.7751 - val_loss: 10.0495\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.7865 - val_loss: 9.6519\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.7256 - val_loss: 9.7205\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 30s 121ms/step - loss: 9.7187 - val_loss: 10.1132\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.7191 - val_loss: 9.6078\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 30s 121ms/step - loss: 9.6994 - val_loss: 9.6028\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 30s 121ms/step - loss: 9.6718 - val_loss: 10.7776\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 30s 121ms/step - loss: 9.6794 - val_loss: 10.3288\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.6453 - val_loss: 9.5596\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.6589 - val_loss: 9.6179\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.6466 - val_loss: 9.7598\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.6373 - val_loss: 9.7801\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 30s 121ms/step - loss: 9.6583 - val_loss: 9.8845\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.6249 - val_loss: 9.5327\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 30s 121ms/step - loss: 9.6112 - val_loss: 9.7064\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 30s 121ms/step - loss: 9.6036 - val_loss: 10.0299\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.5824 - val_loss: 9.6519\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.5836 - val_loss: 9.6037\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.5701 - val_loss: 9.4926\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 30s 121ms/step - loss: 9.5770 - val_loss: 9.4515\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 30s 121ms/step - loss: 9.5579 - val_loss: 9.6464\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 30s 121ms/step - loss: 9.5377 - val_loss: 9.5072\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.5368 - val_loss: 9.5214\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.5332 - val_loss: 9.6467\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 30s 121ms/step - loss: 9.5289 - val_loss: 9.5573\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 30s 121ms/step - loss: 9.5417 - val_loss: 9.4396\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 30s 121ms/step - loss: 9.5137 - val_loss: 9.6131\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.4890 - val_loss: 9.5059\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.4869 - val_loss: 9.4513\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.4865 - val_loss: 9.3973\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 30s 121ms/step - loss: 9.5002 - val_loss: 9.4191\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.4873 - val_loss: 9.8035\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.4812 - val_loss: 9.6591\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.4655 - val_loss: 9.4282\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 30s 121ms/step - loss: 9.4664 - val_loss: 9.4595\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 30s 121ms/step - loss: 9.4742 - val_loss: 9.6384\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 30s 121ms/step - loss: 9.4276 - val_loss: 9.5079\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 30s 121ms/step - loss: 9.4326 - val_loss: 9.6773\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.4182 - val_loss: 9.3475\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.4068 - val_loss: 9.2959\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 30s 121ms/step - loss: 9.4050 - val_loss: 9.5568\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 30s 121ms/step - loss: 9.4073 - val_loss: 9.3498\n",
      "\n",
      "Loss: 934.98%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 252)       2520      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 94, 94, 252)      1008      \n",
      " hNormalization)                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 94, 94, 252)       0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 33)        74877     \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 33)        9834      \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 90, 90, 33)       132       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 90, 90, 33)        0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 113)       33674     \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 113)       115034    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 86, 86, 113)      452       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 84, 84, 81)        82458     \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 84, 84, 81)       324       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 82, 82, 10)        7300      \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 82, 82, 10)       40        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 80, 80, 136)       12376     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 340,033\n",
      "Trainable params: 339,053\n",
      "Non-trainable params: 980\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 27s 99ms/step - loss: 2663.0718 - val_loss: 1724.0491\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 963.5466 - val_loss: 373.6414\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 125.6442 - val_loss: 39.2322\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 19.8825 - val_loss: 12.1388\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 10.8505 - val_loss: 10.1228\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 10.0378 - val_loss: 9.7222\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.9611 - val_loss: 9.7362\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.9475 - val_loss: 9.7476\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.9386 - val_loss: 9.7237\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.9390 - val_loss: 9.7062\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.9418 - val_loss: 9.7116\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.9349 - val_loss: 9.7255\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.9312 - val_loss: 9.6947\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.9299 - val_loss: 9.7526\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.9455 - val_loss: 9.6908\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.9428 - val_loss: 9.6867\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.9287 - val_loss: 9.6982\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.9382 - val_loss: 9.6984\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.9417 - val_loss: 9.7428\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.9307 - val_loss: 9.6924\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.9350 - val_loss: 9.6842\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.9418 - val_loss: 9.6937\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.9445 - val_loss: 9.7452\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.9390 - val_loss: 9.7435\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.9419 - val_loss: 9.7357\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.9381 - val_loss: 9.6784\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.9536 - val_loss: 9.7567\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.9545 - val_loss: 9.7026\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.9449 - val_loss: 9.7179\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.9439 - val_loss: 9.7045\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.9465 - val_loss: 9.7142\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.9442 - val_loss: 9.7656\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.9405 - val_loss: 9.7116\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.9469 - val_loss: 9.7168\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.9440 - val_loss: 9.7968\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.9383 - val_loss: 9.7275\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.9318 - val_loss: 9.7263\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.9454 - val_loss: 9.7398\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.9357 - val_loss: 9.7157\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.9477 - val_loss: 9.7333\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.9383 - val_loss: 9.7390\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.9331 - val_loss: 9.7324\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.9371 - val_loss: 9.7239\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.9311 - val_loss: 9.8709\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.9349 - val_loss: 9.7657\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.9374 - val_loss: 9.7753\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.9461 - val_loss: 10.0479\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.9378 - val_loss: 9.7861\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.9380 - val_loss: 9.8793\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.9340 - val_loss: 10.0979\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.9372 - val_loss: 9.7887\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.9151 - val_loss: 9.9079\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.9293 - val_loss: 9.7421\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.9346 - val_loss: 9.8512\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.9194 - val_loss: 9.8360\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.9083 - val_loss: 9.8428\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.9271 - val_loss: 9.9047\n",
      "Epoch 58/128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 23s 94ms/step - loss: 9.9004 - val_loss: 10.3592\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.9056 - val_loss: 10.0836\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.9097 - val_loss: 9.8771\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.9184 - val_loss: 9.9104\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.9097 - val_loss: 9.8768\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.9135 - val_loss: 10.0913\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.9108 - val_loss: 9.7836\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.9222 - val_loss: 9.9888\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.9112 - val_loss: 9.8304\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.8961 - val_loss: 9.8507\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.8978 - val_loss: 9.8152\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.8886 - val_loss: 9.8068\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.8861 - val_loss: 9.7941\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.8879 - val_loss: 9.6681\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.8911 - val_loss: 9.7926\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.8850 - val_loss: 10.2456\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.8907 - val_loss: 9.7659\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.8882 - val_loss: 9.7120\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.8808 - val_loss: 9.7397\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.8774 - val_loss: 9.7788\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.8707 - val_loss: 9.7318\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.8654 - val_loss: 9.9767\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.8681 - val_loss: 9.8336\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.8628 - val_loss: 10.1175\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.8431 - val_loss: 9.6953\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.8467 - val_loss: 9.7233\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.8392 - val_loss: 10.0126\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.8468 - val_loss: 9.7327\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.8307 - val_loss: 9.7223\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.8326 - val_loss: 9.7380\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.8242 - val_loss: 9.6881\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.8241 - val_loss: 9.8510\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.8139 - val_loss: 10.1099\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.8166 - val_loss: 9.8731\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.8127 - val_loss: 9.8593\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.8061 - val_loss: 9.7139\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.8107 - val_loss: 10.0091\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.7943 - val_loss: 9.8101\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.8075 - val_loss: 9.7911\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.7927 - val_loss: 9.7151\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.7908 - val_loss: 9.8176\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.7933 - val_loss: 9.6983\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.7792 - val_loss: 9.6824\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.7761 - val_loss: 9.7668\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.7821 - val_loss: 9.7732\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.7909 - val_loss: 9.6640\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.7601 - val_loss: 9.8454\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.7774 - val_loss: 9.6867\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.7565 - val_loss: 9.8152\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.7631 - val_loss: 9.7376\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.7504 - val_loss: 9.8428\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.7651 - val_loss: 9.7025\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.7512 - val_loss: 9.7934\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.7458 - val_loss: 9.8398\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.7396 - val_loss: 9.7766\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.7368 - val_loss: 9.6907\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.7311 - val_loss: 9.7138\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.7325 - val_loss: 9.6957\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.7284 - val_loss: 9.7439\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.7269 - val_loss: 9.8012\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.7201 - val_loss: 9.7729\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.7180 - val_loss: 9.7297\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.7284 - val_loss: 9.6880\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.7196 - val_loss: 9.8829\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.7136 - val_loss: 9.8869\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.7041 - val_loss: 9.7925\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.6996 - val_loss: 9.7884\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.7058 - val_loss: 9.7211\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.6936 - val_loss: 9.7808\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.6849 - val_loss: 9.6648\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.6998 - val_loss: 9.7081\n",
      "\n",
      "Loss: 970.81%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 126)       1260      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 126)       143010    \n",
      "                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch_normalization_1 (Batc  (None, 92, 92, 126)      504       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 33)        37455     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 90, 90, 33)       132       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 134)       39932     \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 88, 88, 134)      536       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 136)       164152    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 386,985\n",
      "Trainable params: 386,397\n",
      "Non-trainable params: 588\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 21s 77ms/step - loss: 1197.2500 - val_loss: 276.7130\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 12.3064 - val_loss: 24.6313\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.9138 - val_loss: 15.0784\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.7657 - val_loss: 12.9357\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.6518 - val_loss: 14.4845\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.5033 - val_loss: 12.7282\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.5392 - val_loss: 11.6674\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.4467 - val_loss: 10.9631\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.4273 - val_loss: 11.3402\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.4068 - val_loss: 10.6616\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.4204 - val_loss: 10.8354\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.3108 - val_loss: 10.5630\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.2914 - val_loss: 10.9644\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.3063 - val_loss: 11.1982\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.3229 - val_loss: 11.0173\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.2451 - val_loss: 10.2328\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.2697 - val_loss: 10.6119\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.2442 - val_loss: 10.9228\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.2315 - val_loss: 10.0447\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.1913 - val_loss: 10.4793\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.2024 - val_loss: 10.2491\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.2045 - val_loss: 11.7212\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.1894 - val_loss: 10.8640\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.2186 - val_loss: 10.0721\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.1742 - val_loss: 10.8672\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.1899 - val_loss: 10.4424\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.1818 - val_loss: 9.9410\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.1730 - val_loss: 10.0055\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.1442 - val_loss: 10.8090\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.1308 - val_loss: 10.0301\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.1249 - val_loss: 10.0162\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.1427 - val_loss: 9.9814\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.1633 - val_loss: 10.0870\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 10.1334 - val_loss: 10.1020\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.1853 - val_loss: 10.3362\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.1592 - val_loss: 10.1868\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.1178 - val_loss: 10.4850\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.1587 - val_loss: 10.4177\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.1359 - val_loss: 10.2306\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.1295 - val_loss: 10.6433\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.1180 - val_loss: 10.3028\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.1298 - val_loss: 10.1540\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.1293 - val_loss: 10.0769\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.1130 - val_loss: 10.1897\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.1624 - val_loss: 9.9181\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.1108 - val_loss: 10.0150\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.1502 - val_loss: 10.1202\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.1429 - val_loss: 10.4833\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.1231 - val_loss: 10.1685\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.1343 - val_loss: 10.3847\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.1260 - val_loss: 10.2153\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.1076 - val_loss: 9.9097\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.1224 - val_loss: 9.9952\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.1193 - val_loss: 10.2277\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.1212 - val_loss: 10.0620\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.0877 - val_loss: 9.9273\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.1645 - val_loss: 9.9027\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.1081 - val_loss: 10.2028\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.1238 - val_loss: 9.9339\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.0984 - val_loss: 9.9785\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.1210 - val_loss: 10.3061\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.0961 - val_loss: 9.8537\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.0954 - val_loss: 10.0417\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.0808 - val_loss: 9.9607\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.0993 - val_loss: 10.0338\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.1152 - val_loss: 10.0699\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.1138 - val_loss: 10.2854\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.1183 - val_loss: 10.1038\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.0978 - val_loss: 10.1962\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.0746 - val_loss: 10.0458\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.1057 - val_loss: 9.8800\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.1085 - val_loss: 10.6427\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.0741 - val_loss: 10.3567\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.0918 - val_loss: 9.9378\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.0985 - val_loss: 9.9602\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.0891 - val_loss: 10.0818\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 10.0806 - val_loss: 9.9876\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.0853 - val_loss: 9.8679\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.0729 - val_loss: 10.2017\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.0975 - val_loss: 12.6380\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.0649 - val_loss: 10.1494\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.0781 - val_loss: 9.9213\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.0870 - val_loss: 10.6036\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.0876 - val_loss: 10.3490\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.0934 - val_loss: 10.2212\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.0633 - val_loss: 9.9105\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.0675 - val_loss: 10.0233\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.0812 - val_loss: 10.0201\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.0756 - val_loss: 10.1505\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.0556 - val_loss: 10.1575\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.0580 - val_loss: 9.8966\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.0874 - val_loss: 10.0542\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 10.0716 - val_loss: 10.6973\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.0575 - val_loss: 10.4823\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.0622 - val_loss: 10.3765\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.0623 - val_loss: 9.9295\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.0777 - val_loss: 11.3123\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.0725 - val_loss: 9.8769\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.0432 - val_loss: 9.9043\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.0596 - val_loss: 9.9973\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.0894 - val_loss: 11.0843\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.0632 - val_loss: 10.0178\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.0456 - val_loss: 10.3578\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 10.1055 - val_loss: 11.0033\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 10.0559 - val_loss: 11.0441\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.0793 - val_loss: 9.9821\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.0835 - val_loss: 10.3199\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.0803 - val_loss: 10.5961\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.0759 - val_loss: 10.0765\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 10.0726 - val_loss: 9.8442\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.0828 - val_loss: 10.1783\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.0500 - val_loss: 9.9953\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.0682 - val_loss: 9.8717\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.0729 - val_loss: 9.9762\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.0560 - val_loss: 10.1751\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.0452 - val_loss: 9.9468\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.0760 - val_loss: 10.2231\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.0520 - val_loss: 10.2960\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 10.0617 - val_loss: 10.5513\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.0680 - val_loss: 10.4201\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.0440 - val_loss: 10.0578\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.0812 - val_loss: 11.0737\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.0673 - val_loss: 10.0436\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.0280 - val_loss: 10.1406\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 10.1022 - val_loss: 9.9905\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 10.0472 - val_loss: 9.9075\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.0342 - val_loss: 9.9506\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.0433 - val_loss: 9.9551\n",
      "\n",
      "Loss: 995.51%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 237)       2370      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 237)       505758    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 92, 92, 237)      948       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 92, 92, 237)       0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 8)         17072     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 90, 90, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 90, 90, 8)         0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 186)       13578     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 88, 88, 186)      744       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 88, 88, 186)       0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 256)       428800    \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 84, 84, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 84, 84, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 82, 82, 61)        140605    \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 82, 82, 61)       244       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 80, 80, 136)       74800     \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 80, 80, 136)      544       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 78, 78, 136)       166600    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,943,203\n",
      "Trainable params: 1,941,433\n",
      "Non-trainable params: 1,770\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 55s 208ms/step - loss: 1196.1179 - val_loss: 174.0853\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 10.7367 - val_loss: 17.0426\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 10.1289 - val_loss: 13.6736\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 10.1995 - val_loss: 25.7120\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 10.1298 - val_loss: 11.0017\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 10.0862 - val_loss: 11.6960\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 10.1087 - val_loss: 12.8251\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 10.0879 - val_loss: 10.9691\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 10.1255 - val_loss: 16.2002\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 10.1263 - val_loss: 11.1683\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 10.0778 - val_loss: 10.2323\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 10.1227 - val_loss: 11.0823\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 10.1296 - val_loss: 12.8640\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 10.0885 - val_loss: 10.1348\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 10.1123 - val_loss: 15.2324\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 10.0994 - val_loss: 9.8478\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 10.1074 - val_loss: 11.8746\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 10.0892 - val_loss: 9.9602\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 10.0741 - val_loss: 10.5702\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 10.0573 - val_loss: 12.4866\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 10.1198 - val_loss: 11.4065\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 10.0874 - val_loss: 10.3258\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 10.1210 - val_loss: 11.0326\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 10.0997 - val_loss: 10.5080\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 10.1280 - val_loss: 9.9230\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 10.1024 - val_loss: 10.1738\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 10.1021 - val_loss: 12.5775\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 10.1055 - val_loss: 11.1894\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 10.0818 - val_loss: 10.8384\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 10.0813 - val_loss: 10.5797\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 10.0954 - val_loss: 10.7478\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 10.0769 - val_loss: 9.9521\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 10.1097 - val_loss: 10.4812\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 10.1053 - val_loss: 10.1911\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 10.1260 - val_loss: 13.9446\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 10.0793 - val_loss: 11.0910\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 10.0990 - val_loss: 10.0879\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 10.0868 - val_loss: 10.6053\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 10.0911 - val_loss: 10.4241\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 10.1074 - val_loss: 10.8858\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 10.0710 - val_loss: 10.1085\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 10.1097 - val_loss: 10.2981\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 10.0973 - val_loss: 11.1368\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 10.1036 - val_loss: 10.0383\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 10.1041 - val_loss: 9.9363\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 10.0722 - val_loss: 9.8214\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 10.0801 - val_loss: 10.7364\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 10.0711 - val_loss: 10.3144\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 10.0609 - val_loss: 9.9423\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 10.0885 - val_loss: 10.3010\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 10.0297 - val_loss: 10.4136\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 10.0134 - val_loss: 9.9111\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 10.0473 - val_loss: 10.3098\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 10.0743 - val_loss: 9.9712\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 10.0697 - val_loss: 9.7244\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 10.0664 - val_loss: 10.4427\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 10.0302 - val_loss: 9.9278\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 10.0524 - val_loss: 10.5091\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 10.0261 - val_loss: 11.0180\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 10.0136 - val_loss: 10.2445\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 10.0054 - val_loss: 10.0539\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 10.0290 - val_loss: 9.8156\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 10.0495 - val_loss: 10.0616\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 10.0271 - val_loss: 10.5042\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 10.0224 - val_loss: 10.6146\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 10.0320 - val_loss: 9.7983\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 10.0013 - val_loss: 10.5951\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 10.0198 - val_loss: 10.1167\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 10.0143 - val_loss: 10.1572\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 9.9583 - val_loss: 10.8516\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 9.9897 - val_loss: 11.5283\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 9.9997 - val_loss: 10.3704\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 9.9845 - val_loss: 10.7704\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 9.9926 - val_loss: 9.9406\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 9.9705 - val_loss: 13.8536\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 9.9692 - val_loss: 10.2679\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 9.9651 - val_loss: 9.8226\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 9.9998 - val_loss: 10.4132\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 9.9692 - val_loss: 10.0203\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 9.9698 - val_loss: 10.0931\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 9.9937 - val_loss: 9.8518\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 9.9758 - val_loss: 10.9798\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 9.9799 - val_loss: 10.1126\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 9.9563 - val_loss: 10.2771\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 9.9373 - val_loss: 9.8127\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 9.9426 - val_loss: 10.1719\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 9.9712 - val_loss: 9.7767\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 9.9170 - val_loss: 10.5829\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 9.9644 - val_loss: 12.1679\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 9.9539 - val_loss: 10.6892\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 9.9262 - val_loss: 10.0852\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 9.9195 - val_loss: 9.9522\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 9.9356 - val_loss: 9.9176\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 9.9567 - val_loss: 10.1743\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 9.9032 - val_loss: 10.5107\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 9.9123 - val_loss: 10.4566\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 9.9133 - val_loss: 10.8901\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 9.8922 - val_loss: 10.0790\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 9.9224 - val_loss: 10.0736\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 9.8669 - val_loss: 11.2104\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 9.8860 - val_loss: 10.6807\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 9.8690 - val_loss: 10.5500\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 9.8551 - val_loss: 10.1812\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 9.8637 - val_loss: 10.1114\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 9.9037 - val_loss: 9.7625\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 9.8650 - val_loss: 10.8485\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 9.8701 - val_loss: 11.2200\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 9.8579 - val_loss: 10.1627\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 9.8532 - val_loss: 10.0538\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 9.8471 - val_loss: 11.2295\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 9.8571 - val_loss: 11.3327\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 9.8309 - val_loss: 10.1025\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 9.8128 - val_loss: 13.0478\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 9.8339 - val_loss: 10.3465\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 9.8327 - val_loss: 10.3835\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 9.8263 - val_loss: 10.5582\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 9.8448 - val_loss: 10.0376\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 9.8534 - val_loss: 10.5178\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 9.8199 - val_loss: 10.1592\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 9.8299 - val_loss: 9.9246\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 9.7853 - val_loss: 10.1824\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 9.8244 - val_loss: 10.0059\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 9.7854 - val_loss: 10.7879\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 9.7990 - val_loss: 11.6187\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 9.7767 - val_loss: 11.1463\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 9.7780 - val_loss: 10.6092\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 9.7458 - val_loss: 10.4202\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 50s 201ms/step - loss: 9.7403 - val_loss: 9.9655\n",
      "\n",
      "Loss: 996.55%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 229)       2290      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 94, 94, 229)      916       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dropout_1 (Dropout)         (None, 94, 94, 229)       0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 248)       511376    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 92, 92, 248)      992       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 92, 92, 248)       0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 201)       448833    \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 201)       363810    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 88, 88, 201)      804       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 88, 88, 201)       0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 94)        170140    \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 86, 86, 94)       376       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 84, 84, 104)       88088     \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 84, 84, 104)      416       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 82, 82, 68)        63716     \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 82, 82, 68)       272       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 80, 80, 136)       83368     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,735,401\n",
      "Trainable params: 1,733,511\n",
      "Non-trainable params: 1,890\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 60s 223ms/step - loss: 1567.0193 - val_loss: 103.8088\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 20.5482 - val_loss: 10.1727\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 10.1796 - val_loss: 9.7975\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 10.1157 - val_loss: 9.9155\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 10.0993 - val_loss: 9.8465\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 10.0539 - val_loss: 9.7454\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 10.0674 - val_loss: 9.8503\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 10.0325 - val_loss: 9.9279\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 10.0394 - val_loss: 10.0411\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 10.0317 - val_loss: 9.9260\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 10.0285 - val_loss: 10.0871\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 10.0271 - val_loss: 10.4720\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 10.0324 - val_loss: 9.9846\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 10.0308 - val_loss: 10.2578\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 10.0415 - val_loss: 10.3077\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 10.0281 - val_loss: 9.8102\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 10.0505 - val_loss: 10.1343\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 10.0444 - val_loss: 10.1145\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 10.0319 - val_loss: 9.9013\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 10.0231 - val_loss: 10.0767\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 10.0351 - val_loss: 10.0250\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 10.0483 - val_loss: 10.4659\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 10.0215 - val_loss: 10.1577\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 10.0281 - val_loss: 9.8326\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 10.0513 - val_loss: 9.8946\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 10.0647 - val_loss: 9.9174\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 10.0362 - val_loss: 10.3986\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 10.0387 - val_loss: 9.7969\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 10.0467 - val_loss: 9.8620\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 10.0301 - val_loss: 9.9848\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 10.0345 - val_loss: 10.0065\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 10.0432 - val_loss: 9.7286\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 10.0419 - val_loss: 9.9995\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 10.0365 - val_loss: 9.8048\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 10.0342 - val_loss: 9.8632\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 10.0558 - val_loss: 9.8412\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 10.0110 - val_loss: 9.8436\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 10.0259 - val_loss: 9.8805\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 10.0131 - val_loss: 10.0069\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 10.0017 - val_loss: 9.8369\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.9988 - val_loss: 10.0004\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 10.0109 - val_loss: 9.8917\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.9867 - val_loss: 9.8131\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.9693 - val_loss: 9.7560\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.9698 - val_loss: 10.0426\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.9658 - val_loss: 9.8751\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.9444 - val_loss: 9.8071\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.9707 - val_loss: 10.1529\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.9240 - val_loss: 9.6662\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.9419 - val_loss: 9.7165\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.8936 - val_loss: 10.3003\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.8874 - val_loss: 9.6824\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.8816 - val_loss: 9.7522\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.8764 - val_loss: 9.7844\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.8677 - val_loss: 10.1631\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.8281 - val_loss: 10.5218\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.8244 - val_loss: 10.2673\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.8162 - val_loss: 9.8247\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.7817 - val_loss: 9.8062\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.7717 - val_loss: 9.6845\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.7583 - val_loss: 9.8166\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.7439 - val_loss: 9.6931\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.7492 - val_loss: 10.0365\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.7134 - val_loss: 9.8473\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.7098 - val_loss: 9.6521\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.6908 - val_loss: 9.8099\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.6859 - val_loss: 9.7164\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.6665 - val_loss: 9.8127\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.6681 - val_loss: 9.8283\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.6440 - val_loss: 9.5542\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.6490 - val_loss: 9.7098\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.6423 - val_loss: 9.6933\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.6316 - val_loss: 9.5463\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.5957 - val_loss: 9.5655\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.6091 - val_loss: 9.6202\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.5755 - val_loss: 9.7484\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.5879 - val_loss: 9.6233\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.5705 - val_loss: 9.5915\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.5683 - val_loss: 9.5503\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.5395 - val_loss: 9.5860\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.5345 - val_loss: 9.6722\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.5413 - val_loss: 9.8284\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.5016 - val_loss: 9.7235\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.5136 - val_loss: 9.5179\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.5056 - val_loss: 9.6151\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.5039 - val_loss: 9.5751\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.5132 - val_loss: 9.6856\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.4902 - val_loss: 9.6371\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.4880 - val_loss: 9.4796\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.4577 - val_loss: 9.5034\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.4529 - val_loss: 9.7350\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.4911 - val_loss: 9.4894\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.4305 - val_loss: 9.7665\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.4402 - val_loss: 9.5640\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.4323 - val_loss: 9.5481\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.4193 - val_loss: 9.6577\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.4039 - val_loss: 9.5330\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.3912 - val_loss: 9.4841\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.4142 - val_loss: 9.6268\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.4030 - val_loss: 9.5718\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.3774 - val_loss: 9.7135\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.3912 - val_loss: 9.7131\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.3714 - val_loss: 9.5098\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.3979 - val_loss: 9.5272\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.3524 - val_loss: 9.6547\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.3963 - val_loss: 9.5077\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.3713 - val_loss: 9.6249\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.3750 - val_loss: 9.4437\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.3486 - val_loss: 9.7315\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.3367 - val_loss: 10.1038\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.3484 - val_loss: 9.7507\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.3643 - val_loss: 9.8606\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.3296 - val_loss: 9.6355\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.3341 - val_loss: 9.6561\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.3279 - val_loss: 9.6021\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.3115 - val_loss: 9.6640\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.2985 - val_loss: 9.5934\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.3112 - val_loss: 9.7179\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.3159 - val_loss: 9.8950\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.3251 - val_loss: 9.4924\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.2737 - val_loss: 9.8026\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.2959 - val_loss: 9.5518\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.3034 - val_loss: 9.6841\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.2977 - val_loss: 9.7234\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.2941 - val_loss: 9.7271\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.2713 - val_loss: 9.6378\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.2487 - val_loss: 9.5988\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 9.2589 - val_loss: 9.6127\n",
      "\n",
      "Loss: 961.27%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 233)       2330      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 94, 94, 233)      932       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 94, 94, 233)       0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 82)        172036    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 92, 92, 82)       328       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 92, 92, 82)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 127)       93853     \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 90, 90, 127)      508       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 90, 90, 127)       0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 88)        100672    \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 88, 88, 88)       352       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 8)         6344      \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 86, 86, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 84, 84, 61)        4453      \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 84, 84, 61)       244       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 82, 82, 136)       74800     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 456,888\n",
      "Trainable params: 455,688\n",
      "Non-trainable params: 1,200\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 31s 113ms/step - loss: 1636.1495 - val_loss: 194.2820\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 26.0564 - val_loss: 12.2278\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.1653 - val_loss: 10.5989\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0577 - val_loss: 10.7413\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0817 - val_loss: 11.3628\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0491 - val_loss: 10.5588\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0150 - val_loss: 9.9280\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0413 - val_loss: 9.8387\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0212 - val_loss: 10.0200\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0229 - val_loss: 10.4627\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0195 - val_loss: 9.9526\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0336 - val_loss: 10.7840\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0199 - val_loss: 10.2108\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0239 - val_loss: 10.5460\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0259 - val_loss: 9.7902\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0112 - val_loss: 9.8860\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0362 - val_loss: 10.5706\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0281 - val_loss: 9.8167\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0161 - val_loss: 9.9655\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0298 - val_loss: 10.0798\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0321 - val_loss: 10.5439\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0430 - val_loss: 9.9303\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0150 - val_loss: 9.8282\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0391 - val_loss: 9.8609\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0308 - val_loss: 9.8548\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0162 - val_loss: 10.0073\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0456 - val_loss: 9.8287\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0512 - val_loss: 9.9514\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0438 - val_loss: 10.0973\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0492 - val_loss: 9.8111\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0229 - val_loss: 9.7828\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0290 - val_loss: 10.2081\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0265 - val_loss: 9.8577\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0283 - val_loss: 10.6313\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0320 - val_loss: 9.8133\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0421 - val_loss: 9.9025\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0217 - val_loss: 9.7165\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0412 - val_loss: 9.8959\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0194 - val_loss: 9.7731\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0409 - val_loss: 9.8488\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0206 - val_loss: 9.8588\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0236 - val_loss: 10.1545\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0255 - val_loss: 9.9380\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0148 - val_loss: 9.8116\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0537 - val_loss: 9.7526\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0206 - val_loss: 9.8278\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0197 - val_loss: 9.9547\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0196 - val_loss: 10.0416\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0167 - val_loss: 10.1717\n",
      "Epoch 50/128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0091 - val_loss: 9.9132\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0292 - val_loss: 9.7649\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0161 - val_loss: 9.8653\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0063 - val_loss: 9.8354\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0288 - val_loss: 9.9727\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0355 - val_loss: 9.8110\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0100 - val_loss: 9.8111\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0350 - val_loss: 9.8828\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0049 - val_loss: 9.9203\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0313 - val_loss: 9.8332\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0337 - val_loss: 9.9107\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0109 - val_loss: 9.9390\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0060 - val_loss: 9.9052\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0213 - val_loss: 9.7818\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0041 - val_loss: 9.7297\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0124 - val_loss: 9.8825\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0017 - val_loss: 9.7585\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0064 - val_loss: 9.7558\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0186 - val_loss: 9.8491\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0281 - val_loss: 9.9674\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0233 - val_loss: 9.8523\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0064 - val_loss: 9.7450\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0162 - val_loss: 9.7362\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0033 - val_loss: 9.9892\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0106 - val_loss: 9.8355\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0043 - val_loss: 9.8483\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 9.9999 - val_loss: 10.1538\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0196 - val_loss: 9.9408\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 9.9980 - val_loss: 9.9953\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 9.9885 - val_loss: 10.1326\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 9.9874 - val_loss: 10.0286\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 9.9843 - val_loss: 10.2561\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 9.9859 - val_loss: 10.0829\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 9.9888 - val_loss: 10.0093\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 9.9939 - val_loss: 10.2503\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 9.9964 - val_loss: 9.7838\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0010 - val_loss: 10.1468\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 9.9906 - val_loss: 9.7954\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 9.9967 - val_loss: 10.1111\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 9.9877 - val_loss: 9.8114\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 9.9865 - val_loss: 9.8235\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 9.9724 - val_loss: 9.9754\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 9.9720 - val_loss: 9.8980\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 9.9729 - val_loss: 9.9415\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 9.9922 - val_loss: 10.1430\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 9.9720 - val_loss: 9.8760\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 9.9622 - val_loss: 10.0363\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 9.9793 - val_loss: 10.1284\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 9.9612 - val_loss: 9.9291\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 9.9807 - val_loss: 9.7831\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 9.9958 - val_loss: 9.8432\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0013 - val_loss: 10.1243\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 9.9700 - val_loss: 9.8520\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 9.9850 - val_loss: 10.3253\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 9.9761 - val_loss: 9.7435\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 9.9701 - val_loss: 10.0146\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 9.9810 - val_loss: 10.1196\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 9.9743 - val_loss: 9.8031\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 9.9784 - val_loss: 9.9308\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 9.9749 - val_loss: 9.8942\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 9.9741 - val_loss: 9.8808\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 9.9691 - val_loss: 10.2108\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 9.9603 - val_loss: 9.8085\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 9.9513 - val_loss: 9.7681\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 9.9730 - val_loss: 9.9090\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 9.9550 - val_loss: 9.8559\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 9.9787 - val_loss: 10.2522\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 9.9654 - val_loss: 9.9327\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 9.9521 - val_loss: 9.9927\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 9.9540 - val_loss: 10.1165\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 9.9480 - val_loss: 9.8022\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 9.9444 - val_loss: 10.2436\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 9.9421 - val_loss: 10.0859\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 9.9451 - val_loss: 10.0471\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 9.9525 - val_loss: 10.0254\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 9.9602 - val_loss: 9.8058\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 9.9429 - val_loss: 9.9201\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 9.9281 - val_loss: 14.5759\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 9.9372 - val_loss: 9.9437\n",
      "\n",
      "Loss: 994.37%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 250)       2500      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 94, 94, 250)      1000      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 94, 94, 250)       0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 105)       236355    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 92, 92, 105)      420       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 92, 92, 105)       0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 248)       234608    \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 248)       553784    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 88, 88, 248)      992       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 88, 88, 248)       0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 122)       272426    \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 86, 86, 122)      488       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 84, 84, 60)        65940     \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 84, 84, 60)       240       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 82, 82, 89)        48149     \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 82, 82, 89)       356       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 80, 80, 136)       109072    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,526,334\n",
      "Trainable params: 1,524,584\n",
      "Non-trainable params: 1,750\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 52s 196ms/step - loss: 1414.2069 - val_loss: 58.3582\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 13.6063 - val_loss: 10.5691\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 10.1823 - val_loss: 10.3827\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 10.1146 - val_loss: 10.0887\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 10.0681 - val_loss: 9.9945\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 10.0823 - val_loss: 10.3601\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 10.0329 - val_loss: 9.9726\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 10.0541 - val_loss: 10.3181\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 10.0420 - val_loss: 10.0982\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 10.0599 - val_loss: 9.8447\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 10.0651 - val_loss: 9.9964\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 10.0368 - val_loss: 9.7891\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 10.0608 - val_loss: 9.9707\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 10.0486 - val_loss: 10.8534\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 10.0527 - val_loss: 10.0623\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 10.0506 - val_loss: 10.0301\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 10.0417 - val_loss: 9.7633\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 10.0769 - val_loss: 10.0204\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 10.0404 - val_loss: 9.9099\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 10.0625 - val_loss: 9.9968\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 10.0600 - val_loss: 9.9743\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 10.0421 - val_loss: 9.9853\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 10.0458 - val_loss: 10.0846\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 10.0535 - val_loss: 9.8790\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 10.0367 - val_loss: 9.7693\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 10.0361 - val_loss: 11.6884\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 10.0597 - val_loss: 10.0511\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 10.0549 - val_loss: 10.0141\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 10.0852 - val_loss: 9.8863\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 10.0506 - val_loss: 10.1765\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 10.0178 - val_loss: 9.7497\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 10.0242 - val_loss: 9.9142\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 10.0317 - val_loss: 9.7853\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 10.0156 - val_loss: 9.7187\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.9918 - val_loss: 9.7618\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.9953 - val_loss: 9.9814\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.9899 - val_loss: 9.8474\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.9937 - val_loss: 10.0841\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.9710 - val_loss: 9.7131\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.9606 - val_loss: 10.3198\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.9318 - val_loss: 9.9982\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.9166 - val_loss: 9.6528\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.9174 - val_loss: 9.9179\n",
      "Epoch 44/128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 47s 189ms/step - loss: 9.8608 - val_loss: 10.1477\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.8471 - val_loss: 9.8554\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.8333 - val_loss: 10.6071\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.8348 - val_loss: 10.6830\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.7767 - val_loss: 10.2516\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.7934 - val_loss: 9.7600\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.7573 - val_loss: 9.9119\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.7512 - val_loss: 9.5181\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.7002 - val_loss: 9.8607\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.7168 - val_loss: 9.5914\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.6871 - val_loss: 9.6518\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.6673 - val_loss: 9.6741\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.6589 - val_loss: 9.7280\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.6023 - val_loss: 9.4750\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.5998 - val_loss: 9.9740\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.5874 - val_loss: 9.3998\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.6001 - val_loss: 10.3487\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.5697 - val_loss: 9.5697\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.5412 - val_loss: 9.7101\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.5177 - val_loss: 9.5318\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.5270 - val_loss: 9.5739\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.6004 - val_loss: 34.8533\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.5172 - val_loss: 10.3334\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.4997 - val_loss: 9.7725\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.4940 - val_loss: 9.4764\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.4367 - val_loss: 9.7411\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.4505 - val_loss: 9.4292\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.4470 - val_loss: 9.5523\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.4247 - val_loss: 9.7541\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.4078 - val_loss: 10.5368\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.4029 - val_loss: 9.2500\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.3712 - val_loss: 9.2821\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.3816 - val_loss: 9.5928\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.3742 - val_loss: 10.2946\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.3564 - val_loss: 9.3346\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.3590 - val_loss: 9.4885\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.3307 - val_loss: 9.3875\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.2925 - val_loss: 9.4136\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.3273 - val_loss: 10.4394\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.3009 - val_loss: 9.5496\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.3138 - val_loss: 9.3054\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.2930 - val_loss: 10.0816\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.2993 - val_loss: 9.4972\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.2864 - val_loss: 9.6683\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.2345 - val_loss: 9.7273\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.2099 - val_loss: 9.2948\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.2410 - val_loss: 9.2290\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.2609 - val_loss: 9.5266\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.2208 - val_loss: 9.7092\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.1832 - val_loss: 9.5841\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.1980 - val_loss: 9.1245\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.1826 - val_loss: 9.1751\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.1849 - val_loss: 9.6489\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.1696 - val_loss: 9.1398\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.1498 - val_loss: 9.1723\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.1572 - val_loss: 9.5303\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.1608 - val_loss: 9.3612\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.1729 - val_loss: 9.1909\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.1270 - val_loss: 9.1344\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.1113 - val_loss: 9.4112\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.1008 - val_loss: 9.5769\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.0948 - val_loss: 9.1602\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.0863 - val_loss: 9.1791\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.0862 - val_loss: 9.3851\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.0684 - val_loss: 9.1813\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.0910 - val_loss: 9.2097\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.0955 - val_loss: 9.3210\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.0678 - val_loss: 9.2278\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.0654 - val_loss: 9.1082\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.0630 - val_loss: 9.1381\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 8.9991 - val_loss: 9.2341\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.0541 - val_loss: 9.0777\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.0188 - val_loss: 9.5455\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.0501 - val_loss: 9.1673\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 8.9909 - val_loss: 9.3987\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.0401 - val_loss: 9.1669\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.0240 - val_loss: 9.1249\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 8.9957 - val_loss: 9.4849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 122/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 8.9967 - val_loss: 9.6756\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 9.0071 - val_loss: 9.0758\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 8.9877 - val_loss: 9.2744\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 8.9962 - val_loss: 9.2222\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 8.9708 - val_loss: 9.2552\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 8.9312 - val_loss: 9.2415\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 47s 189ms/step - loss: 8.9587 - val_loss: 9.1679\n",
      "\n",
      "Loss: 916.79%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 256)       2560      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 94, 94, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 94, 94, 256)       0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 167)       384935    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 92, 92, 167)      668       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 92, 92, 167)       0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 256)       385024    \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 88, 88, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 88, 88, 256)       0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 169)       389545    \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 86, 86, 169)      676       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 84, 84, 61)        92842     \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 84, 84, 61)       244       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 82, 82, 108)       59400     \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 82, 82, 108)      432       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 80, 80, 136)       132328    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,040,786\n",
      "Trainable params: 2,038,750\n",
      "Non-trainable params: 2,036\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 59s 223ms/step - loss: 1300.6927 - val_loss: 71.2502\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 11.8619 - val_loss: 13.1463\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 10.2360 - val_loss: 10.3581\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 10.1866 - val_loss: 10.3978\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 10.1224 - val_loss: 12.4330\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 10.1137 - val_loss: 10.5791\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 10.1349 - val_loss: 10.3637\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 10.0820 - val_loss: 10.1797\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 10.0745 - val_loss: 10.3240\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 10.1199 - val_loss: 10.3327\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 10.1047 - val_loss: 10.4131\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 10.1142 - val_loss: 10.2475\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 10.0857 - val_loss: 9.9861\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 10.0860 - val_loss: 10.3380\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 10.0859 - val_loss: 10.1947\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 10.1098 - val_loss: 10.0751\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 10.1127 - val_loss: 10.1662\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 10.1175 - val_loss: 10.0509\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 10.1142 - val_loss: 10.3639\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 10.0980 - val_loss: 10.4400\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 10.0745 - val_loss: 10.5433\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 10.1035 - val_loss: 10.1457\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 10.0722 - val_loss: 9.9126\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 10.0885 - val_loss: 10.1330\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 10.0624 - val_loss: 10.0302\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 10.1058 - val_loss: 9.8217\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 10.0793 - val_loss: 11.3324\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 10.0793 - val_loss: 9.8652\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 10.0381 - val_loss: 9.8453\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 10.0453 - val_loss: 9.9122\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 10.0123 - val_loss: 10.1385\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 10.0515 - val_loss: 10.1008\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 10.0301 - val_loss: 9.8807\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 9.9790 - val_loss: 10.0862\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 9.9707 - val_loss: 10.2282\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 9.9729 - val_loss: 10.0509\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 9.9705 - val_loss: 10.1739\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 9.9846 - val_loss: 10.2497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 9.9673 - val_loss: 9.9556\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 9.9199 - val_loss: 9.7854\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 9.9058 - val_loss: 10.3836\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 9.9176 - val_loss: 9.6851\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 9.8810 - val_loss: 10.6484\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 9.8868 - val_loss: 9.7942\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 9.8383 - val_loss: 9.7339\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 9.8132 - val_loss: 9.7167\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 9.7728 - val_loss: 9.7482\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 9.7827 - val_loss: 9.7116\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 9.7313 - val_loss: 9.8122\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 9.7105 - val_loss: 9.8177\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 9.7066 - val_loss: 9.7770\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 9.6539 - val_loss: 9.6975\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 9.6512 - val_loss: 9.5807\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 9.6379 - val_loss: 9.4934\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 9.6360 - val_loss: 9.5472\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 9.6201 - val_loss: 9.7327\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 9.5908 - val_loss: 10.0488\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 9.5917 - val_loss: 9.6477\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 9.5326 - val_loss: 9.3716\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 9.5188 - val_loss: 9.6948\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 9.5115 - val_loss: 9.4933\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 9.5156 - val_loss: 9.4045\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 9.4660 - val_loss: 9.7227\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 9.4447 - val_loss: 9.3899\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 9.4389 - val_loss: 10.1386\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 9.4266 - val_loss: 9.5520\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 9.4274 - val_loss: 9.5120\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 9.3749 - val_loss: 10.3268\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 9.3748 - val_loss: 9.2313\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 9.3552 - val_loss: 9.4761\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 9.3170 - val_loss: 9.4918\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 9.3451 - val_loss: 9.7175\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 9.3119 - val_loss: 9.3555\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 9.2945 - val_loss: 9.2198\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 9.2724 - val_loss: 9.7793\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 9.2713 - val_loss: 9.4697\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 9.2483 - val_loss: 9.2862\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 9.2339 - val_loss: 9.8596\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 9.2125 - val_loss: 9.5594\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 9.2058 - val_loss: 9.3188\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 9.2236 - val_loss: 9.1649\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 9.2123 - val_loss: 9.2386\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 9.1566 - val_loss: 9.1726\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 9.1559 - val_loss: 9.3670\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 9.1606 - val_loss: 9.8354\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 9.1450 - val_loss: 9.6036\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 9.1232 - val_loss: 9.0669\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 9.1327 - val_loss: 9.1550\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 9.1104 - val_loss: 9.0776\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 9.0893 - val_loss: 9.2375\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 9.0698 - val_loss: 9.2588\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 9.0646 - val_loss: 9.2918\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 9.0667 - val_loss: 9.2448\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 9.0683 - val_loss: 9.1578\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 9.0240 - val_loss: 9.0328\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 9.0016 - val_loss: 9.4145\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 9.0239 - val_loss: 9.0920\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 9.0094 - val_loss: 9.0157\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 9.0012 - val_loss: 9.0753\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 9.0048 - val_loss: 9.0743\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 8.9898 - val_loss: 9.0297\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 8.9598 - val_loss: 9.1789\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 8.9505 - val_loss: 9.0833\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 8.9334 - val_loss: 9.2271\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 8.9638 - val_loss: 9.2587\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 8.9213 - val_loss: 9.5424\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 8.9267 - val_loss: 9.0680\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 8.9151 - val_loss: 9.1729\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 8.9224 - val_loss: 9.1204\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 8.9236 - val_loss: 9.0658\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 8.8727 - val_loss: 9.0644\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 8.8939 - val_loss: 9.2545\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 8.8891 - val_loss: 9.1250\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 8.8433 - val_loss: 9.0333\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 8.8552 - val_loss: 9.1079\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 8.8372 - val_loss: 9.2373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 8.8487 - val_loss: 9.1987\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 8.8527 - val_loss: 9.9425\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 8.8775 - val_loss: 9.8486\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 8.8144 - val_loss: 8.9081\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 8.8271 - val_loss: 8.9662\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 8.7840 - val_loss: 8.9544\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 8.8056 - val_loss: 9.2230\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 8.7694 - val_loss: 9.2751\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 8.7920 - val_loss: 8.8884\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 8.7887 - val_loss: 9.2031\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 8.7801 - val_loss: 9.0762\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 8.7661 - val_loss: 9.0938\n",
      "\n",
      "Loss: 909.38%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 186)       1860      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 94, 94, 186)      744       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 94, 94, 186)       0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 179)       299825    \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 179)       288548    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 90, 90, 179)      716       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 136)       219232    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 88, 88, 136)      544       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 136)       166600    \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 86, 86, 136)      544       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 84, 84, 136)       166600    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,145,217\n",
      "Trainable params: 1,143,941\n",
      "Non-trainable params: 1,276\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 41s 155ms/step - loss: 1194.1627 - val_loss: 24.1090\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 11.3026 - val_loss: 10.1985\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 10.3721 - val_loss: 11.3048\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 10.3049 - val_loss: 10.4495\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 10.2293 - val_loss: 10.3341\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 10.2270 - val_loss: 10.0410\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 10.1873 - val_loss: 11.6982\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 10.2117 - val_loss: 10.9678\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 10.2074 - val_loss: 11.8118\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 10.1483 - val_loss: 11.0312\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 10.1349 - val_loss: 9.9767\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 10.1739 - val_loss: 9.9924\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 10.1039 - val_loss: 10.1284\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 10.1209 - val_loss: 10.1359\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 10.1060 - val_loss: 9.9418\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 10.1589 - val_loss: 10.1847\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 10.1630 - val_loss: 10.0164\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 10.1171 - val_loss: 10.0022\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 10.1396 - val_loss: 10.2836\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 10.1382 - val_loss: 10.0532\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 10.1251 - val_loss: 9.9486\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 10.1379 - val_loss: 9.8871\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 10.1590 - val_loss: 9.9758\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 10.1648 - val_loss: 9.9204\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 10.1673 - val_loss: 9.8604\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 10.1451 - val_loss: 10.2185\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 10.1357 - val_loss: 10.3882\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 10.1395 - val_loss: 9.9220\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 10.1412 - val_loss: 9.8788\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 10.1061 - val_loss: 10.1334\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 10.1292 - val_loss: 9.9139\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 10.1432 - val_loss: 9.9591\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 10.1110 - val_loss: 9.9089\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 10.0991 - val_loss: 10.0938\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 10.0956 - val_loss: 9.9578\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 10.1066 - val_loss: 9.9973\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 10.0972 - val_loss: 10.1062\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 10.1130 - val_loss: 10.6038\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 10.1325 - val_loss: 9.8551\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 10.1541 - val_loss: 9.9064\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 10.1500 - val_loss: 10.1446\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 37s 149ms/step - loss: 10.1249 - val_loss: 10.3740\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 10.0861 - val_loss: 9.8672\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 10.1406 - val_loss: 9.8440\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 10.1033 - val_loss: 10.2980\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 10.0911 - val_loss: 9.8867\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 10.0613 - val_loss: 9.9043\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 10.1189 - val_loss: 9.9260\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 10.1020 - val_loss: 9.8850\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 10.0883 - val_loss: 9.8828\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 10.0637 - val_loss: 9.9474\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 10.0602 - val_loss: 9.8478\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 10.0785 - val_loss: 10.2248\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 10.0824 - val_loss: 10.3457\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 10.0578 - val_loss: 9.9400\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 10.0748 - val_loss: 10.0643\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 10.0790 - val_loss: 10.0007\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 10.0409 - val_loss: 10.1579\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 10.0337 - val_loss: 9.9789\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 10.0646 - val_loss: 10.0798\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 10.0323 - val_loss: 10.2911\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 10.0722 - val_loss: 10.0946\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 10.0244 - val_loss: 11.4630\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 10.0303 - val_loss: 10.2472\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 10.0305 - val_loss: 9.9264\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 10.0348 - val_loss: 10.2373\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 10.0166 - val_loss: 10.3009\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 10.0092 - val_loss: 10.1677\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 10.0039 - val_loss: 9.8273\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 10.0034 - val_loss: 10.3870\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 10.0077 - val_loss: 10.2002\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.9787 - val_loss: 10.0230\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.9828 - val_loss: 10.1301\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.9472 - val_loss: 10.4462\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.9553 - val_loss: 10.1887\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.9719 - val_loss: 11.4538\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.9533 - val_loss: 10.6530\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.9478 - val_loss: 10.5928\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.9590 - val_loss: 10.2755\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.9493 - val_loss: 9.9357\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.9352 - val_loss: 10.3045\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.8994 - val_loss: 10.3861\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.9469 - val_loss: 9.7746\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.9180 - val_loss: 9.7163\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.9111 - val_loss: 10.1131\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.9357 - val_loss: 9.6505\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.9120 - val_loss: 10.1546\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.9372 - val_loss: 10.1226\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.8808 - val_loss: 9.9133\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.9101 - val_loss: 9.7005\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.8775 - val_loss: 9.7424\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.8834 - val_loss: 9.8602\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.9180 - val_loss: 9.7701\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.8842 - val_loss: 9.7976\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.8827 - val_loss: 9.6738\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.8785 - val_loss: 9.6786\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.8673 - val_loss: 9.8188\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.8681 - val_loss: 9.8502\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.8691 - val_loss: 9.8274\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.8906 - val_loss: 9.5944\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.8506 - val_loss: 9.6584\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.8551 - val_loss: 9.7234\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.8544 - val_loss: 9.7577\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.8664 - val_loss: 9.7362\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.8600 - val_loss: 9.6868\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.8609 - val_loss: 9.7902\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.8375 - val_loss: 9.7556\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.8478 - val_loss: 9.7188\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.8630 - val_loss: 9.6338\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.8297 - val_loss: 9.6510\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.8315 - val_loss: 9.7370\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.8252 - val_loss: 10.1597\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.8412 - val_loss: 9.6237\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.8212 - val_loss: 9.8221\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.8070 - val_loss: 9.6994\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.8529 - val_loss: 9.8863\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.7902 - val_loss: 9.8559\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.8139 - val_loss: 9.8236\n",
      "Epoch 119/128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 37s 148ms/step - loss: 9.8143 - val_loss: 10.0170\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.8051 - val_loss: 249.7834\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.7896 - val_loss: 9.7296\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.7908 - val_loss: 9.5455\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.7775 - val_loss: 9.5529\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.8225 - val_loss: 9.6636\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.7950 - val_loss: 9.6324\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.7996 - val_loss: 9.6443\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.7978 - val_loss: 9.5612\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.7510 - val_loss: 41.7138\n",
      "\n",
      "Loss: 4171.38%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 32)        320       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 32)        9248      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 92, 92, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 8)         2312      \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 90, 90, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 131)       9563      \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 88, 88, 131)      524       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 136)       160480    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 182,611\n",
      "Trainable params: 182,267\n",
      "Non-trainable params: 344\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 12s 44ms/step - loss: 1187.8442 - val_loss: 50.2766\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 11.1910 - val_loss: 22.3590\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.4403 - val_loss: 15.8133\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.2770 - val_loss: 12.1874\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.2064 - val_loss: 11.0480\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.1638 - val_loss: 11.8099\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.1340 - val_loss: 11.2398\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.1157 - val_loss: 10.2790\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0998 - val_loss: 10.7195\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0927 - val_loss: 10.6078\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.1308 - val_loss: 10.9445\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.1056 - val_loss: 10.0669\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.1136 - val_loss: 9.9164\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.1433 - val_loss: 10.2542\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.1087 - val_loss: 10.8426\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.1135 - val_loss: 10.0705\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.1340 - val_loss: 10.0468\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.1234 - val_loss: 10.2480\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.1374 - val_loss: 10.1274\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.1026 - val_loss: 10.1407\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.1330 - val_loss: 9.8612\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.1199 - val_loss: 10.3282\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.1363 - val_loss: 10.2226\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.1211 - val_loss: 10.5732\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.1686 - val_loss: 10.7282\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.1153 - val_loss: 10.4521\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0950 - val_loss: 10.0685\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.1357 - val_loss: 10.4342\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.1202 - val_loss: 9.9158\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.1041 - val_loss: 10.3463\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.1323 - val_loss: 10.3338\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.1201 - val_loss: 10.0308\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.1515 - val_loss: 10.2933\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.1684 - val_loss: 9.9307\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0955 - val_loss: 9.9049\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0949 - val_loss: 10.3558\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0936 - val_loss: 9.9234\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.1239 - val_loss: 10.0073\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.1144 - val_loss: 10.1244\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0910 - val_loss: 10.3682\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0962 - val_loss: 10.1614\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0828 - val_loss: 9.7939\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0933 - val_loss: 9.8443\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0766 - val_loss: 9.9969\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0603 - val_loss: 10.3814\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.1314 - val_loss: 10.0729\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0954 - val_loss: 10.1513\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0673 - val_loss: 9.8233\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0766 - val_loss: 9.8902\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0811 - val_loss: 10.0463\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0828 - val_loss: 10.5259\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0879 - val_loss: 9.8205\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0806 - val_loss: 10.3800\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.1010 - val_loss: 9.9323\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0852 - val_loss: 9.9062\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0651 - val_loss: 9.9937\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0638 - val_loss: 10.0646\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0801 - val_loss: 9.8353\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0555 - val_loss: 9.8726\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0597 - val_loss: 10.0449\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0582 - val_loss: 10.1268\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0880 - val_loss: 10.1814\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0858 - val_loss: 9.7875\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0753 - val_loss: 10.1006\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0592 - val_loss: 9.9769\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.1015 - val_loss: 10.2275\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0563 - val_loss: 9.7846\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0605 - val_loss: 10.0981\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0604 - val_loss: 9.9280\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0784 - val_loss: 9.8303\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0790 - val_loss: 9.9102\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0687 - val_loss: 9.8895\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0737 - val_loss: 9.8677\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0474 - val_loss: 9.7561\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.1115 - val_loss: 9.8171\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0814 - val_loss: 9.8211\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0445 - val_loss: 9.8133\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0821 - val_loss: 10.0150\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0525 - val_loss: 9.9065\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0459 - val_loss: 10.5190\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.1099 - val_loss: 9.8757\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0940 - val_loss: 9.8797\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0799 - val_loss: 9.7632\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0584 - val_loss: 9.8766\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0879 - val_loss: 10.0912\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0737 - val_loss: 10.0259\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0612 - val_loss: 10.1600\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0654 - val_loss: 9.8270\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0706 - val_loss: 10.0656\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0710 - val_loss: 9.7495\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0479 - val_loss: 9.8940\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0497 - val_loss: 10.0121\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0817 - val_loss: 9.8669\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0563 - val_loss: 9.8917\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0772 - val_loss: 9.9111\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0549 - val_loss: 9.9785\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0621 - val_loss: 9.9348\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.1066 - val_loss: 10.2004\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0642 - val_loss: 10.5987\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0514 - val_loss: 9.9832\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0491 - val_loss: 10.1129\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0746 - val_loss: 9.8780\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0537 - val_loss: 9.7325\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0459 - val_loss: 10.0757\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0653 - val_loss: 9.8935\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0448 - val_loss: 9.7515\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0415 - val_loss: 9.9013\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0401 - val_loss: 9.9898\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0724 - val_loss: 9.7790\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0200 - val_loss: 9.9865\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0517 - val_loss: 9.8840\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0524 - val_loss: 10.0028\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0688 - val_loss: 9.8725\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0595 - val_loss: 9.9161\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0491 - val_loss: 10.0348\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0565 - val_loss: 9.8910\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0595 - val_loss: 9.9576\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0436 - val_loss: 10.0422\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0840 - val_loss: 9.8545\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0718 - val_loss: 10.2578\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0423 - val_loss: 9.8179\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0432 - val_loss: 9.9778\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0566 - val_loss: 10.1648\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0612 - val_loss: 9.9273\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0649 - val_loss: 9.8833\n",
      "Epoch 126/128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0327 - val_loss: 9.9000\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0408 - val_loss: 9.9223\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0628 - val_loss: 9.8405\n",
      "\n",
      "Loss: 984.05%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 191)       1910      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 94, 94, 191)      764       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 94, 94, 191)       0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 240)       412800    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 92, 92, 240)      960       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 92, 92, 240)       0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 189)       408429    \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 189)       321678    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 88, 88, 189)      756       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 51)        86802     \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 86, 86, 51)       204       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 84, 84, 65)        29900     \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 84, 84, 65)       260       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 82, 82, 136)       79696     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,344,163\n",
      "Trainable params: 1,342,689\n",
      "Non-trainable params: 1,474\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 46s 173ms/step - loss: 1601.1820 - val_loss: 240.9807\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 23.0516 - val_loss: 12.5293\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 10.3866 - val_loss: 10.7983\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 10.2345 - val_loss: 12.8873\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 10.2504 - val_loss: 10.5902\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 10.1592 - val_loss: 10.9174\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 10.1348 - val_loss: 10.2868\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 10.1171 - val_loss: 12.3353\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 10.0744 - val_loss: 9.9812\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 10.1132 - val_loss: 11.8205\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 10.1055 - val_loss: 10.4380\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 10.0793 - val_loss: 9.9952\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 10.0567 - val_loss: 10.2165\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 10.0637 - val_loss: 10.0836\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 10.0549 - val_loss: 12.9350\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 10.0714 - val_loss: 10.0513\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 10.0499 - val_loss: 10.1197\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 10.0731 - val_loss: 10.2134\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 10.0453 - val_loss: 10.3140\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 10.0683 - val_loss: 9.9198\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 10.0730 - val_loss: 9.8448\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 10.0763 - val_loss: 10.3848\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 10.0773 - val_loss: 10.2057\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 10.0494 - val_loss: 9.9042\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 10.0340 - val_loss: 9.9078\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 10.0608 - val_loss: 10.1321\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 10.0512 - val_loss: 10.1047\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 10.0642 - val_loss: 10.5579\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 10.0690 - val_loss: 9.8465\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 10.0436 - val_loss: 10.1260\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 10.0488 - val_loss: 9.8858\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 10.0260 - val_loss: 9.8110\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 10.0261 - val_loss: 9.9781\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 10.0422 - val_loss: 10.1411\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 10.0582 - val_loss: 9.9599\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 10.0218 - val_loss: 9.7802\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 10.0321 - val_loss: 9.7938\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 10.0312 - val_loss: 9.9766\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 10.0472 - val_loss: 9.8041\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 10.0134 - val_loss: 10.1165\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 10.0440 - val_loss: 9.9005\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 10.0241 - val_loss: 9.8905\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 10.0281 - val_loss: 9.9569\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 10.0464 - val_loss: 9.8300\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 10.0109 - val_loss: 9.8513\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 10.0508 - val_loss: 9.9216\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 10.0472 - val_loss: 10.2296\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 10.0225 - val_loss: 9.8320\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 10.0151 - val_loss: 9.9019\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 10.0041 - val_loss: 9.8363\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 10.0190 - val_loss: 9.7944\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 10.0070 - val_loss: 9.7972\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 10.0170 - val_loss: 9.8533\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 10.0120 - val_loss: 9.8895\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 9.9864 - val_loss: 9.7946\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 10.0000 - val_loss: 9.7918\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 10.0177 - val_loss: 9.8871\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 10.0087 - val_loss: 9.9081\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 9.9997 - val_loss: 9.8485\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 9.9826 - val_loss: 9.7112\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 9.9763 - val_loss: 10.0132\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 9.9980 - val_loss: 9.7237\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 9.9827 - val_loss: 9.7004\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 9.9774 - val_loss: 9.7663\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 9.9686 - val_loss: 9.8843\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 9.9904 - val_loss: 9.8381\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 9.9623 - val_loss: 9.8867\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 9.9795 - val_loss: 9.7230\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 9.9775 - val_loss: 9.6824\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 9.9754 - val_loss: 9.8444\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 9.9491 - val_loss: 9.7482\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 9.9587 - val_loss: 9.8890\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 9.9373 - val_loss: 9.8303\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 9.9616 - val_loss: 9.6741\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 9.9325 - val_loss: 9.7827\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 9.9531 - val_loss: 9.7887\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 9.9342 - val_loss: 9.8509\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 9.9243 - val_loss: 9.6800\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 9.9159 - val_loss: 9.6462\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 9.9019 - val_loss: 9.7544\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 9.9281 - val_loss: 9.7280\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 9.9149 - val_loss: 9.6960\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 9.9095 - val_loss: 9.7199\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 9.8980 - val_loss: 9.7567\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 9.8931 - val_loss: 9.7721\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 9.8701 - val_loss: 9.6893\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 9.8958 - val_loss: 9.8739\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 9.8698 - val_loss: 9.8451\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 9.8725 - val_loss: 9.7209\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 9.8807 - val_loss: 9.6631\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 9.8769 - val_loss: 9.7176\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 9.8518 - val_loss: 9.8472\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 9.8341 - val_loss: 9.9209\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 9.8408 - val_loss: 9.6553\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 9.8677 - val_loss: 9.5465\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 9.8322 - val_loss: 9.9170\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 9.8152 - val_loss: 9.6742\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 9.7964 - val_loss: 9.6970\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 9.8259 - val_loss: 9.8252\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 9.7860 - val_loss: 9.7790\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 9.7791 - val_loss: 9.6807\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 9.7910 - val_loss: 9.5968\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 9.7750 - val_loss: 9.6278\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 9.7704 - val_loss: 9.7991\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 9.7603 - val_loss: 9.7537\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 9.7629 - val_loss: 9.8283\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 9.7701 - val_loss: 9.5694\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 9.7620 - val_loss: 9.6330\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 9.7590 - val_loss: 9.5605\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 9.7052 - val_loss: 9.8000\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 9.7203 - val_loss: 9.6425\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 9.7171 - val_loss: 9.8665\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 9.7102 - val_loss: 9.5193\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 9.7018 - val_loss: 9.8837\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 9.6923 - val_loss: 9.5739\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 9.6916 - val_loss: 9.7188\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 9.6764 - val_loss: 9.9397\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 9.6723 - val_loss: 9.6636\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 9.6790 - val_loss: 9.7516\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 9.6640 - val_loss: 9.8132\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 9.6459 - val_loss: 9.4325\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 9.6714 - val_loss: 9.5160\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 9.6209 - val_loss: 9.4957\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 9.6337 - val_loss: 9.6109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 125/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 9.6111 - val_loss: 9.6866\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 9.6139 - val_loss: 9.7341\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 9.6157 - val_loss: 9.6313\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 9.6332 - val_loss: 9.5398\n",
      "\n",
      "Loss: 953.98%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 170)       1700      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 94, 94, 170)      680       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 136)       208216    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 92, 92, 136)      544       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 8)         9800      \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 90, 90, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 136)       9928      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 230,904\n",
      "Trainable params: 230,274\n",
      "Non-trainable params: 630\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 16s 58ms/step - loss: 2735.4995 - val_loss: 1935.9321\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 1204.1394 - val_loss: 470.5211\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 219.7583 - val_loss: 65.9745\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 34.0871 - val_loss: 16.0353\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 12.8048 - val_loss: 10.6538\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 10.3305 - val_loss: 9.8400\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 10.0127 - val_loss: 9.7892\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9744 - val_loss: 9.7526\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9885 - val_loss: 9.7783\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9537 - val_loss: 9.7418\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9728 - val_loss: 9.7942\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9624 - val_loss: 9.7706\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9553 - val_loss: 9.7575\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9622 - val_loss: 9.7100\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9590 - val_loss: 9.7269\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9614 - val_loss: 9.7529\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9625 - val_loss: 9.7842\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9605 - val_loss: 9.6861\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9554 - val_loss: 9.7618\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9567 - val_loss: 9.7271\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9502 - val_loss: 9.7981\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9487 - val_loss: 9.7783\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9585 - val_loss: 9.8456\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9650 - val_loss: 9.7202\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9638 - val_loss: 9.7771\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9558 - val_loss: 9.7092\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9729 - val_loss: 9.7202\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9549 - val_loss: 9.7917\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9655 - val_loss: 9.6998\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9566 - val_loss: 9.7267\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9546 - val_loss: 9.7238\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9589 - val_loss: 9.7098\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9612 - val_loss: 9.6798\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9588 - val_loss: 9.8097\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9585 - val_loss: 9.7316\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9668 - val_loss: 9.7163\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9640 - val_loss: 9.6924\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9645 - val_loss: 9.8294\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9652 - val_loss: 9.6958\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9511 - val_loss: 9.7223\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9645 - val_loss: 9.7717\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9606 - val_loss: 9.7732\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9575 - val_loss: 9.6894\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9463 - val_loss: 9.7271\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9635 - val_loss: 9.6980\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9556 - val_loss: 9.7544\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9545 - val_loss: 9.7351\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9545 - val_loss: 9.7406\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9591 - val_loss: 9.7827\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9479 - val_loss: 9.7479\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9501 - val_loss: 9.7315\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9623 - val_loss: 9.7484\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9498 - val_loss: 9.7394\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9485 - val_loss: 9.7147\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9519 - val_loss: 9.8410\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9617 - val_loss: 9.6911\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9454 - val_loss: 9.7040\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9628 - val_loss: 9.7273\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9505 - val_loss: 9.7279\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9443 - val_loss: 9.7752\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9567 - val_loss: 9.7499\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9533 - val_loss: 9.7083\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9476 - val_loss: 9.7035\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9425 - val_loss: 9.7148\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9358 - val_loss: 9.7364\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9568 - val_loss: 9.7078\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9473 - val_loss: 9.7235\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9439 - val_loss: 9.7492\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9535 - val_loss: 9.7283\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9523 - val_loss: 9.7261\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9417 - val_loss: 9.8251\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9428 - val_loss: 9.6720\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9474 - val_loss: 9.7602\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9530 - val_loss: 9.7376\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9344 - val_loss: 9.6879\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9572 - val_loss: 9.7194\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9349 - val_loss: 9.7209\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9492 - val_loss: 9.7123\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9421 - val_loss: 9.7126\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9539 - val_loss: 9.6910\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9470 - val_loss: 9.7678\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9433 - val_loss: 9.7150\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9437 - val_loss: 9.7156\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9424 - val_loss: 9.7782\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9474 - val_loss: 9.6998\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9422 - val_loss: 9.7359\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9493 - val_loss: 9.7670\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9506 - val_loss: 9.7418\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9436 - val_loss: 9.7144\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9521 - val_loss: 9.7294\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9405 - val_loss: 9.7689\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9416 - val_loss: 9.7473\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9430 - val_loss: 9.7208\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9405 - val_loss: 9.7491\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9408 - val_loss: 9.7422\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9405 - val_loss: 9.7653\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9439 - val_loss: 9.7124\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9552 - val_loss: 9.7163\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9463 - val_loss: 9.7244\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9353 - val_loss: 9.6797\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9356 - val_loss: 9.7300\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9380 - val_loss: 9.7155\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9340 - val_loss: 9.7675\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9390 - val_loss: 9.7328\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9418 - val_loss: 9.7071\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9352 - val_loss: 9.8194\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9350 - val_loss: 9.7952\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9399 - val_loss: 9.7110\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9377 - val_loss: 9.7441\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9414 - val_loss: 9.7101\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9365 - val_loss: 9.7041\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9397 - val_loss: 9.7730\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9438 - val_loss: 9.7089\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9320 - val_loss: 9.7081\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9504 - val_loss: 9.7382\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9360 - val_loss: 9.6849\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9333 - val_loss: 9.6934\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9355 - val_loss: 9.7178\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9305 - val_loss: 9.7432\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9277 - val_loss: 9.7339\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9407 - val_loss: 9.7007\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9412 - val_loss: 9.7625\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9345 - val_loss: 9.6897\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9274 - val_loss: 9.7314\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9301 - val_loss: 9.6717\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9305 - val_loss: 9.6974\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9281 - val_loss: 9.6841\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 9.9316 - val_loss: 9.7342\n",
      "\n",
      "Loss: 973.42%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 161)       1610      \n",
      "                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " conv2d_1 (Conv2D)           (None, 92, 92, 161)       233450    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 92, 92, 161)      644       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 92, 92, 161)       0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 196)       284200    \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 196)       345940    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 88, 88, 196)      784       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 40)        70600     \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 86, 86, 40)       160       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 84, 84, 82)        29602     \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 84, 84, 82)       328       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 82, 82, 136)       100504    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,067,826\n",
      "Trainable params: 1,066,866\n",
      "Non-trainable params: 960\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 40s 149ms/step - loss: 1459.8392 - val_loss: 80.4218\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 15.2194 - val_loss: 12.6401\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 10.2305 - val_loss: 10.3119\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 10.1545 - val_loss: 10.2540\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 10.2174 - val_loss: 10.7421\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 10.1587 - val_loss: 10.3118\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 10.0838 - val_loss: 10.2450\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 10.1000 - val_loss: 10.2241\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 10.0872 - val_loss: 10.3811\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 10.0675 - val_loss: 9.9372\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 10.1197 - val_loss: 10.0933\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 10.0764 - val_loss: 10.3553\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 10.0485 - val_loss: 9.8214\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 10.0841 - val_loss: 9.7857\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 10.0399 - val_loss: 9.9610\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 10.0648 - val_loss: 10.0698\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 10.0656 - val_loss: 9.9036\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 10.0459 - val_loss: 9.8669\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 10.0264 - val_loss: 9.8173\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 10.0389 - val_loss: 9.9321\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 10.0455 - val_loss: 9.7311\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 10.0260 - val_loss: 9.7045\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 10.0368 - val_loss: 10.3760\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 10.0268 - val_loss: 9.8744\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 10.0194 - val_loss: 9.7622\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 10.0008 - val_loss: 9.6794\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.9921 - val_loss: 9.7266\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.9933 - val_loss: 9.8279\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.9702 - val_loss: 9.8137\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.9592 - val_loss: 9.7435\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.9629 - val_loss: 9.8695\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.9542 - val_loss: 9.6874\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.9495 - val_loss: 9.6723\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.9042 - val_loss: 9.6392\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.9014 - val_loss: 9.6420\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.9101 - val_loss: 9.7192\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.8992 - val_loss: 9.8948\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.8604 - val_loss: 9.6141\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.8506 - val_loss: 9.5556\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.8281 - val_loss: 9.5904\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.8276 - val_loss: 9.6056\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.7954 - val_loss: 9.7111\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.8027 - val_loss: 9.6980\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.8051 - val_loss: 9.4981\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.7580 - val_loss: 9.7985\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.7813 - val_loss: 9.6379\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.7185 - val_loss: 9.8097\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.7196 - val_loss: 9.5381\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.7098 - val_loss: 9.6492\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.7235 - val_loss: 9.4506\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.6872 - val_loss: 9.5426\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.6546 - val_loss: 9.5435\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.6118 - val_loss: 9.4636\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.6603 - val_loss: 9.4203\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.6043 - val_loss: 9.5163\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.6009 - val_loss: 9.6171\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.5758 - val_loss: 9.3327\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.5819 - val_loss: 9.6785\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.5676 - val_loss: 9.2785\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.5717 - val_loss: 9.6934\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.5515 - val_loss: 9.5724\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.5207 - val_loss: 9.4195\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.5176 - val_loss: 9.3651\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.5075 - val_loss: 9.4026\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.5193 - val_loss: 9.4013\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.5092 - val_loss: 9.6637\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.5057 - val_loss: 9.3634\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.4663 - val_loss: 9.4098\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.4720 - val_loss: 9.5444\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.4664 - val_loss: 9.3487\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.4336 - val_loss: 9.3303\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.4618 - val_loss: 9.3933\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.4251 - val_loss: 9.2139\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.4143 - val_loss: 9.2772\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.4240 - val_loss: 9.3151\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.4184 - val_loss: 9.3587\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.4000 - val_loss: 9.2623\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.3935 - val_loss: 9.3286\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.3818 - val_loss: 9.1998\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.3756 - val_loss: 9.1731\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.3896 - val_loss: 9.3280\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.3779 - val_loss: 9.3351\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.3660 - val_loss: 9.2397\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.3665 - val_loss: 9.5163\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.3224 - val_loss: 9.4178\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.3324 - val_loss: 9.2708\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.3527 - val_loss: 9.2166\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.3247 - val_loss: 9.2206\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.3221 - val_loss: 9.2208\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.3166 - val_loss: 9.3416\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.2898 - val_loss: 9.2836\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.3055 - val_loss: 9.2764\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.3349 - val_loss: 9.3224\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.2962 - val_loss: 9.5191\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.2793 - val_loss: 9.2167\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.2823 - val_loss: 9.2651\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.2745 - val_loss: 9.2067\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.2652 - val_loss: 9.5331\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.2409 - val_loss: 9.3065\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.2510 - val_loss: 9.3165\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.2376 - val_loss: 9.2719\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.2272 - val_loss: 9.3678\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.2500 - val_loss: 9.2180\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.2490 - val_loss: 9.1932\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.2265 - val_loss: 9.1898\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.2148 - val_loss: 9.3717\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.2248 - val_loss: 9.2438\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.2121 - val_loss: 9.1154\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.2131 - val_loss: 9.3149\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.2276 - val_loss: 9.3380\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.1838 - val_loss: 9.1784\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.1734 - val_loss: 9.3753\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.1811 - val_loss: 9.2569\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.1726 - val_loss: 9.3081\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.1834 - val_loss: 9.3944\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.1805 - val_loss: 9.1239\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.1680 - val_loss: 9.3127\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.1634 - val_loss: 9.1699\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.1758 - val_loss: 9.2448\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.1625 - val_loss: 9.3703\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.1460 - val_loss: 9.3675\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.1295 - val_loss: 9.5569\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.1559 - val_loss: 9.2742\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.1434 - val_loss: 9.2681\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.1405 - val_loss: 9.1551\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.1531 - val_loss: 9.1230\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.1023 - val_loss: 9.1360\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 9.1117 - val_loss: 9.2248\n",
      "\n",
      "Loss: 922.48%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 137)       1370      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 94, 94, 137)      548       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 94, 94, 137)       0         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 128)       157952    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 92, 92, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 49)        56497     \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 90, 90, 49)       196       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 108)       47736     \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 88, 88, 108)      432       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 136)       132328    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 397,575\n",
      "Trainable params: 396,729\n",
      "Non-trainable params: 846\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 22s 82ms/step - loss: 1300.9855 - val_loss: 19.9363\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 11.4663 - val_loss: 9.8460\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0083 - val_loss: 9.8362\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0039 - val_loss: 10.1278\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0060 - val_loss: 9.7848\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 9.9972 - val_loss: 9.9327\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 9.9986 - val_loss: 9.8182\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 9.9998 - val_loss: 9.6992\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0134 - val_loss: 9.7912\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0356 - val_loss: 9.7721\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0204 - val_loss: 9.7969\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0276 - val_loss: 9.7484\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0246 - val_loss: 9.8185\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0350 - val_loss: 9.8046\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0683 - val_loss: 12.6189\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0478 - val_loss: 9.8747\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0591 - val_loss: 9.7665\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0502 - val_loss: 9.7331\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0372 - val_loss: 9.9187\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0598 - val_loss: 9.9071\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0985 - val_loss: 10.0923\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0545 - val_loss: 10.0342\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0613 - val_loss: 9.8303\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0705 - val_loss: 9.9565\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0584 - val_loss: 9.7931\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0518 - val_loss: 10.2929\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.1010 - val_loss: 9.8626\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0707 - val_loss: 9.8539\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0526 - val_loss: 9.8250\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0879 - val_loss: 9.8129\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0516 - val_loss: 10.2257\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0808 - val_loss: 9.7614\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0836 - val_loss: 9.8674\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0754 - val_loss: 9.8760\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0900 - val_loss: 9.8532\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0654 - val_loss: 10.1709\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0610 - val_loss: 9.8285\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0529 - val_loss: 10.0053\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0300 - val_loss: 9.9435\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0639 - val_loss: 9.8380\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0566 - val_loss: 9.9396\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0548 - val_loss: 9.7311\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0681 - val_loss: 10.2666\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0815 - val_loss: 9.9822\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0487 - val_loss: 10.1355\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0365 - val_loss: 9.9355\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0706 - val_loss: 10.2512\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0498 - val_loss: 9.8605\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0127 - val_loss: 9.8805\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0252 - val_loss: 9.9581\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0408 - val_loss: 9.7237\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0478 - val_loss: 9.7726\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0440 - val_loss: 9.8420\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0087 - val_loss: 9.8072\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0407 - val_loss: 10.2769\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0138 - val_loss: 11.5484\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0208 - val_loss: 9.8413\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0070 - val_loss: 10.1316\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 9.9836 - val_loss: 10.0041\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 9.9870 - val_loss: 10.1117\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0111 - val_loss: 10.1378\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 9.9800 - val_loss: 10.0125\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 9.9951 - val_loss: 10.1772\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 9.9937 - val_loss: 9.8876\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 9.9768 - val_loss: 9.8542\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 9.9890 - val_loss: 9.7972\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 9.9505 - val_loss: 10.1534\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 9.9635 - val_loss: 9.9574\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 9.9747 - val_loss: 10.0983\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 9.9878 - val_loss: 9.8023\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 9.9601 - val_loss: 9.8900\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 9.9798 - val_loss: 9.6721\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 9.9493 - val_loss: 9.9414\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 9.9451 - val_loss: 9.7828\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 9.9576 - val_loss: 9.8233\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 9.9787 - val_loss: 9.8418\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 9.9757 - val_loss: 9.9677\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 9.9450 - val_loss: 10.0705\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 9.9430 - val_loss: 9.9634\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 9.9590 - val_loss: 9.7474\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 9.9343 - val_loss: 159.4885\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 9.9951 - val_loss: 9.8005\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0221 - val_loss: 9.6749\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 9.9809 - val_loss: 11.9941\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 9.9729 - val_loss: 9.6875\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 9.9307 - val_loss: 9.8115\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 9.9614 - val_loss: 9.7378\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 9.9480 - val_loss: 10.1756\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 9.9356 - val_loss: 10.1320\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 9.9508 - val_loss: 9.7825\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 9.9422 - val_loss: 9.7368\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 9.9436 - val_loss: 9.9164\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 9.9492 - val_loss: 10.4636\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 9.9536 - val_loss: 9.8866\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 9.9429 - val_loss: 11.4894\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 9.9755 - val_loss: 9.8277\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 9.9778 - val_loss: 9.7211\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 9.9531 - val_loss: 10.0697\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 9.9489 - val_loss: 9.8456\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 9.9535 - val_loss: 9.9973\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 9.9598 - val_loss: 9.8430\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 9.9399 - val_loss: 10.0078\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 9.9266 - val_loss: 10.5328\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 9.9789 - val_loss: 10.0853\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 9.9332 - val_loss: 9.8099\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 9.9446 - val_loss: 9.7129\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 9.9481 - val_loss: 9.8744\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 9.9348 - val_loss: 9.9191\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 9.9182 - val_loss: 9.8400\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 9.9304 - val_loss: 40.1792\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 9.9738 - val_loss: 9.7592\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 9.9421 - val_loss: 9.8363\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 9.9344 - val_loss: 9.8992\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 9.9496 - val_loss: 9.7950\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 9.9507 - val_loss: 9.7506\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 9.9359 - val_loss: 9.9561\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 9.9474 - val_loss: 10.0507\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 9.9246 - val_loss: 10.3645\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 9.9261 - val_loss: 9.9354\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 9.9293 - val_loss: 9.7886\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 9.9247 - val_loss: 10.0847\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 9.9503 - val_loss: 9.7708\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 9.9091 - val_loss: 9.7342\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 9.9440 - val_loss: 10.4669\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 9.9197 - val_loss: 9.9482\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 9.9378 - val_loss: 10.1711\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 9.9359 - val_loss: 9.9060\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 9.9237 - val_loss: 9.7068\n",
      "\n",
      "Loss: 970.68%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 208)       2080      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 94, 94, 208)      832       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 94, 94, 208)       0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 105)       196665    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 92, 92, 105)      420       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 102)       96492     \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 90, 90, 102)      408       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 31)        28489     \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 88, 88, 31)       124       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 136)       38080     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 363,594\n",
      "Trainable params: 362,700\n",
      "Non-trainable params: 894\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 23s 83ms/step - loss: 2091.0100 - val_loss: 790.3298\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 140.8927 - val_loss: 18.6196\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 20s 80ms/step - loss: 11.6236 - val_loss: 10.4370\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 20s 80ms/step - loss: 10.0556 - val_loss: 9.9748\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 20s 80ms/step - loss: 10.0233 - val_loss: 9.9233\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 20s 80ms/step - loss: 10.0180 - val_loss: 9.7845\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 20s 80ms/step - loss: 9.9970 - val_loss: 9.8700\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.9995 - val_loss: 9.7436\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 20s 80ms/step - loss: 9.9939 - val_loss: 9.7803\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 20s 80ms/step - loss: 10.0103 - val_loss: 9.8132\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 20s 80ms/step - loss: 9.9909 - val_loss: 9.8220\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 20s 80ms/step - loss: 9.9904 - val_loss: 9.8081\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.9912 - val_loss: 9.7753\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 10.0125 - val_loss: 9.7552\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 20s 80ms/step - loss: 9.9898 - val_loss: 9.7985\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.9806 - val_loss: 9.8488\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 20s 80ms/step - loss: 9.9807 - val_loss: 9.7465\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 20s 80ms/step - loss: 9.9902 - val_loss: 9.7403\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 20s 80ms/step - loss: 10.0012 - val_loss: 9.7732\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 20s 80ms/step - loss: 9.9955 - val_loss: 9.7895\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.9870 - val_loss: 9.7814\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.9936 - val_loss: 9.7330\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.9981 - val_loss: 9.8086\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.9861 - val_loss: 9.9312\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.9923 - val_loss: 9.7550\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.9965 - val_loss: 9.8125\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 10.0144 - val_loss: 9.7475\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.9959 - val_loss: 9.7924\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 10.0033 - val_loss: 9.7036\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 10.0136 - val_loss: 9.7597\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 10.0032 - val_loss: 9.7044\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 10.0077 - val_loss: 9.7579\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.9811 - val_loss: 9.7411\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.9884 - val_loss: 9.8028\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.9872 - val_loss: 9.7318\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.9822 - val_loss: 9.7444\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.9743 - val_loss: 9.8337\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.9956 - val_loss: 9.8895\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.9873 - val_loss: 9.7480\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 10.0035 - val_loss: 9.7705\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 20s 80ms/step - loss: 9.9826 - val_loss: 9.7882\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 20s 80ms/step - loss: 9.9769 - val_loss: 9.8151\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.9992 - val_loss: 9.7907\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 10.0039 - val_loss: 9.7921\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.9746 - val_loss: 9.7141\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 20s 80ms/step - loss: 9.9656 - val_loss: 9.7312\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 20s 80ms/step - loss: 9.9787 - val_loss: 9.7292\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.9848 - val_loss: 9.8319\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 20s 80ms/step - loss: 9.9681 - val_loss: 9.6981\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.9700 - val_loss: 9.7542\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.9740 - val_loss: 9.7133\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.9686 - val_loss: 9.8684\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.9769 - val_loss: 9.7182\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.9644 - val_loss: 9.7826\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.9644 - val_loss: 9.7124\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 20s 80ms/step - loss: 9.9794 - val_loss: 9.7115\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 20s 80ms/step - loss: 9.9688 - val_loss: 9.8889\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.9682 - val_loss: 9.7311\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.9662 - val_loss: 9.7614\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.9554 - val_loss: 9.7107\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.9515 - val_loss: 9.8245\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.9682 - val_loss: 9.7832\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.9514 - val_loss: 9.7781\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.9527 - val_loss: 9.7217\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.9456 - val_loss: 9.6910\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.9352 - val_loss: 9.6657\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.9436 - val_loss: 9.8468\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.9429 - val_loss: 9.9342\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.9502 - val_loss: 9.6642\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.9476 - val_loss: 9.7117\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.9243 - val_loss: 9.7201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.9374 - val_loss: 9.6976\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.9530 - val_loss: 9.6763\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.9387 - val_loss: 9.9128\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 20s 80ms/step - loss: 9.9299 - val_loss: 9.7620\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.9342 - val_loss: 9.7624\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.9495 - val_loss: 9.7209\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.9418 - val_loss: 9.7746\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.9507 - val_loss: 9.6944\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.9131 - val_loss: 9.6605\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.9245 - val_loss: 9.6820\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.9318 - val_loss: 9.8170\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.9485 - val_loss: 9.6662\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.9239 - val_loss: 9.7394\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.9317 - val_loss: 9.6640\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.9265 - val_loss: 9.6886\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.9283 - val_loss: 9.6909\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.9149 - val_loss: 9.8635\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.9118 - val_loss: 9.7306\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 20s 80ms/step - loss: 9.9202 - val_loss: 9.7616\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.9290 - val_loss: 9.6856\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 20s 80ms/step - loss: 9.9377 - val_loss: 9.6382\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.9143 - val_loss: 9.8537\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.9144 - val_loss: 9.7555\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.9090 - val_loss: 9.6180\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.9145 - val_loss: 9.7554\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.9119 - val_loss: 9.7404\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.9136 - val_loss: 9.6659\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.9165 - val_loss: 9.6766\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.9140 - val_loss: 9.7234\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.9107 - val_loss: 9.6689\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.8991 - val_loss: 9.6284\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.9039 - val_loss: 9.6363\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.8891 - val_loss: 9.6978\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.8901 - val_loss: 9.8123\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.9124 - val_loss: 9.6617\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.9042 - val_loss: 9.6654\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.8920 - val_loss: 9.7242\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.8958 - val_loss: 9.6524\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.9074 - val_loss: 9.6496\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.8967 - val_loss: 9.7875\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.8989 - val_loss: 9.7242\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.8897 - val_loss: 9.6512\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.8877 - val_loss: 9.8068\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.9016 - val_loss: 9.6841\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.8989 - val_loss: 9.7985\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.8933 - val_loss: 9.6598\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.8847 - val_loss: 9.9361\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.8948 - val_loss: 9.6860\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.8884 - val_loss: 9.7502\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.8777 - val_loss: 9.7884\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.8700 - val_loss: 9.6528\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.8694 - val_loss: 9.7587\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.8617 - val_loss: 9.6140\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.8835 - val_loss: 9.9435\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.8957 - val_loss: 9.8632\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.8784 - val_loss: 9.7436\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 9.8657 - val_loss: 9.6841\n",
      "\n",
      "Loss: 968.41%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 66)        660       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 66)        39270     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 92, 92, 66)       264       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 92, 92, 66)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 165)       98175     \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 165)       245190    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 88, 88, 165)      660       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 88, 88, 165)       0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 233)       346238    \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 84, 84, 233)       488834    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 84, 84, 233)      932       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 84, 84, 233)       0         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 82, 82, 250)       524500    \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 80, 80, 250)       562750    \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 80, 80, 250)      1000      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 78, 78, 104)       234104    \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 78, 78, 104)      416       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 76, 76, 117)       109629    \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 76, 76, 117)      468       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 74, 74, 136)       143344    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,796,438\n",
      "Trainable params: 2,794,566\n",
      "Non-trainable params: 1,872\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 70s 264ms/step - loss: 1271.7389 - val_loss: 38.0503\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 64s 255ms/step - loss: 11.3067 - val_loss: 11.6117\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 64s 255ms/step - loss: 10.2747 - val_loss: 16.3384\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 64s 255ms/step - loss: 10.1855 - val_loss: 9.8381\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 64s 255ms/step - loss: 10.1391 - val_loss: 10.2054\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 64s 255ms/step - loss: 10.1385 - val_loss: 10.5839\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 64s 255ms/step - loss: 10.1141 - val_loss: 10.0287\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 64s 255ms/step - loss: 10.1366 - val_loss: 10.2377\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 10.0941 - val_loss: 10.1215\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 10.0835 - val_loss: 10.0849\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 10.1195 - val_loss: 10.0682\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 10.1067 - val_loss: 9.9176\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 10.0771 - val_loss: 10.8624\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 10.1100 - val_loss: 10.1371\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 10.1047 - val_loss: 10.3127\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 10.1643 - val_loss: 10.4793\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 10.1484 - val_loss: 10.5869\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 10.1198 - val_loss: 10.1265\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 10.1278 - val_loss: 10.0095\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 10.1148 - val_loss: 10.8443\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 10.1308 - val_loss: 9.7781\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 10.0994 - val_loss: 10.1531\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 10.1704 - val_loss: 11.0818\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 63s 254ms/step - loss: 10.1273 - val_loss: 10.5622\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 10.1595 - val_loss: 10.6573\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 10.1341 - val_loss: 9.8865\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 10.1063 - val_loss: 10.0763\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 10.1282 - val_loss: 9.8247\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 10.0921 - val_loss: 11.1614\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 10.0786 - val_loss: 9.9262\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 10.1228 - val_loss: 10.4829\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 10.1003 - val_loss: 9.9803\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 10.0827 - val_loss: 9.9104\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 10.0936 - val_loss: 9.9395\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 10.0781 - val_loss: 9.8133\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 10.0757 - val_loss: 9.8386\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 10.0750 - val_loss: 9.7540\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 10.1009 - val_loss: 10.2491\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 10.0686 - val_loss: 10.1026\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 10.0408 - val_loss: 9.8143\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 10.0143 - val_loss: 10.0684\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 9.9977 - val_loss: 9.9614\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 9.9955 - val_loss: 9.8940\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 9.9519 - val_loss: 10.1810\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 9.9287 - val_loss: 9.7120\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 9.9665 - val_loss: 9.8531\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 9.9737 - val_loss: 9.8145\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 64s 255ms/step - loss: 9.9573 - val_loss: 9.8161\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 64s 255ms/step - loss: 9.9309 - val_loss: 9.8981\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 9.9186 - val_loss: 9.7421\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 64s 255ms/step - loss: 9.9121 - val_loss: 9.6429\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 9.8776 - val_loss: 9.5978\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 9.8427 - val_loss: 9.8585\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 64s 255ms/step - loss: 9.8533 - val_loss: 10.2107\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 9.8504 - val_loss: 10.2111\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 64s 255ms/step - loss: 9.8185 - val_loss: 9.9164\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 9.8021 - val_loss: 10.0653\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 64s 255ms/step - loss: 9.8019 - val_loss: 9.7146\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 64s 255ms/step - loss: 9.8067 - val_loss: 9.8461\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 64s 255ms/step - loss: 9.7616 - val_loss: 9.9910\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 64s 255ms/step - loss: 9.7542 - val_loss: 9.7416\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 9.6893 - val_loss: 10.5048\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 9.6956 - val_loss: 9.6614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 9.6692 - val_loss: 9.7529\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 9.6296 - val_loss: 9.7977\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 9.6275 - val_loss: 9.7978\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 64s 255ms/step - loss: 9.5346 - val_loss: 9.7089\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 9.4790 - val_loss: 10.2137\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 64s 255ms/step - loss: 9.4038 - val_loss: 9.1943\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 9.3374 - val_loss: 9.8067\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 9.2878 - val_loss: 9.2374\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 64s 255ms/step - loss: 9.2832 - val_loss: 9.1475\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 9.2355 - val_loss: 9.0252\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 64s 255ms/step - loss: 9.1653 - val_loss: 9.5400\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 9.1385 - val_loss: 9.4080\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 9.0708 - val_loss: 9.3158\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 64s 255ms/step - loss: 9.0639 - val_loss: 9.2113\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 9.0162 - val_loss: 9.7889\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 8.9739 - val_loss: 8.9329\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 8.9642 - val_loss: 9.0309\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 8.9299 - val_loss: 9.0581\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 8.9095 - val_loss: 8.8647\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 8.8551 - val_loss: 8.7477\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 8.8340 - val_loss: 9.1474\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 8.8128 - val_loss: 8.8737\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 8.8325 - val_loss: 8.7013\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 8.7627 - val_loss: 8.8639\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 8.7439 - val_loss: 8.9417\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 8.6591 - val_loss: 9.0954\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 8.6201 - val_loss: 9.0698\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 8.6576 - val_loss: 8.5871\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 8.6144 - val_loss: 8.5712\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 8.6116 - val_loss: 10.1467\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 8.5815 - val_loss: 8.6836\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 8.5438 - val_loss: 8.6931\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 8.4877 - val_loss: 8.6521\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 8.4931 - val_loss: 8.6848\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 8.4883 - val_loss: 9.8626\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 8.4921 - val_loss: 8.8122\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 8.4038 - val_loss: 8.6422\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 8.4420 - val_loss: 8.4405\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 8.4128 - val_loss: 8.5806\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 8.3477 - val_loss: 8.4998\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 8.3530 - val_loss: 8.7369\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 8.3358 - val_loss: 8.6675\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 8.3151 - val_loss: 8.4273\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 8.2638 - val_loss: 8.7197\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 8.2982 - val_loss: 8.9557\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 8.2286 - val_loss: 8.8166\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 8.2218 - val_loss: 8.6074\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 8.2270 - val_loss: 8.3208\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 8.2446 - val_loss: 8.3032\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 8.1892 - val_loss: 8.8851\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 8.2054 - val_loss: 8.7584\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 8.1691 - val_loss: 8.4420\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 8.1203 - val_loss: 8.8018\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 8.1259 - val_loss: 8.5918\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 8.0803 - val_loss: 8.5299\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 8.1273 - val_loss: 8.2942\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 8.0901 - val_loss: 8.3257\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 8.0508 - val_loss: 8.6564\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 8.0871 - val_loss: 8.8556\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 8.0141 - val_loss: 8.2255\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 8.0242 - val_loss: 9.0251\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 8.0028 - val_loss: 9.2194\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 7.9836 - val_loss: 8.8378\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 7.9358 - val_loss: 8.6316\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 7.9380 - val_loss: 8.6362\n",
      "\n",
      "Loss: 863.62%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 256)       2560      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 94, 94, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 94, 94, 256)       0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 8)         18440     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 92, 92, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dropout_2 (Dropout)         (None, 92, 92, 8)         0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 256)       18688     \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 88, 88, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 136)       313480    \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 86, 86, 136)      544       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 84, 84, 136)       166600    \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 84, 84, 136)      544       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 82, 82, 136)       166600    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,279,620\n",
      "Trainable params: 1,278,034\n",
      "Non-trainable params: 1,586\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 42s 160ms/step - loss: 1186.7067 - val_loss: 23.4870\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.5328 - val_loss: 11.1102\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0885 - val_loss: 10.1480\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0281 - val_loss: 10.0200\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0419 - val_loss: 9.9966\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0711 - val_loss: 10.3498\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0496 - val_loss: 9.7865\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0706 - val_loss: 9.8791\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0891 - val_loss: 10.3082\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0543 - val_loss: 9.7989\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0890 - val_loss: 9.9546\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0682 - val_loss: 10.1515\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0493 - val_loss: 9.8419\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0868 - val_loss: 10.6448\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.1216 - val_loss: 9.8367\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0856 - val_loss: 10.3433\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0895 - val_loss: 10.0435\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0822 - val_loss: 9.8512\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.1071 - val_loss: 9.9440\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0758 - val_loss: 10.0751\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0892 - val_loss: 10.3060\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.1064 - val_loss: 9.9530\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.1118 - val_loss: 9.9558\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0784 - val_loss: 10.0890\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0927 - val_loss: 10.0956\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.1029 - val_loss: 10.0495\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0733 - val_loss: 9.9920\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.1045 - val_loss: 9.8147\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0880 - val_loss: 10.5853\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0692 - val_loss: 9.8314\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0984 - val_loss: 10.0223\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0785 - val_loss: 9.8636\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0955 - val_loss: 9.9715\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0890 - val_loss: 9.7238\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0748 - val_loss: 10.0472\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0691 - val_loss: 9.7826\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0662 - val_loss: 9.7853\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0609 - val_loss: 10.4231\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0858 - val_loss: 10.2659\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.1307 - val_loss: 10.4613\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0718 - val_loss: 10.5327\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0805 - val_loss: 10.0235\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0755 - val_loss: 10.0010\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0863 - val_loss: 9.9452\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.1019 - val_loss: 9.8806\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0641 - val_loss: 9.7375\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0935 - val_loss: 10.1252\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0725 - val_loss: 9.9275\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0962 - val_loss: 9.8786\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0721 - val_loss: 9.8258\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0743 - val_loss: 9.9856\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0618 - val_loss: 10.1032\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.1231 - val_loss: 9.8263\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.1005 - val_loss: 9.8491\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0934 - val_loss: 9.8615\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0847 - val_loss: 9.7167\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0783 - val_loss: 10.0742\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0782 - val_loss: 9.9271\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0771 - val_loss: 9.7817\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0284 - val_loss: 9.9688\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0543 - val_loss: 9.8398\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0376 - val_loss: 10.2421\n",
      "Epoch 63/128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0190 - val_loss: 9.8104\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0527 - val_loss: 10.0402\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0465 - val_loss: 9.8266\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 39s 156ms/step - loss: 10.0206 - val_loss: 10.0867\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0473 - val_loss: 9.8229\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0344 - val_loss: 9.7256\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0071 - val_loss: 9.7860\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0294 - val_loss: 9.8481\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0530 - val_loss: 10.0155\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0279 - val_loss: 9.8475\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0528 - val_loss: 9.8409\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0250 - val_loss: 9.9201\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0046 - val_loss: 9.8259\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0378 - val_loss: 9.7521\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0530 - val_loss: 9.8165\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0140 - val_loss: 9.8484\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0313 - val_loss: 9.8249\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0255 - val_loss: 10.0625\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0810 - val_loss: 9.8864\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0128 - val_loss: 9.9921\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0351 - val_loss: 9.8080\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0360 - val_loss: 10.2226\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0678 - val_loss: 9.7107\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0336 - val_loss: 10.0345\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0419 - val_loss: 9.7617\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0025 - val_loss: 9.7788\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0475 - val_loss: 9.7937\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0012 - val_loss: 9.8700\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0151 - val_loss: 9.9511\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0064 - val_loss: 9.8731\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 39s 156ms/step - loss: 10.0204 - val_loss: 9.7676\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 39s 156ms/step - loss: 10.0201 - val_loss: 9.8834\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0268 - val_loss: 9.8604\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 39s 156ms/step - loss: 10.0016 - val_loss: 9.8977\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 39s 156ms/step - loss: 9.9967 - val_loss: 10.1351\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 39s 156ms/step - loss: 9.9748 - val_loss: 9.9631\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 39s 156ms/step - loss: 10.0085 - val_loss: 9.8990\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 39s 156ms/step - loss: 10.0380 - val_loss: 9.9746\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 39s 156ms/step - loss: 9.9971 - val_loss: 10.0550\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0266 - val_loss: 9.9456\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 39s 156ms/step - loss: 10.0074 - val_loss: 10.3576\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 39s 156ms/step - loss: 9.9831 - val_loss: 10.3923\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 39s 156ms/step - loss: 9.9888 - val_loss: 10.0982\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 39s 156ms/step - loss: 10.0115 - val_loss: 10.3159\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 39s 156ms/step - loss: 9.9999 - val_loss: 10.4636\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 39s 156ms/step - loss: 10.0243 - val_loss: 10.2838\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 39s 156ms/step - loss: 9.9986 - val_loss: 10.2425\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 39s 156ms/step - loss: 10.0636 - val_loss: 10.0772\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 39s 156ms/step - loss: 10.0178 - val_loss: 10.1282\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 39s 156ms/step - loss: 10.0151 - val_loss: 10.1814\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 39s 156ms/step - loss: 10.0299 - val_loss: 10.1802\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 39s 156ms/step - loss: 10.0154 - val_loss: 10.2792\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0159 - val_loss: 10.6916\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0011 - val_loss: 9.9898\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 9.9909 - val_loss: 11.4554\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0067 - val_loss: 10.8016\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 9.9980 - val_loss: 10.4959\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0194 - val_loss: 11.4067\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0027 - val_loss: 10.3675\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0132 - val_loss: 11.2198\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0041 - val_loss: 10.1402\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 9.9986 - val_loss: 10.2318\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 9.9968 - val_loss: 10.9499\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 9.9679 - val_loss: 10.0469\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 9.9926 - val_loss: 10.4300\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 10.0203 - val_loss: 109.5471\n",
      "\n",
      "Loss: 10954.71%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 8)         80        \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 8)         584       \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 92, 92, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 92, 92, 8)         0         \n",
      "                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " conv2d_2 (Conv2D)           (None, 90, 90, 256)       18688     \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 88, 88, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 8)         18440     \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 86, 86, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 84, 84, 24)        1752      \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 84, 84, 24)       96        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 82, 82, 136)       29512     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 660,324\n",
      "Trainable params: 659,730\n",
      "Non-trainable params: 594\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 20s 76ms/step - loss: 2249.8250 - val_loss: 1020.1544\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 249.8992 - val_loss: 48.1819\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 15.2476 - val_loss: 12.7747\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 10.1974 - val_loss: 12.1801\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 10.0246 - val_loss: 10.6074\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.9942 - val_loss: 10.0711\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.9964 - val_loss: 10.2856\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.9874 - val_loss: 10.2930\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.9899 - val_loss: 10.3062\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.9797 - val_loss: 9.7494\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.9682 - val_loss: 9.8664\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.9676 - val_loss: 9.8701\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.9665 - val_loss: 9.9151\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.9919 - val_loss: 9.9040\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.9951 - val_loss: 10.0202\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.9786 - val_loss: 9.8680\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.9633 - val_loss: 9.7601\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.9789 - val_loss: 9.9741\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.9665 - val_loss: 9.8506\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.9824 - val_loss: 10.3038\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.9808 - val_loss: 9.9012\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.9784 - val_loss: 9.9276\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.9712 - val_loss: 10.1819\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.9778 - val_loss: 10.0366\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.9780 - val_loss: 10.1008\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.9911 - val_loss: 9.8744\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.9963 - val_loss: 9.9531\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 10.0038 - val_loss: 9.9044\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.9637 - val_loss: 10.3318\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 10.0005 - val_loss: 9.8214\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.9742 - val_loss: 9.9157\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.9903 - val_loss: 9.7190\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.9825 - val_loss: 9.9719\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.9778 - val_loss: 10.0756\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.9789 - val_loss: 9.7917\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.9824 - val_loss: 10.2237\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.9839 - val_loss: 9.8723\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 10.0060 - val_loss: 9.8889\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.9736 - val_loss: 9.9224\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.9881 - val_loss: 9.7485\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.9736 - val_loss: 9.8481\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.9737 - val_loss: 10.3525\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.9884 - val_loss: 9.7487\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.9679 - val_loss: 9.8794\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.9890 - val_loss: 10.0279\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.9721 - val_loss: 10.7130\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.9704 - val_loss: 9.7218\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.9646 - val_loss: 9.8877\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.9800 - val_loss: 9.7165\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.9697 - val_loss: 9.8657\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.9681 - val_loss: 9.7892\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.9573 - val_loss: 9.8900\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.9551 - val_loss: 10.4542\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.9459 - val_loss: 9.8070\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.9640 - val_loss: 9.7830\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.9578 - val_loss: 9.9152\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.9445 - val_loss: 9.9518\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.9425 - val_loss: 9.7353\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.9552 - val_loss: 9.7923\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.9561 - val_loss: 10.2057\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.9399 - val_loss: 9.7009\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.9401 - val_loss: 9.8264\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.9361 - val_loss: 9.9574\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.9225 - val_loss: 9.9180\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.9381 - val_loss: 9.7893\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.9218 - val_loss: 10.8999\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.9288 - val_loss: 9.8294\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.9233 - val_loss: 9.6763\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.9128 - val_loss: 9.9063\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.9133 - val_loss: 9.8713\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.9231 - val_loss: 9.7430\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.9144 - val_loss: 9.7384\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.9164 - val_loss: 9.9316\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.9096 - val_loss: 9.8992\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.9054 - val_loss: 9.7646\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.8899 - val_loss: 10.1033\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.8985 - val_loss: 9.8142\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.8898 - val_loss: 10.0178\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.8979 - val_loss: 9.8475\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.8918 - val_loss: 9.7438\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.8968 - val_loss: 9.8569\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.8786 - val_loss: 9.7939\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.8768 - val_loss: 9.7091\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.8592 - val_loss: 9.7894\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.8721 - val_loss: 9.7267\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.8661 - val_loss: 9.6882\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.8618 - val_loss: 9.7650\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.8630 - val_loss: 9.6966\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.8618 - val_loss: 9.8425\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.8524 - val_loss: 9.8897\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.8477 - val_loss: 9.6754\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.8629 - val_loss: 9.7890\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.8397 - val_loss: 9.6997\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.8393 - val_loss: 9.7301\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.8461 - val_loss: 9.8168\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.8200 - val_loss: 9.6729\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.8350 - val_loss: 9.6913\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.8189 - val_loss: 9.7744\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.8161 - val_loss: 9.7346\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.8126 - val_loss: 9.6406\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.8070 - val_loss: 10.0055\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.8200 - val_loss: 9.8229\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.8133 - val_loss: 9.6494\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.8033 - val_loss: 10.5601\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.8034 - val_loss: 9.8859\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.7983 - val_loss: 9.6710\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.7712 - val_loss: 9.8411\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.7800 - val_loss: 9.5498\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.7914 - val_loss: 9.8493\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.7841 - val_loss: 9.7681\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.7833 - val_loss: 9.7873\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.7608 - val_loss: 9.7578\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.7620 - val_loss: 9.6088\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.7552 - val_loss: 9.8095\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.7624 - val_loss: 9.6086\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.7362 - val_loss: 9.8223\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.7412 - val_loss: 9.7274\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.7659 - val_loss: 9.7648\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.7216 - val_loss: 9.6175\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.7389 - val_loss: 9.8361\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.7416 - val_loss: 9.8179\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.7479 - val_loss: 10.0089\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.7270 - val_loss: 9.5980\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.7193 - val_loss: 9.5844\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.6984 - val_loss: 9.7649\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.7288 - val_loss: 9.6388\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.7071 - val_loss: 9.8332\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 9.7126 - val_loss: 9.6348\n",
      "\n",
      "Loss: 963.48%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 8)         80        \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 94, 94, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 94, 94, 8)         0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 8)         584       \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 92, 92, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 8)         584       \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 90, 90, 8)        32        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 8)         584       \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 88, 88, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 136)       9928      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,892\n",
      "Trainable params: 11,826\n",
      "Non-trainable params: 66\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 5s 18ms/step - loss: 2735.6968 - val_loss: 2028.6893\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 1202.1396 - val_loss: 571.6287\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 219.2399 - val_loss: 80.6889\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 33.9660 - val_loss: 18.2341\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 12.7304 - val_loss: 11.0028\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 10.2878 - val_loss: 9.9618\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 4s 17ms/step - loss: 9.9798 - val_loss: 9.8738\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9495 - val_loss: 9.9832\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9367 - val_loss: 9.7400\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9437 - val_loss: 9.7629\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9392 - val_loss: 9.6999\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 4s 17ms/step - loss: 9.9374 - val_loss: 9.7090\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9327 - val_loss: 9.7391\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9361 - val_loss: 9.7444\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9305 - val_loss: 9.7209\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9276 - val_loss: 9.8032\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9341 - val_loss: 9.6911\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9288 - val_loss: 9.7228\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9407 - val_loss: 9.7228\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9320 - val_loss: 9.7928\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9348 - val_loss: 9.9059\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9374 - val_loss: 9.7724\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9336 - val_loss: 9.7971\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9420 - val_loss: 9.9091\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9306 - val_loss: 9.7188\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9371 - val_loss: 9.8856\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9388 - val_loss: 9.6767\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9393 - val_loss: 9.7987\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9472 - val_loss: 9.9529\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9401 - val_loss: 9.8923\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9370 - val_loss: 9.8697\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9291 - val_loss: 9.8166\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9491 - val_loss: 9.6958\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9413 - val_loss: 9.6856\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9391 - val_loss: 9.7834\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9351 - val_loss: 9.7208\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9464 - val_loss: 9.7699\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 4s 17ms/step - loss: 9.9450 - val_loss: 9.7426\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 4s 17ms/step - loss: 9.9448 - val_loss: 9.9188\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 4s 17ms/step - loss: 9.9304 - val_loss: 9.7171\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9416 - val_loss: 9.8013\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 4s 17ms/step - loss: 9.9449 - val_loss: 9.8504\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9521 - val_loss: 9.8409\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9452 - val_loss: 9.6804\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9483 - val_loss: 9.7403\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9392 - val_loss: 9.7146\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9430 - val_loss: 9.8089\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9434 - val_loss: 9.8801\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9373 - val_loss: 9.8278\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9454 - val_loss: 9.7047\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9433 - val_loss: 9.6843\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9491 - val_loss: 9.7001\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9406 - val_loss: 9.7661\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9477 - val_loss: 9.7673\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9430 - val_loss: 9.8149\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9479 - val_loss: 9.7814\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9484 - val_loss: 9.7761\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9434 - val_loss: 9.7832\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9389 - val_loss: 9.8776\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9400 - val_loss: 9.9256\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9396 - val_loss: 9.7180\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9273 - val_loss: 9.7670\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9403 - val_loss: 9.6990\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9422 - val_loss: 9.7463\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9366 - val_loss: 9.7003\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9441 - val_loss: 9.8118\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9469 - val_loss: 9.7514\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 4s 17ms/step - loss: 9.9453 - val_loss: 9.9175\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 4s 17ms/step - loss: 9.9353 - val_loss: 9.6803\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9470 - val_loss: 9.7883\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 4s 17ms/step - loss: 9.9410 - val_loss: 9.7060\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 4s 17ms/step - loss: 9.9410 - val_loss: 9.7546\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9489 - val_loss: 9.7315\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 4s 17ms/step - loss: 9.9416 - val_loss: 9.6902\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9452 - val_loss: 9.7073\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9379 - val_loss: 9.6857\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9470 - val_loss: 9.8180\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9491 - val_loss: 9.7277\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9366 - val_loss: 9.9621\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9419 - val_loss: 9.7705\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9389 - val_loss: 9.7562\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9364 - val_loss: 9.7305\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9395 - val_loss: 9.7430\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9440 - val_loss: 9.6825\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9393 - val_loss: 9.7991\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9428 - val_loss: 9.6925\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9441 - val_loss: 9.7267\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 4s 17ms/step - loss: 9.9509 - val_loss: 9.7876\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9418 - val_loss: 9.7255\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 4s 17ms/step - loss: 9.9425 - val_loss: 9.7211\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9435 - val_loss: 9.7099\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9353 - val_loss: 9.7585\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9472 - val_loss: 9.7123\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 4s 17ms/step - loss: 9.9431 - val_loss: 9.7972\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 4s 17ms/step - loss: 9.9396 - val_loss: 9.7934\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9403 - val_loss: 9.9963\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 4s 17ms/step - loss: 9.9357 - val_loss: 9.6837\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9348 - val_loss: 9.7238\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9369 - val_loss: 9.7482\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9454 - val_loss: 9.8097\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9302 - val_loss: 9.7630\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9439 - val_loss: 9.8523\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9423 - val_loss: 9.7242\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9433 - val_loss: 9.7122\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9427 - val_loss: 9.7122\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9421 - val_loss: 9.7703\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9355 - val_loss: 9.6865\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9408 - val_loss: 9.7168\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9414 - val_loss: 9.7092\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9329 - val_loss: 9.7072\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9378 - val_loss: 9.7640\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 4s 17ms/step - loss: 9.9432 - val_loss: 9.7581\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9483 - val_loss: 9.7767\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9271 - val_loss: 9.8325\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9441 - val_loss: 9.7402\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9363 - val_loss: 9.7729\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9438 - val_loss: 9.7244\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9441 - val_loss: 9.7123\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9403 - val_loss: 9.7735\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9342 - val_loss: 9.7658\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9421 - val_loss: 9.7468\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9357 - val_loss: 9.7510\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9341 - val_loss: 9.7590\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9378 - val_loss: 9.6711\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9439 - val_loss: 9.7442\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9391 - val_loss: 9.7321\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9421 - val_loss: 9.7932\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9378 - val_loss: 9.7977\n",
      "\n",
      "Loss: 979.77%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 8)         80        \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 8)         584       \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 92, 92, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 92, 92, 8)         0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 8)         584       \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 90, 90, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 90, 90, 8)         0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 8)         584       \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 8)         584       \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 86, 86, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 84, 84, 136)       9928      \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 84, 84, 136)      544       \n",
      " hNormalization)                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 82, 82, 8)         9800      \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 82, 82, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 80, 80, 136)       9928      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 32,748\n",
      "Trainable params: 32,410\n",
      "Non-trainable params: 338\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 9s 32ms/step - loss: 2731.5720 - val_loss: 2035.9003\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 7s 29ms/step - loss: 1201.0012 - val_loss: 555.7987\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 7s 29ms/step - loss: 219.3828 - val_loss: 76.3511\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 34.0484 - val_loss: 17.5051\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 12.7393 - val_loss: 10.6811\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 10.2625 - val_loss: 9.8150\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9607 - val_loss: 9.7542\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9235 - val_loss: 9.6849\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9200 - val_loss: 9.7166\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9183 - val_loss: 9.7376\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9169 - val_loss: 9.7676\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9153 - val_loss: 9.6898\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9150 - val_loss: 9.7156\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9169 - val_loss: 9.7615\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9248 - val_loss: 9.7880\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9243 - val_loss: 9.7137\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 7s 29ms/step - loss: 9.9176 - val_loss: 9.6854\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 7s 29ms/step - loss: 9.9241 - val_loss: 9.7346\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 7s 29ms/step - loss: 9.9207 - val_loss: 9.6772\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9197 - val_loss: 9.7352\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9185 - val_loss: 9.7032\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9256 - val_loss: 9.7133\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9203 - val_loss: 9.7481\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9249 - val_loss: 9.7101\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9198 - val_loss: 9.7138\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9222 - val_loss: 9.6933\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9217 - val_loss: 9.7444\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9197 - val_loss: 9.7124\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9408 - val_loss: 9.8141\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9268 - val_loss: 9.7711\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9433 - val_loss: 9.7296\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9434 - val_loss: 9.7905\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9412 - val_loss: 9.7165\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9377 - val_loss: 9.7244\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9462 - val_loss: 9.7141\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9420 - val_loss: 9.7410\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9428 - val_loss: 9.6796\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9419 - val_loss: 9.7545\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9404 - val_loss: 9.7676\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9353 - val_loss: 9.7142\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9373 - val_loss: 9.7337\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9295 - val_loss: 9.7154\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9320 - val_loss: 9.7107\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9359 - val_loss: 9.7756\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9339 - val_loss: 9.7166\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9308 - val_loss: 9.7467\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9245 - val_loss: 9.6846\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9292 - val_loss: 9.7260\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9291 - val_loss: 9.7179\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9421 - val_loss: 9.6889\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9377 - val_loss: 9.7421\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9264 - val_loss: 9.7562\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9171 - val_loss: 9.6960\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9223 - val_loss: 9.6744\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9165 - val_loss: 9.7323\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9213 - val_loss: 9.6914\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9181 - val_loss: 9.6758\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9211 - val_loss: 9.6694\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9148 - val_loss: 9.7267\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9144 - val_loss: 9.6453\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9149 - val_loss: 9.6696\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9068 - val_loss: 9.6824\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9143 - val_loss: 9.6761\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9028 - val_loss: 9.7424\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9035 - val_loss: 9.7071\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9143 - val_loss: 9.6933\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9032 - val_loss: 9.6841\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9093 - val_loss: 9.6595\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9030 - val_loss: 9.6623\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8960 - val_loss: 9.6562\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8958 - val_loss: 9.6623\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8930 - val_loss: 9.6400\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8862 - val_loss: 9.6409\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9001 - val_loss: 9.6483\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9042 - val_loss: 9.7616\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9006 - val_loss: 9.7089\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9082 - val_loss: 9.6999\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8862 - val_loss: 9.7077\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8905 - val_loss: 9.6763\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8881 - val_loss: 9.7227\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8981 - val_loss: 9.7045\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9020 - val_loss: 9.6719\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8916 - val_loss: 9.6988\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8939 - val_loss: 9.7006\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8894 - val_loss: 9.6429\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8936 - val_loss: 9.6889\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8844 - val_loss: 9.6814\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8828 - val_loss: 9.6962\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8770 - val_loss: 9.7228\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8858 - val_loss: 9.6767\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8932 - val_loss: 9.6327\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8870 - val_loss: 9.6478\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8796 - val_loss: 9.7014\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8744 - val_loss: 9.6978\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8875 - val_loss: 9.7265\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8845 - val_loss: 9.6107\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8931 - val_loss: 9.6499\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8771 - val_loss: 9.7167\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8843 - val_loss: 9.6418\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8765 - val_loss: 9.6457\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8797 - val_loss: 9.6654\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8766 - val_loss: 9.6903\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8798 - val_loss: 9.6160\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8821 - val_loss: 9.6360\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8700 - val_loss: 9.6749\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8645 - val_loss: 9.6758\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8725 - val_loss: 9.7022\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8773 - val_loss: 9.6843\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8719 - val_loss: 9.6317\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8716 - val_loss: 9.6712\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8592 - val_loss: 9.7368\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8674 - val_loss: 9.7009\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8667 - val_loss: 9.6479\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8727 - val_loss: 9.6623\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8751 - val_loss: 9.7410\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8617 - val_loss: 9.7321\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8708 - val_loss: 9.6281\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8503 - val_loss: 9.6328\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8604 - val_loss: 9.6748\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8539 - val_loss: 9.6132\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8535 - val_loss: 9.5964\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8764 - val_loss: 9.6398\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8579 - val_loss: 9.6230\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8517 - val_loss: 9.6525\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8509 - val_loss: 9.7318\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8602 - val_loss: 9.6504\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8665 - val_loss: 9.6731\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8484 - val_loss: 9.6651\n",
      "\n",
      "Loss: 966.51%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 8)         80        \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 8)         584       \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 92, 92, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 92, 92, 8)         0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 256)       18688     \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 88, 88, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 8)         18440     \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 86, 86, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 84, 84, 18)        1314      \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 84, 84, 18)       72        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 82, 82, 136)       22168     \n",
      "                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Total params: 652,518\n",
      "Trainable params: 651,936\n",
      "Non-trainable params: 582\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 2418.7087 - val_loss: 1479.3333\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 429.7546 - val_loss: 87.8991\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 26.4787 - val_loss: 13.4619\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 10.6588 - val_loss: 10.3304\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 10.0436 - val_loss: 10.0695\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9972 - val_loss: 9.7682\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9921 - val_loss: 9.9483\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9689 - val_loss: 9.7950\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9726 - val_loss: 9.7469\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9821 - val_loss: 9.7888\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9788 - val_loss: 9.8455\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9680 - val_loss: 9.8026\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9753 - val_loss: 9.9833\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9837 - val_loss: 9.7488\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9696 - val_loss: 9.7339\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9557 - val_loss: 9.8182\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9518 - val_loss: 9.8713\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9772 - val_loss: 9.8125\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9680 - val_loss: 9.8794\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9683 - val_loss: 9.7787\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9724 - val_loss: 9.8697\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9683 - val_loss: 9.7199\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9757 - val_loss: 9.7962\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9514 - val_loss: 9.6691\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9621 - val_loss: 9.7517\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9740 - val_loss: 9.7401\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9683 - val_loss: 9.7435\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9640 - val_loss: 9.7262\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9764 - val_loss: 9.9305\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9802 - val_loss: 9.9418\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9699 - val_loss: 9.7758\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9869 - val_loss: 9.7367\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9620 - val_loss: 9.7949\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9696 - val_loss: 9.7114\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9706 - val_loss: 9.8336\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9653 - val_loss: 9.8122\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9690 - val_loss: 9.7190\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9622 - val_loss: 9.7834\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9605 - val_loss: 9.7453\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9519 - val_loss: 9.7577\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9473 - val_loss: 9.7151\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9593 - val_loss: 9.7620\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9480 - val_loss: 9.7034\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9548 - val_loss: 9.7491\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9562 - val_loss: 9.7673\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9638 - val_loss: 9.7028\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9531 - val_loss: 9.6655\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9424 - val_loss: 9.7423\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9214 - val_loss: 9.7712\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9344 - val_loss: 9.7370\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9415 - val_loss: 9.7173\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9413 - val_loss: 9.7377\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9419 - val_loss: 9.6724\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9215 - val_loss: 9.7957\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9363 - val_loss: 9.6569\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9256 - val_loss: 9.7678\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9207 - val_loss: 9.6877\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 18s 73ms/step - loss: 9.9271 - val_loss: 9.6765\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9471 - val_loss: 9.7012\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9238 - val_loss: 9.7722\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9286 - val_loss: 9.8014\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9192 - val_loss: 9.7018\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9077 - val_loss: 9.7030\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9099 - val_loss: 9.6475\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.8975 - val_loss: 9.7012\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9181 - val_loss: 9.7206\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9022 - val_loss: 9.7898\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.8973 - val_loss: 9.7307\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.9008 - val_loss: 9.7162\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 18s 73ms/step - loss: 9.8972 - val_loss: 9.7238\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 18s 73ms/step - loss: 9.8875 - val_loss: 9.6917\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.8840 - val_loss: 9.6294\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.8723 - val_loss: 9.6051\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 18s 73ms/step - loss: 9.8894 - val_loss: 9.6982\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.8935 - val_loss: 9.7687\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.8674 - val_loss: 9.8402\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 18s 73ms/step - loss: 9.8880 - val_loss: 9.8013\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 18s 73ms/step - loss: 9.8755 - val_loss: 9.7078\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.8697 - val_loss: 9.6577\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 18s 73ms/step - loss: 9.8803 - val_loss: 9.6131\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.8710 - val_loss: 9.7306\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.8613 - val_loss: 9.7114\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 18s 73ms/step - loss: 9.8498 - val_loss: 9.6175\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.8624 - val_loss: 9.7859\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.8611 - val_loss: 9.6464\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 18s 73ms/step - loss: 9.8608 - val_loss: 9.5944\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.8507 - val_loss: 9.6722\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.8524 - val_loss: 9.5649\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.8456 - val_loss: 9.6174\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 18s 73ms/step - loss: 9.8457 - val_loss: 9.5608\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.8237 - val_loss: 9.6410\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 18s 73ms/step - loss: 9.8284 - val_loss: 9.5707\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 18s 73ms/step - loss: 9.8419 - val_loss: 9.6121\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 18s 73ms/step - loss: 9.8258 - val_loss: 9.6176\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.8328 - val_loss: 9.6616\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.8119 - val_loss: 9.5412\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.7992 - val_loss: 9.5968\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 18s 73ms/step - loss: 9.8187 - val_loss: 9.5079\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 18s 73ms/step - loss: 9.8148 - val_loss: 9.5182\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.8089 - val_loss: 9.5824\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 18s 73ms/step - loss: 9.7948 - val_loss: 9.5586\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 18s 73ms/step - loss: 9.7859 - val_loss: 9.7219\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.7835 - val_loss: 9.6095\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.7759 - val_loss: 9.5109\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.7857 - val_loss: 9.7035\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 18s 73ms/step - loss: 9.7797 - val_loss: 9.5595\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 18s 73ms/step - loss: 9.7854 - val_loss: 9.5290\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.7851 - val_loss: 9.5584\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 18s 73ms/step - loss: 9.7598 - val_loss: 9.5606\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 18s 73ms/step - loss: 9.7789 - val_loss: 9.5495\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 18s 73ms/step - loss: 9.7743 - val_loss: 9.5598\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.7744 - val_loss: 9.5299\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.7656 - val_loss: 9.4802\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 18s 73ms/step - loss: 9.7561 - val_loss: 9.6308\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.7466 - val_loss: 9.5297\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.7549 - val_loss: 9.6650\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.7462 - val_loss: 9.5784\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.7547 - val_loss: 9.6207\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.7625 - val_loss: 9.4961\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.7523 - val_loss: 9.4569\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.7528 - val_loss: 9.4878\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.7348 - val_loss: 9.5898\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.7305 - val_loss: 9.7232\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.7448 - val_loss: 9.4966\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.7280 - val_loss: 9.4815\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.7299 - val_loss: 9.5771\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.7390 - val_loss: 9.6177\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 9.7261 - val_loss: 9.4825\n",
      "\n",
      "Loss: 948.25%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 154)       1540      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 154)       213598    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 92, 92, 154)      616       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 92, 92, 154)       0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 256)       355072    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 90, 90, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 90, 90, 256)       0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 134)       308870    \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 134)       161738    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 86, 86, 134)      536       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 84, 84, 8)         9656      \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 84, 84, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 82, 82, 8)         584       \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 82, 82, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 80, 80, 136)       9928      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,063,230\n",
      "Trainable params: 1,062,108\n",
      "Non-trainable params: 1,122\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 42s 159ms/step - loss: 2734.2053 - val_loss: 2057.4207\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 1204.2435 - val_loss: 496.5248\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 220.3388 - val_loss: 66.2863\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 34.0352 - val_loss: 16.2543\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 12.7113 - val_loss: 10.5638\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 10.2545 - val_loss: 9.7900\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 9.9455 - val_loss: 9.6961\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 9.9062 - val_loss: 9.6707\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 9.8904 - val_loss: 9.6486\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 9.8818 - val_loss: 9.8560\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 9.8777 - val_loss: 9.6515\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 9.8694 - val_loss: 9.6600\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 9.8539 - val_loss: 9.6181\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 9.8498 - val_loss: 9.6261\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 9.8412 - val_loss: 9.6458\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 9.8256 - val_loss: 9.6091\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 9.8224 - val_loss: 9.6032\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 9.8063 - val_loss: 9.5869\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 9.8014 - val_loss: 9.5824\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 9.7863 - val_loss: 9.5578\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 9.7641 - val_loss: 9.5613\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 38s 154ms/step - loss: 9.7539 - val_loss: 9.5000\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 38s 154ms/step - loss: 9.7418 - val_loss: 9.5316\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 38s 154ms/step - loss: 9.7420 - val_loss: 9.5077\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 9.7174 - val_loss: 9.5305\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 9.6900 - val_loss: 9.5231\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 9.6765 - val_loss: 9.4776\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 38s 154ms/step - loss: 9.6641 - val_loss: 9.5390\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 9.6543 - val_loss: 9.5239\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 9.6480 - val_loss: 9.5705\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 9.6318 - val_loss: 11.0976\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 9.6031 - val_loss: 9.8473\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 9.5883 - val_loss: 9.4116\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 9.5695 - val_loss: 9.4887\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 9.5570 - val_loss: 9.4888\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 9.5369 - val_loss: 9.4857\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 9.5115 - val_loss: 9.4109\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 9.4969 - val_loss: 9.3645\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 9.4687 - val_loss: 9.3998\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 9.4496 - val_loss: 9.3758\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 9.4409 - val_loss: 9.5414\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 9.4526 - val_loss: 9.3934\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 9.4047 - val_loss: 9.4241\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 9.3814 - val_loss: 9.2292\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 9.3133 - val_loss: 9.9170\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 9.3648 - val_loss: 9.2407\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 9.3004 - val_loss: 9.2747\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 9.3028 - val_loss: 9.3268\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 9.2258 - val_loss: 9.1227\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 9.1733 - val_loss: 9.2636\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 9.1434 - val_loss: 9.1075\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 9.1146 - val_loss: 9.1294\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 9.1218 - val_loss: 9.0513\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 9.0764 - val_loss: 9.3712\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 9.0492 - val_loss: 9.0325\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 8.9970 - val_loss: 8.9711\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 9.0220 - val_loss: 8.9636\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 8.9897 - val_loss: 8.9210\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 8.9551 - val_loss: 9.0399\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 8.9444 - val_loss: 8.9667\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 8.9281 - val_loss: 9.0158\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 8.9285 - val_loss: 9.0173\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 8.9071 - val_loss: 9.0629\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 8.9116 - val_loss: 9.0998\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 8.8792 - val_loss: 9.1108\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 8.8580 - val_loss: 8.9935\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 8.8426 - val_loss: 9.0979\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 8.8476 - val_loss: 9.0410\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 8.8206 - val_loss: 8.9904\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 8.8104 - val_loss: 8.9580\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 8.8000 - val_loss: 9.0843\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 8.7773 - val_loss: 8.9680\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 8.7572 - val_loss: 8.9649\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 8.7585 - val_loss: 9.0822\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 8.7423 - val_loss: 9.0442\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 8.8904 - val_loss: 8.9467\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 8.7860 - val_loss: 9.0937\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 8.7602 - val_loss: 9.1964\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 8.6839 - val_loss: 8.9499\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 8.6853 - val_loss: 8.9136\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 8.6935 - val_loss: 9.1759\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 8.7296 - val_loss: 26.6435\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 8.6888 - val_loss: 8.9408\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 8.6618 - val_loss: 8.9843\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 8.6571 - val_loss: 8.9217\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 8.6367 - val_loss: 9.0121\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 8.6150 - val_loss: 8.9489\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 8.6148 - val_loss: 9.1535\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 8.6386 - val_loss: 9.0899\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 8.5988 - val_loss: 8.9498\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 8.5912 - val_loss: 8.9546\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 8.6215 - val_loss: 9.0472\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 8.5843 - val_loss: 8.9329\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 8.5564 - val_loss: 8.9404\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 8.5624 - val_loss: 9.0961\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 8.5490 - val_loss: 9.2510\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 8.5735 - val_loss: 10.2075\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 8.5709 - val_loss: 9.1491\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 8.5294 - val_loss: 9.1027\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 8.5391 - val_loss: 8.9131\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 8.5188 - val_loss: 8.9075\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 8.5099 - val_loss: 9.0871\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 8.7297 - val_loss: 8.9867\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 8.5727 - val_loss: 8.8993\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 8.5147 - val_loss: 8.9273\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 8.5005 - val_loss: 8.8859\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 8.4915 - val_loss: 8.9535\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 8.4760 - val_loss: 9.0720\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 8.4726 - val_loss: 8.9194\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 8.4552 - val_loss: 9.0545\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 8.4833 - val_loss: 9.2368\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 8.4525 - val_loss: 9.0479\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 8.4377 - val_loss: 9.1135\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 8.4528 - val_loss: 9.0855\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 8.5872 - val_loss: 9.6481\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 8.7307 - val_loss: 9.1197\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 8.5168 - val_loss: 8.9519\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 8.4590 - val_loss: 8.9860\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 8.4462 - val_loss: 8.9579\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 8.4270 - val_loss: 9.1061\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 8.4321 - val_loss: 8.9514\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 8.4189 - val_loss: 9.3696\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 8.3913 - val_loss: 8.9591\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 8.3957 - val_loss: 9.2968\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 8.4079 - val_loss: 9.2506\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 38s 154ms/step - loss: 8.3540 - val_loss: 9.3869\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 8.3599 - val_loss: 9.0198\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 8.3843 - val_loss: 9.2237\n",
      "\n",
      "Loss: 922.37%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 105)       1050      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 94, 94, 105)      420       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 136)       128656    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 92, 92, 136)      544       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 136)       166600    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 297,274\n",
      "Trainable params: 296,790\n",
      "Non-trainable params: 484\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 16s 58ms/step - loss: 1194.9276 - val_loss: 314.5795\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.8381 - val_loss: 44.8512\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.2608 - val_loss: 10.2257\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.2041 - val_loss: 10.3847\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.1669 - val_loss: 10.6260\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.1832 - val_loss: 10.0471\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.1941 - val_loss: 10.1856\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.1353 - val_loss: 9.7949\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.1461 - val_loss: 9.9295\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.1261 - val_loss: 9.9680\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.1464 - val_loss: 10.4197\n",
      "Epoch 12/128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 14s 56ms/step - loss: 10.1199 - val_loss: 10.1171\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.1636 - val_loss: 9.8884\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.1686 - val_loss: 10.0925\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.1487 - val_loss: 9.8655\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.1232 - val_loss: 10.0367\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.1521 - val_loss: 10.1226\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.1420 - val_loss: 10.0579\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.1313 - val_loss: 9.9621\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.1364 - val_loss: 9.7815\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.1164 - val_loss: 9.8207\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.1283 - val_loss: 9.9412\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.1293 - val_loss: 9.9194\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.1289 - val_loss: 10.1507\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.1376 - val_loss: 9.8340\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.1279 - val_loss: 9.9161\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.1344 - val_loss: 9.9762\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.1144 - val_loss: 9.9038\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.1404 - val_loss: 9.9166\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.1350 - val_loss: 9.9728\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.1492 - val_loss: 9.8231\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.1392 - val_loss: 10.0095\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.1315 - val_loss: 9.7528\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.1223 - val_loss: 9.9635\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.1554 - val_loss: 10.0384\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.1315 - val_loss: 9.8979\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0919 - val_loss: 9.9511\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0970 - val_loss: 10.0444\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.1255 - val_loss: 10.0917\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.1400 - val_loss: 9.7833\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.1068 - val_loss: 9.9086\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.1218 - val_loss: 9.9346\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.1433 - val_loss: 10.0176\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0799 - val_loss: 9.9453\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.1079 - val_loss: 10.1750\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.1037 - val_loss: 9.8850\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.1262 - val_loss: 9.8497\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.1226 - val_loss: 10.0996\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.1004 - val_loss: 9.9725\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0883 - val_loss: 10.0015\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.1058 - val_loss: 9.9321\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.1159 - val_loss: 9.9490\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.1336 - val_loss: 9.8904\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0882 - val_loss: 9.8100\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.1277 - val_loss: 9.8786\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.1120 - val_loss: 9.8963\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0961 - val_loss: 10.0293\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0914 - val_loss: 9.7950\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.1114 - val_loss: 9.7984\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0790 - val_loss: 9.9155\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.1081 - val_loss: 9.8998\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0931 - val_loss: 9.9147\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0920 - val_loss: 9.9227\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0735 - val_loss: 9.8650\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.1078 - val_loss: 9.8211\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.1005 - val_loss: 9.8921\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0930 - val_loss: 9.8211\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0847 - val_loss: 9.9453\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.1118 - val_loss: 9.7728\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0887 - val_loss: 9.8158\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0810 - val_loss: 9.9521\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0795 - val_loss: 10.0434\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.1054 - val_loss: 9.9294\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0858 - val_loss: 9.9132\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.1086 - val_loss: 9.8468\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.1063 - val_loss: 9.8710\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.1125 - val_loss: 9.8159\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0759 - val_loss: 10.0582\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.1092 - val_loss: 9.8893\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0731 - val_loss: 9.9195\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0834 - val_loss: 10.1079\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0899 - val_loss: 9.8166\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0567 - val_loss: 9.8580\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.1105 - val_loss: 10.0853\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.1227 - val_loss: 9.9350\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.1065 - val_loss: 9.7705\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0822 - val_loss: 9.9700\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0957 - val_loss: 9.8848\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 10.0745 - val_loss: 9.8429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0751 - val_loss: 9.7737\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 10.0748 - val_loss: 9.9157\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.1015 - val_loss: 9.8562\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0776 - val_loss: 9.9316\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0959 - val_loss: 9.7610\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 10.1018 - val_loss: 9.9880\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0984 - val_loss: 9.9573\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.1039 - val_loss: 9.9762\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0771 - val_loss: 9.9694\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.1065 - val_loss: 10.0727\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0730 - val_loss: 10.0287\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0804 - val_loss: 9.8712\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0530 - val_loss: 9.7644\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0797 - val_loss: 9.8847\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.1031 - val_loss: 9.7681\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0812 - val_loss: 9.8204\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0961 - val_loss: 9.8768\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0750 - val_loss: 9.8584\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0571 - val_loss: 9.8091\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.1105 - val_loss: 9.7975\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0877 - val_loss: 9.7639\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0732 - val_loss: 9.7587\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0496 - val_loss: 9.8299\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0889 - val_loss: 9.8115\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0803 - val_loss: 9.8712\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 10.0828 - val_loss: 9.7458\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0655 - val_loss: 9.8619\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 10.1005 - val_loss: 9.7799\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0538 - val_loss: 9.9712\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0574 - val_loss: 9.7531\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0933 - val_loss: 9.8553\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 10.0613 - val_loss: 9.8593\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.1163 - val_loss: 9.8843\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0819 - val_loss: 9.9327\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0901 - val_loss: 10.0451\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0766 - val_loss: 9.7879\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0719 - val_loss: 10.1149\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0906 - val_loss: 9.8418\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0518 - val_loss: 9.8077\n",
      "\n",
      "Loss: 980.77%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 8)         80        \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 94, 94, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 8)         584       \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 92, 92, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 8)         584       \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 90, 90, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 136)       9928      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,276\n",
      "Trainable params: 11,226\n",
      "Non-trainable params: 50\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 5s 16ms/step - loss: 2733.1042 - val_loss: 2022.6289\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 1200.9059 - val_loss: 516.1339\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 218.8988 - val_loss: 70.9418\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 33.8280 - val_loss: 16.8763\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 12.7065 - val_loss: 10.7282\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 10.2960 - val_loss: 9.8898\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 10.0005 - val_loss: 9.7512\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9639 - val_loss: 9.7480\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9540 - val_loss: 9.7324\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9528 - val_loss: 9.7015\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9416 - val_loss: 9.6885\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9482 - val_loss: 9.7268\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9385 - val_loss: 9.7236\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9374 - val_loss: 9.8309\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9407 - val_loss: 9.8172\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9355 - val_loss: 9.6900\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9336 - val_loss: 9.7526\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9328 - val_loss: 9.7578\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9367 - val_loss: 9.7067\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9341 - val_loss: 9.7403\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9419 - val_loss: 9.6882\n",
      "Epoch 22/128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9385 - val_loss: 9.7214\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9377 - val_loss: 9.7007\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9370 - val_loss: 9.7282\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9332 - val_loss: 9.6778\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9382 - val_loss: 9.7464\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9386 - val_loss: 9.7851\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9454 - val_loss: 9.7255\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9468 - val_loss: 9.6983\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9319 - val_loss: 9.7164\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9475 - val_loss: 9.7044\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9475 - val_loss: 9.7979\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9391 - val_loss: 9.6846\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9396 - val_loss: 9.7956\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9395 - val_loss: 9.7234\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9396 - val_loss: 9.7393\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9471 - val_loss: 9.7083\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9280 - val_loss: 9.8566\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9507 - val_loss: 9.7132\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9444 - val_loss: 9.8845\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9371 - val_loss: 9.6896\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9435 - val_loss: 9.7361\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9543 - val_loss: 9.7544\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9346 - val_loss: 9.6978\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9370 - val_loss: 9.7424\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9487 - val_loss: 9.6987\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9424 - val_loss: 9.7401\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9330 - val_loss: 9.7537\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9377 - val_loss: 9.7322\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9426 - val_loss: 9.7273\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9484 - val_loss: 9.7804\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9399 - val_loss: 9.8076\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9384 - val_loss: 9.6885\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9393 - val_loss: 9.6871\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9438 - val_loss: 9.7361\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9389 - val_loss: 9.7991\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9389 - val_loss: 9.7646\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9402 - val_loss: 9.8288\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9460 - val_loss: 9.7158\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9515 - val_loss: 9.7605\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9435 - val_loss: 9.7666\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9370 - val_loss: 9.7457\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9374 - val_loss: 9.7098\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9342 - val_loss: 9.7234\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9408 - val_loss: 9.7429\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9311 - val_loss: 9.7174\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9354 - val_loss: 9.6768\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9433 - val_loss: 9.7881\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9401 - val_loss: 9.8020\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9449 - val_loss: 9.7411\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9393 - val_loss: 9.7734\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9471 - val_loss: 9.7063\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9325 - val_loss: 9.7541\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9357 - val_loss: 9.7286\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9338 - val_loss: 9.7088\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9501 - val_loss: 9.7432\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9465 - val_loss: 9.8019\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9275 - val_loss: 9.7180\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9467 - val_loss: 9.6835\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9339 - val_loss: 9.7568\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9388 - val_loss: 9.6760\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9473 - val_loss: 9.7194\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9537 - val_loss: 9.7074\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9391 - val_loss: 9.6787\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9434 - val_loss: 9.7461\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9455 - val_loss: 9.6808\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9436 - val_loss: 9.8239\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9367 - val_loss: 9.7717\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9486 - val_loss: 9.7425\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9317 - val_loss: 9.6999\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9438 - val_loss: 9.7591\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9435 - val_loss: 9.7220\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9334 - val_loss: 9.7277\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9391 - val_loss: 9.7564\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9387 - val_loss: 9.7546\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9306 - val_loss: 9.7837\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9386 - val_loss: 9.7986\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9373 - val_loss: 9.7241\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9398 - val_loss: 9.7406\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9381 - val_loss: 9.7382\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9368 - val_loss: 9.6892\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9432 - val_loss: 9.7500\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9346 - val_loss: 9.6778\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9315 - val_loss: 9.7144\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9431 - val_loss: 9.7639\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9378 - val_loss: 9.6996\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9442 - val_loss: 9.7169\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9382 - val_loss: 9.7766\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9393 - val_loss: 9.7105\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9425 - val_loss: 9.7134\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9398 - val_loss: 9.6960\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9353 - val_loss: 9.6990\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9337 - val_loss: 9.7143\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9359 - val_loss: 9.7303\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9342 - val_loss: 9.7617\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9445 - val_loss: 9.7273\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9409 - val_loss: 9.7287\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9292 - val_loss: 9.7112\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9385 - val_loss: 9.7882\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9425 - val_loss: 9.7267\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9355 - val_loss: 9.6994\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9500 - val_loss: 9.7234\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9301 - val_loss: 9.7023\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9446 - val_loss: 9.6833\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9497 - val_loss: 9.7158\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9377 - val_loss: 9.7228\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9370 - val_loss: 9.7151\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9506 - val_loss: 9.7072\n",
      "\n",
      "Loss: 970.72%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 8)         80        \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 94, 94, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 8)         584       \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 92, 92, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 8)         584       \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 90, 90, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 136)       9928      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,276\n",
      "Trainable params: 11,226\n",
      "Non-trainable params: 50\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 2732.9670 - val_loss: 1956.9004\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 1201.4828 - val_loss: 413.5317\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 219.3202 - val_loss: 57.1266\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 33.9730 - val_loss: 15.5489\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 12.7700 - val_loss: 10.5474\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 10.3078 - val_loss: 10.0108\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 10.0029 - val_loss: 9.8380\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9701 - val_loss: 9.9071\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9617 - val_loss: 9.7626\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9477 - val_loss: 9.8688\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9504 - val_loss: 9.8385\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9545 - val_loss: 9.9172\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9510 - val_loss: 9.7988\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9437 - val_loss: 9.7727\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9422 - val_loss: 9.7895\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9475 - val_loss: 9.7405\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9452 - val_loss: 9.8471\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9417 - val_loss: 9.7634\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9397 - val_loss: 9.7708\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9453 - val_loss: 9.7831\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9477 - val_loss: 9.7314\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9394 - val_loss: 9.7560\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9439 - val_loss: 9.7309\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9466 - val_loss: 9.7742\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9355 - val_loss: 9.8555\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9454 - val_loss: 9.7348\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9442 - val_loss: 9.7115\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9579 - val_loss: 9.9210\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9453 - val_loss: 9.7949\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9476 - val_loss: 9.6916\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9451 - val_loss: 9.7584\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9461 - val_loss: 9.6854\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9421 - val_loss: 9.7211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9501 - val_loss: 9.7364\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9447 - val_loss: 9.9342\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 9.9418 - val_loss: 9.7356\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9501 - val_loss: 9.6925\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9387 - val_loss: 9.7559\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9453 - val_loss: 9.8076\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9445 - val_loss: 9.7515\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9427 - val_loss: 9.7457\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9446 - val_loss: 9.7117\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9542 - val_loss: 9.7116\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9496 - val_loss: 9.7551\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9404 - val_loss: 9.8744\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9514 - val_loss: 9.7112\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9553 - val_loss: 9.7104\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9597 - val_loss: 9.7554\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9423 - val_loss: 9.6916\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9454 - val_loss: 9.7364\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9452 - val_loss: 9.7382\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9405 - val_loss: 9.8490\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9485 - val_loss: 9.7535\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9432 - val_loss: 9.8057\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9460 - val_loss: 9.7005\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9440 - val_loss: 9.7329\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9383 - val_loss: 9.7572\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9535 - val_loss: 9.7067\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9383 - val_loss: 9.7261\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9459 - val_loss: 9.7814\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9518 - val_loss: 9.7747\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9473 - val_loss: 9.6980\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9387 - val_loss: 9.7807\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9389 - val_loss: 9.7573\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9524 - val_loss: 9.7077\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9390 - val_loss: 9.7116\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9398 - val_loss: 9.6982\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9491 - val_loss: 9.7301\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9481 - val_loss: 9.7642\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9392 - val_loss: 9.7211\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9468 - val_loss: 9.6915\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9433 - val_loss: 9.7029\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9444 - val_loss: 9.7934\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9485 - val_loss: 9.7699\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9399 - val_loss: 9.7761\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9449 - val_loss: 9.7009\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9462 - val_loss: 9.7874\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9422 - val_loss: 9.7265\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9466 - val_loss: 9.7388\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9402 - val_loss: 9.7052\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9398 - val_loss: 9.7449\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9384 - val_loss: 9.7463\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9385 - val_loss: 9.7431\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9397 - val_loss: 9.8635\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9423 - val_loss: 9.7725\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9423 - val_loss: 9.6816\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9364 - val_loss: 9.8521\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9470 - val_loss: 9.7275\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9375 - val_loss: 9.7089\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9348 - val_loss: 9.7919\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9386 - val_loss: 9.7080\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9396 - val_loss: 9.7071\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9506 - val_loss: 9.7943\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9467 - val_loss: 9.8147\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9407 - val_loss: 9.7300\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9459 - val_loss: 9.7803\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9412 - val_loss: 9.7590\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9467 - val_loss: 9.7480\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9385 - val_loss: 9.7774\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9405 - val_loss: 9.7438\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9433 - val_loss: 9.7387\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9466 - val_loss: 9.6860\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9347 - val_loss: 9.8088\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9411 - val_loss: 9.8986\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9449 - val_loss: 9.7471\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9391 - val_loss: 9.7348\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9432 - val_loss: 9.7865\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9467 - val_loss: 9.7654\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9348 - val_loss: 9.7375\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9361 - val_loss: 9.7067\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9413 - val_loss: 9.7274\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9407 - val_loss: 9.7584\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9403 - val_loss: 9.7332\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9436 - val_loss: 9.7748\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9319 - val_loss: 9.7262\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9433 - val_loss: 9.7849\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9437 - val_loss: 9.7000\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9390 - val_loss: 9.7856\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9350 - val_loss: 9.8452\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9442 - val_loss: 9.7716\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9297 - val_loss: 9.7337\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9495 - val_loss: 9.7230\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9411 - val_loss: 9.7857\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9366 - val_loss: 9.8137\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9437 - val_loss: 9.7396\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9475 - val_loss: 9.7767\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9467 - val_loss: 9.7177\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 9.9404 - val_loss: 9.7673\n",
      "\n",
      "Loss: 976.73%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 8)         80        \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 8)         584       \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 92, 92, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 92, 92, 8)         0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 74)        5402      \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 90, 90, 74)       296       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 90, 90, 74)        0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 256)       170752    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 88, 88, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 8)         18440     \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 86, 86, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 84, 84, 8)         584       \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 84, 84, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 82, 82, 136)       9928      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 207,190\n",
      "Trainable params: 206,480\n",
      "Non-trainable params: 710\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 15s 58ms/step - loss: 2734.5496 - val_loss: 1989.6639\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 1202.5469 - val_loss: 504.6726\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 219.1583 - val_loss: 68.0290\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 33.7080 - val_loss: 16.8262\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 12.6305 - val_loss: 10.6940\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.2383 - val_loss: 9.8156\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.9471 - val_loss: 9.7359\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.9217 - val_loss: 9.6965\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.9181 - val_loss: 9.7185\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.9168 - val_loss: 9.7395\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.9138 - val_loss: 9.7551\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.9144 - val_loss: 9.7280\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.9176 - val_loss: 9.7172\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.9155 - val_loss: 9.6945\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.9121 - val_loss: 9.7523\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.9113 - val_loss: 9.7146\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.9110 - val_loss: 9.6766\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.9106 - val_loss: 9.6894\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.9101 - val_loss: 9.7257\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.9182 - val_loss: 9.7301\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.9100 - val_loss: 9.7776\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.9153 - val_loss: 9.6984\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.9184 - val_loss: 9.7394\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.9178 - val_loss: 9.6958\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.9097 - val_loss: 9.7011\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.9103 - val_loss: 9.7839\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.9082 - val_loss: 9.6919\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.9011 - val_loss: 9.7368\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.9032 - val_loss: 9.6699\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.8963 - val_loss: 9.7462\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.8994 - val_loss: 9.7060\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.9006 - val_loss: 9.6790\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.8932 - val_loss: 9.7593\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.8837 - val_loss: 9.6975\n",
      "Epoch 35/128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 14s 56ms/step - loss: 9.8854 - val_loss: 9.7519\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.8893 - val_loss: 9.8044\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.8804 - val_loss: 9.6912\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.8828 - val_loss: 9.7351\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.8764 - val_loss: 9.6880\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.8716 - val_loss: 9.7666\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.8884 - val_loss: 9.7162\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.8712 - val_loss: 9.7218\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.8738 - val_loss: 9.6972\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.8709 - val_loss: 9.7285\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.8667 - val_loss: 9.6804\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.8689 - val_loss: 9.6972\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.8639 - val_loss: 9.7513\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.8653 - val_loss: 9.7549\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.8619 - val_loss: 9.6927\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.8534 - val_loss: 9.6793\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.8601 - val_loss: 9.6776\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.8555 - val_loss: 9.6688\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.8455 - val_loss: 9.8284\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.8481 - val_loss: 9.8928\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.8467 - val_loss: 9.9146\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.8339 - val_loss: 9.6484\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.8423 - val_loss: 9.7493\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.8375 - val_loss: 9.7517\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.8373 - val_loss: 9.7387\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.8524 - val_loss: 9.7073\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.8323 - val_loss: 9.7381\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.8420 - val_loss: 10.5499\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.8483 - val_loss: 9.7111\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.8346 - val_loss: 9.7833\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.8284 - val_loss: 9.7520\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.8261 - val_loss: 9.7354\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.8233 - val_loss: 9.7963\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.8199 - val_loss: 9.7412\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.8263 - val_loss: 9.6957\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.8330 - val_loss: 10.0748\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.8240 - val_loss: 10.9069\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.8332 - val_loss: 10.2202\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.8247 - val_loss: 9.7465\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.8165 - val_loss: 10.0071\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.8286 - val_loss: 9.7476\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.8167 - val_loss: 9.9419\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.8173 - val_loss: 10.1079\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.8215 - val_loss: 11.5494\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.8201 - val_loss: 9.7592\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.8114 - val_loss: 9.6865\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.8153 - val_loss: 10.0139\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.8168 - val_loss: 10.8523\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.8027 - val_loss: 9.7668\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.8168 - val_loss: 9.6516\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.8168 - val_loss: 9.8006\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.8032 - val_loss: 9.6847\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.8189 - val_loss: 9.6723\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.7958 - val_loss: 9.7845\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.7918 - val_loss: 9.7393\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.7982 - val_loss: 9.8675\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.7923 - val_loss: 9.7396\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.7802 - val_loss: 9.6890\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.7802 - val_loss: 9.8242\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.7703 - val_loss: 9.8219\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.7564 - val_loss: 9.6967\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.7539 - val_loss: 9.6692\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.7552 - val_loss: 10.0039\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.7461 - val_loss: 9.6161\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.7515 - val_loss: 11.2651\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.7429 - val_loss: 9.6122\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.7415 - val_loss: 9.7873\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.7510 - val_loss: 9.7519\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.7491 - val_loss: 9.6676\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.7482 - val_loss: 9.5937\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.7373 - val_loss: 9.6074\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.7325 - val_loss: 9.7253\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.7273 - val_loss: 9.6643\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.7368 - val_loss: 9.6766\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.7221 - val_loss: 9.7563\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.7220 - val_loss: 9.6710\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.7231 - val_loss: 10.8212\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.7233 - val_loss: 9.6538\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.7247 - val_loss: 9.6678\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.7268 - val_loss: 9.7406\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.7237 - val_loss: 9.8149\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.7189 - val_loss: 9.6329\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.7141 - val_loss: 9.6724\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.7089 - val_loss: 9.6316\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.7054 - val_loss: 9.7551\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.7182 - val_loss: 9.7639\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.7081 - val_loss: 9.8466\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.7110 - val_loss: 9.7159\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.7046 - val_loss: 9.6432\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.7056 - val_loss: 9.6795\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.7002 - val_loss: 9.6836\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.7098 - val_loss: 9.6244\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.7067 - val_loss: 9.6827\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.6951 - val_loss: 9.7803\n",
      "\n",
      "Loss: 978.03%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 180)       1800      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 180)       291780    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 92, 92, 180)      720       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 92, 92, 180)       0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 237)       384177    \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 237)       505758    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 88, 88, 237)      948       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 88, 88, 237)       0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 256)       546304    \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 84, 84, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 84, 84, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 84, 84, 256)       0         \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 82, 82, 256)       590080    \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 80, 80, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 80, 80, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 78, 78, 78)        179790    \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 78, 78, 78)       312       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 76, 76, 136)       95608     \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 76, 76, 136)      544       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 74, 74, 136)       166600    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,946,633\n",
      "Trainable params: 3,944,345\n",
      "Non-trainable params: 2,288\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 86s 329ms/step - loss: 1202.1805 - val_loss: 58.1151\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 10.9450 - val_loss: 11.3834\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 10.2799 - val_loss: 10.2064\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 10.3061 - val_loss: 9.7854\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 10.1102 - val_loss: 9.9230\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 80s 320ms/step - loss: 10.1587 - val_loss: 10.1118\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 80s 320ms/step - loss: 10.2064 - val_loss: 10.4978\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 80s 320ms/step - loss: 10.1429 - val_loss: 10.6992\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 80s 320ms/step - loss: 10.1258 - val_loss: 9.8275\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 80s 320ms/step - loss: 10.1218 - val_loss: 10.2105\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 80s 320ms/step - loss: 10.0958 - val_loss: 10.6414\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 80s 320ms/step - loss: 10.1820 - val_loss: 10.2393\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 80s 320ms/step - loss: 10.1228 - val_loss: 10.0901\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 80s 320ms/step - loss: 10.1056 - val_loss: 9.9469\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 80s 320ms/step - loss: 10.1266 - val_loss: 10.4026\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 80s 320ms/step - loss: 10.1171 - val_loss: 10.2341\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 80s 320ms/step - loss: 10.0966 - val_loss: 10.5495\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 80s 320ms/step - loss: 10.1116 - val_loss: 10.0563\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 80s 320ms/step - loss: 10.0884 - val_loss: 10.7400\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 80s 320ms/step - loss: 10.0276 - val_loss: 11.3590\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 80s 320ms/step - loss: 10.0147 - val_loss: 10.7814\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 80s 320ms/step - loss: 9.9747 - val_loss: 10.3627\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 80s 320ms/step - loss: 9.9729 - val_loss: 10.0152\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 80s 320ms/step - loss: 9.9532 - val_loss: 10.4438\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 80s 320ms/step - loss: 9.9377 - val_loss: 11.0744\n",
      "Epoch 26/128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 80s 320ms/step - loss: 9.9525 - val_loss: 9.8058\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 80s 320ms/step - loss: 9.8631 - val_loss: 10.3757\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 80s 320ms/step - loss: 9.8291 - val_loss: 9.6283\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 80s 320ms/step - loss: 9.8314 - val_loss: 9.9042\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 80s 320ms/step - loss: 9.7664 - val_loss: 10.1545\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 80s 320ms/step - loss: 9.7436 - val_loss: 10.5222\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 80s 320ms/step - loss: 9.7731 - val_loss: 9.9444\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 80s 320ms/step - loss: 9.6692 - val_loss: 9.5598\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 80s 320ms/step - loss: 9.6501 - val_loss: 9.7347\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 80s 320ms/step - loss: 9.6043 - val_loss: 9.6849\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 80s 320ms/step - loss: 9.5695 - val_loss: 9.4433\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 80s 320ms/step - loss: 9.5538 - val_loss: 10.2784\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 80s 320ms/step - loss: 9.4958 - val_loss: 10.1188\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 80s 320ms/step - loss: 9.3997 - val_loss: 9.5584\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 80s 320ms/step - loss: 9.2387 - val_loss: 9.3556\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 80s 320ms/step - loss: 9.1777 - val_loss: 9.1720\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 80s 320ms/step - loss: 9.0793 - val_loss: 9.1718\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 80s 320ms/step - loss: 9.0330 - val_loss: 9.6274\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 80s 320ms/step - loss: 8.9439 - val_loss: 9.1975\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 80s 320ms/step - loss: 8.8893 - val_loss: 9.8430\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 80s 320ms/step - loss: 8.8818 - val_loss: 8.7597\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 80s 320ms/step - loss: 8.7916 - val_loss: 8.9876\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 8.7160 - val_loss: 8.7973\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 80s 320ms/step - loss: 8.7294 - val_loss: 9.3435\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 8.5957 - val_loss: 10.1315\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 8.6070 - val_loss: 8.9884\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 8.5515 - val_loss: 9.1831\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 80s 320ms/step - loss: 8.4882 - val_loss: 8.3364\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 80s 320ms/step - loss: 8.4653 - val_loss: 8.7692\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 80s 320ms/step - loss: 8.4348 - val_loss: 8.6195\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 80s 320ms/step - loss: 8.4022 - val_loss: 8.8971\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 8.3521 - val_loss: 8.1081\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 8.3203 - val_loss: 9.2824\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 8.2489 - val_loss: 8.2780\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 8.2403 - val_loss: 8.2067\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 8.2051 - val_loss: 9.3832\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 8.1443 - val_loss: 8.2645\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 8.1597 - val_loss: 8.3194\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 8.1163 - val_loss: 8.6678\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 80s 320ms/step - loss: 8.0821 - val_loss: 8.0065\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 8.0356 - val_loss: 8.7373\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 80s 320ms/step - loss: 8.0413 - val_loss: 12.3328\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 80s 320ms/step - loss: 7.9881 - val_loss: 8.2813\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 7.9389 - val_loss: 8.5424\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 7.9176 - val_loss: 8.4778\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 7.9453 - val_loss: 8.4190\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 7.9006 - val_loss: 8.8258\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 7.8410 - val_loss: 8.3443\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 7.8273 - val_loss: 8.3696\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 7.7492 - val_loss: 8.2370\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 7.7856 - val_loss: 8.2966\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 7.7386 - val_loss: 8.2129\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 7.7146 - val_loss: 8.3916\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 7.6646 - val_loss: 8.4359\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 7.6277 - val_loss: 8.3485\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 7.6130 - val_loss: 9.0154\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 8.4962 - val_loss: 9.9431\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 8.3679 - val_loss: 8.0469\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 7.9411 - val_loss: 8.0192\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 7.7548 - val_loss: 8.2232\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 7.6648 - val_loss: 8.6255\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 80s 320ms/step - loss: 7.5593 - val_loss: 7.9326\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 7.5870 - val_loss: 7.9342\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 7.5386 - val_loss: 8.4027\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 7.5051 - val_loss: 8.3422\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 7.4730 - val_loss: 8.0262\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 7.4551 - val_loss: 8.0328\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 7.4449 - val_loss: 8.0035\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 7.3812 - val_loss: 8.3793\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 7.3474 - val_loss: 8.3076\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 7.4020 - val_loss: 8.1003\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 7.3533 - val_loss: 8.1111\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 7.3343 - val_loss: 9.1441\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 7.3256 - val_loss: 8.2927\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 7.2970 - val_loss: 8.1515\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 7.2926 - val_loss: 8.2905\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 7.3402 - val_loss: 13.4701\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 7.3555 - val_loss: 8.1077\n",
      "Epoch 104/128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 80s 319ms/step - loss: 7.2520 - val_loss: 8.0201\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 7.1788 - val_loss: 7.9628\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 7.4550 - val_loss: 8.9184\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 7.3394 - val_loss: 8.0719\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 7.2293 - val_loss: 8.4179\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 7.1909 - val_loss: 8.7646\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 7.1738 - val_loss: 8.1278\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 7.0690 - val_loss: 8.0014\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 7.1021 - val_loss: 7.9869\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 6.9859 - val_loss: 8.0326\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 7.0427 - val_loss: 8.1456\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 6.9897 - val_loss: 7.9910\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 6.9654 - val_loss: 8.4614\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 6.9318 - val_loss: 8.4055\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 6.9455 - val_loss: 8.0696\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 6.9816 - val_loss: 8.3303\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 6.9498 - val_loss: 8.2532\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 7.0620 - val_loss: 10.0607\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 7.0373 - val_loss: 8.1121\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 6.8753 - val_loss: 8.1944\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 6.8727 - val_loss: 8.5087\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 80s 320ms/step - loss: 6.7682 - val_loss: 8.2847\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 6.7964 - val_loss: 8.1955\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 80s 320ms/step - loss: 6.7867 - val_loss: 8.6334\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 80s 320ms/step - loss: 6.7865 - val_loss: 8.4497\n",
      "\n",
      "Loss: 844.97%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 166)       1660      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 166)       248170    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 92, 92, 166)      664       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 92, 92, 166)       0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 256)       382720    \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 88, 88, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 88, 88, 256)       0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 256)       590080    \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 84, 84, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 84, 84, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 84, 84, 256)       0         \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 82, 82, 256)       590080    \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 80, 80, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 80, 80, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 78, 78, 99)        228195    \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 78, 78, 99)       396       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 76, 76, 136)       121312    \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 76, 76, 136)      544       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 74, 74, 136)       166600    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,103,737\n",
      "Trainable params: 4,101,397\n",
      "Non-trainable params: 2,340\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 81s 319ms/step - loss: 1201.1947 - val_loss: 13.6748\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 10.5305 - val_loss: 10.4766\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 10.0657 - val_loss: 10.3063\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 10.0536 - val_loss: 10.3862\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 10.0511 - val_loss: 10.4289\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 10.0619 - val_loss: 10.2586\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 10.0440 - val_loss: 9.9124\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 10.0531 - val_loss: 10.0067\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 10.0901 - val_loss: 9.9640\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 10.0884 - val_loss: 10.2150\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 10.0509 - val_loss: 10.2129\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 10.0731 - val_loss: 420.3156\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 10.1501 - val_loss: 10.0092\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 10.0746 - val_loss: 10.1440\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 79s 315ms/step - loss: 10.1251 - val_loss: 10.0090\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 10.0906 - val_loss: 9.8024\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 79s 315ms/step - loss: 10.0800 - val_loss: 10.8792\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 79s 315ms/step - loss: 10.0924 - val_loss: 10.0366\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 79s 315ms/step - loss: 10.1047 - val_loss: 9.8324\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 10.0730 - val_loss: 10.5516\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 79s 315ms/step - loss: 10.1007 - val_loss: 10.1125\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 10.1008 - val_loss: 9.7778\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 10.1170 - val_loss: 10.0780\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 10.0707 - val_loss: 10.1470\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 79s 315ms/step - loss: 10.0784 - val_loss: 9.7672\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 10.0951 - val_loss: 9.8952\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 10.0563 - val_loss: 9.7499\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 10.0645 - val_loss: 9.7916\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 10.0483 - val_loss: 10.1984\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 10.0220 - val_loss: 9.9813\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 10.0019 - val_loss: 9.7733\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 9.9420 - val_loss: 9.6427\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 9.9160 - val_loss: 9.6573\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 9.8968 - val_loss: 9.9554\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 9.8472 - val_loss: 9.9511\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 9.8353 - val_loss: 9.4341\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 9.7605 - val_loss: 10.1181\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 9.7185 - val_loss: 10.2035\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 9.6485 - val_loss: 9.8980\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 9.6246 - val_loss: 9.3385\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 9.5371 - val_loss: 9.1957\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 9.3800 - val_loss: 9.1499\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 9.2885 - val_loss: 9.0700\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 9.1997 - val_loss: 9.8689\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 9.1176 - val_loss: 9.0600\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 9.0076 - val_loss: 8.8878\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 8.9292 - val_loss: 8.7256\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 8.8960 - val_loss: 8.5818\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 8.7327 - val_loss: 8.3056\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 8.6822 - val_loss: 8.7344\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 8.6227 - val_loss: 8.5172\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 8.5703 - val_loss: 8.8431\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 8.5018 - val_loss: 8.5664\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 8.4777 - val_loss: 8.1667\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 8.4099 - val_loss: 8.4828\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 8.3401 - val_loss: 8.3074\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 8.2945 - val_loss: 8.4956\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 8.2461 - val_loss: 8.1675\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 8.1929 - val_loss: 8.0541\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 8.1684 - val_loss: 8.4683\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 8.0758 - val_loss: 8.1419\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 8.0818 - val_loss: 7.9343\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 8.0341 - val_loss: 8.0484\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 7.9927 - val_loss: 7.9500\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 79s 315ms/step - loss: 7.9649 - val_loss: 7.9557\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 7.9328 - val_loss: 7.9267\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 7.8925 - val_loss: 8.1233\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 7.8795 - val_loss: 7.8709\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 7.7949 - val_loss: 7.8317\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 7.7946 - val_loss: 7.8685\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 7.7463 - val_loss: 7.8486\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 7.7243 - val_loss: 7.7436\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 7.6610 - val_loss: 7.9159\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 7.6565 - val_loss: 8.1114\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 79s 315ms/step - loss: 7.5990 - val_loss: 7.8212\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 7.5510 - val_loss: 7.8032\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 7.5661 - val_loss: 8.0683\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 7.5347 - val_loss: 8.1150\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 79s 315ms/step - loss: 7.5259 - val_loss: 7.8221\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 79s 315ms/step - loss: 7.5193 - val_loss: 7.7927\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 79s 315ms/step - loss: 7.4480 - val_loss: 8.0210\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 7.4153 - val_loss: 7.8273\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 7.3989 - val_loss: 7.8229\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 79s 315ms/step - loss: 7.3920 - val_loss: 7.9770\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 7.3474 - val_loss: 7.7010\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 79s 315ms/step - loss: 7.3356 - val_loss: 7.7810\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 7.3034 - val_loss: 7.6229\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 79s 315ms/step - loss: 7.2934 - val_loss: 7.8681\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 79s 315ms/step - loss: 7.2785 - val_loss: 8.0671\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 79s 315ms/step - loss: 7.2215 - val_loss: 7.9339\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 7.1758 - val_loss: 8.0542\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 79s 315ms/step - loss: 7.1609 - val_loss: 7.9185\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 79s 315ms/step - loss: 7.1607 - val_loss: 7.8556\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 79s 315ms/step - loss: 7.1699 - val_loss: 7.8832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/128\n",
      "250/250 [==============================] - 79s 315ms/step - loss: 7.0902 - val_loss: 7.8682\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 79s 315ms/step - loss: 7.0530 - val_loss: 7.9092\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 79s 315ms/step - loss: 7.0466 - val_loss: 8.0925\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 79s 315ms/step - loss: 7.0782 - val_loss: 8.3843\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 79s 315ms/step - loss: 7.0943 - val_loss: 8.3145\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 79s 315ms/step - loss: 7.0071 - val_loss: 7.8779\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 79s 315ms/step - loss: 6.9852 - val_loss: 7.9942\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 6.9219 - val_loss: 7.9074\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 79s 315ms/step - loss: 6.9016 - val_loss: 7.8323\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 79s 315ms/step - loss: 6.8749 - val_loss: 7.8748\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 79s 315ms/step - loss: 6.8590 - val_loss: 8.0494\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 6.8522 - val_loss: 7.8291\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 79s 315ms/step - loss: 6.8281 - val_loss: 7.9836\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 6.7983 - val_loss: 8.2586\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 79s 315ms/step - loss: 6.8110 - val_loss: 8.1062\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 6.8162 - val_loss: 8.0100\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 6.7799 - val_loss: 8.3131\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 6.7591 - val_loss: 7.9418\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 6.7199 - val_loss: 7.9842\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 79s 315ms/step - loss: 6.6647 - val_loss: 7.9382\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 79s 315ms/step - loss: 6.6757 - val_loss: 8.1300\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 6.6376 - val_loss: 7.8937\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 79s 315ms/step - loss: 6.6059 - val_loss: 8.1767\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 79s 315ms/step - loss: 6.6166 - val_loss: 8.0495\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 79s 315ms/step - loss: 6.6655 - val_loss: 7.9541\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 79s 315ms/step - loss: 6.5760 - val_loss: 8.0486\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 79s 315ms/step - loss: 6.5776 - val_loss: 8.0037\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 79s 315ms/step - loss: 6.5456 - val_loss: 8.2435\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 79s 315ms/step - loss: 6.5550 - val_loss: 7.8664\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 6.5102 - val_loss: 7.9902\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 79s 315ms/step - loss: 6.4920 - val_loss: 8.4707\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 79s 315ms/step - loss: 6.4735 - val_loss: 8.0360\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 79s 315ms/step - loss: 6.4727 - val_loss: 7.9822\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 79s 315ms/step - loss: 6.4095 - val_loss: 8.0127\n",
      "\n",
      "Loss: 801.27%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 8)         80        \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 94, 94, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 94, 94, 8)         0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 60)        4380      \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 60)        32460     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 90, 90, 60)       240       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 8)         4328      \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 88, 88, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 8)         584       \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 86, 86, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 84, 84, 136)       9928      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 52,100\n",
      "Trainable params: 51,930\n",
      "Non-trainable params: 170\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 9s 30ms/step - loss: 2737.5710 - val_loss: 1994.5127\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 1203.4259 - val_loss: 514.0627\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 219.1039 - val_loss: 73.4477\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 33.8276 - val_loss: 17.7725\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 12.6833 - val_loss: 10.6750\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 10.2644 - val_loss: 9.9277\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9704 - val_loss: 9.7782\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9376 - val_loss: 9.7697\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9282 - val_loss: 9.7012\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9241 - val_loss: 9.7124\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9238 - val_loss: 9.7237\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9229 - val_loss: 9.7148\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9233 - val_loss: 9.7037\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9236 - val_loss: 9.6975\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9227 - val_loss: 9.7135\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9240 - val_loss: 9.7071\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9267 - val_loss: 9.7139\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9220 - val_loss: 9.7075\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9177 - val_loss: 9.7425\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9262 - val_loss: 9.7288\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9280 - val_loss: 9.7201\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9211 - val_loss: 9.7217\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9267 - val_loss: 9.7335\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9268 - val_loss: 9.7271\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9252 - val_loss: 9.7233\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9287 - val_loss: 9.6872\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9349 - val_loss: 9.7100\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9338 - val_loss: 9.6861\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9238 - val_loss: 9.7420\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9273 - val_loss: 9.8563\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9381 - val_loss: 9.7043\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9264 - val_loss: 9.7036\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9354 - val_loss: 9.7061\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9246 - val_loss: 9.6904\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9261 - val_loss: 9.7039\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9309 - val_loss: 9.6956\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9277 - val_loss: 9.7596\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9183 - val_loss: 9.6571\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9284 - val_loss: 9.7192\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9243 - val_loss: 9.6788\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9315 - val_loss: 9.7263\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9214 - val_loss: 9.8011\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9233 - val_loss: 9.6888\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9285 - val_loss: 9.6857\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9277 - val_loss: 9.7166\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9257 - val_loss: 9.6759\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9249 - val_loss: 9.7325\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9136 - val_loss: 9.8087\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9268 - val_loss: 9.6993\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9144 - val_loss: 9.6820\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9281 - val_loss: 9.6646\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9143 - val_loss: 9.7303\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9125 - val_loss: 9.6821\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9119 - val_loss: 9.7605\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9119 - val_loss: 9.6820\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9190 - val_loss: 9.6681\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9137 - val_loss: 9.7097\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9059 - val_loss: 9.6879\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9110 - val_loss: 9.6506\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9119 - val_loss: 9.8046\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9080 - val_loss: 9.6857\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9108 - val_loss: 9.7080\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8960 - val_loss: 9.6906\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9002 - val_loss: 9.7017\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8925 - val_loss: 9.6599\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8890 - val_loss: 9.6743\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.9025 - val_loss: 9.7028\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8901 - val_loss: 9.6383\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8884 - val_loss: 9.6760\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8888 - val_loss: 9.6655\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8862 - val_loss: 9.6465\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8813 - val_loss: 9.6979\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8766 - val_loss: 9.6575\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8807 - val_loss: 9.6561\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8815 - val_loss: 9.6580\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8820 - val_loss: 9.7692\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8685 - val_loss: 9.7153\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8649 - val_loss: 9.7037\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8664 - val_loss: 9.7966\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8749 - val_loss: 9.7916\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8721 - val_loss: 9.6462\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8614 - val_loss: 9.6928\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8662 - val_loss: 9.7061\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8580 - val_loss: 9.7839\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8672 - val_loss: 9.7347\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8523 - val_loss: 9.6992\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8509 - val_loss: 9.7632\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8655 - val_loss: 9.6371\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8589 - val_loss: 9.6321\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8481 - val_loss: 9.7301\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8483 - val_loss: 9.6234\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8456 - val_loss: 9.8554\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8466 - val_loss: 9.6628\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8423 - val_loss: 9.6541\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8524 - val_loss: 9.7165\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8445 - val_loss: 9.6961\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8431 - val_loss: 9.7859\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8350 - val_loss: 9.7749\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8416 - val_loss: 9.6064\n",
      "Epoch 100/128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8401 - val_loss: 9.5996\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8310 - val_loss: 9.6995\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8336 - val_loss: 9.7072\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8275 - val_loss: 9.7338\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8319 - val_loss: 9.8040\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8408 - val_loss: 9.6633\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8292 - val_loss: 9.6642\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8344 - val_loss: 9.6978\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8179 - val_loss: 9.6453\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8318 - val_loss: 9.6053\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8293 - val_loss: 9.8796\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8254 - val_loss: 9.8516\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8180 - val_loss: 9.6856\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8245 - val_loss: 9.6363\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8152 - val_loss: 9.7183\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8234 - val_loss: 9.5818\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8249 - val_loss: 9.8300\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8267 - val_loss: 9.6701\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8255 - val_loss: 9.6322\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8186 - val_loss: 9.6798\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.7988 - val_loss: 9.6342\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8222 - val_loss: 9.7128\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8116 - val_loss: 9.6244\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8115 - val_loss: 9.6076\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8248 - val_loss: 9.6177\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8025 - val_loss: 9.6499\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8109 - val_loss: 9.6319\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8050 - val_loss: 9.5999\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 7s 28ms/step - loss: 9.8108 - val_loss: 9.6088\n",
      "\n",
      "Loss: 960.88%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 111)       1110      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 111)       111000    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 92, 92, 111)      444       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 92, 92, 111)       0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 32)        32000     \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 32)        9248      \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 88, 88, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 88, 88, 32)        0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 198)       57222     \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 86, 86, 198)      792       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 84, 84, 119)       212177    \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 84, 84, 119)      476       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 82, 82, 74)        79328     \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 82, 82, 74)       296       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 80, 80, 136)       90712     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 594,937\n",
      "Trainable params: 593,867\n",
      "Non-trainable params: 1,070\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 29s 106ms/step - loss: 1519.1232 - val_loss: 96.1573\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 17.5889 - val_loss: 13.6441\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.1455 - val_loss: 11.7618\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0787 - val_loss: 9.9787\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0421 - val_loss: 9.9801\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0468 - val_loss: 10.3979\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0481 - val_loss: 9.8283\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0397 - val_loss: 9.9557\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0389 - val_loss: 10.0329\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0428 - val_loss: 10.3853\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0470 - val_loss: 9.7849\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0351 - val_loss: 9.9006\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0740 - val_loss: 11.0933\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0492 - val_loss: 9.7831\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0322 - val_loss: 9.8459\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0700 - val_loss: 10.1722\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0678 - val_loss: 9.7739\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0414 - val_loss: 9.9692\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0314 - val_loss: 10.0251\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0460 - val_loss: 10.0851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0549 - val_loss: 10.0957\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0617 - val_loss: 9.7856\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0547 - val_loss: 10.5714\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0480 - val_loss: 10.4139\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0426 - val_loss: 9.8996\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0592 - val_loss: 10.2050\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0673 - val_loss: 10.1547\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0475 - val_loss: 9.9056\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0545 - val_loss: 9.7662\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0401 - val_loss: 9.8457\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0525 - val_loss: 10.2010\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0534 - val_loss: 9.7909\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0505 - val_loss: 10.2299\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0185 - val_loss: 10.2714\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0461 - val_loss: 10.2680\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0649 - val_loss: 9.9077\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0461 - val_loss: 9.7961\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0369 - val_loss: 10.3094\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0248 - val_loss: 9.7510\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0386 - val_loss: 9.8542\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0580 - val_loss: 9.9320\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0406 - val_loss: 9.8290\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0298 - val_loss: 9.7727\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0296 - val_loss: 9.7672\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0724 - val_loss: 9.7413\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0229 - val_loss: 9.7869\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0157 - val_loss: 9.8846\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0576 - val_loss: 9.7521\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0294 - val_loss: 9.7934\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0273 - val_loss: 9.9468\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0282 - val_loss: 10.8312\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0187 - val_loss: 9.7672\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0255 - val_loss: 9.8895\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0162 - val_loss: 9.8059\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0163 - val_loss: 9.7708\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0347 - val_loss: 9.7978\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0213 - val_loss: 9.7256\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0302 - val_loss: 9.8297\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0291 - val_loss: 9.9824\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0259 - val_loss: 9.9636\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0483 - val_loss: 10.1774\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0457 - val_loss: 9.7215\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0330 - val_loss: 9.8484\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0338 - val_loss: 9.7341\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0130 - val_loss: 9.8079\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0445 - val_loss: 9.7375\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0182 - val_loss: 9.8096\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0118 - val_loss: 9.7554\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0171 - val_loss: 9.8542\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0200 - val_loss: 9.8287\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0227 - val_loss: 9.8407\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0236 - val_loss: 9.9502\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0041 - val_loss: 9.9112\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0177 - val_loss: 9.7996\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0316 - val_loss: 9.7723\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0128 - val_loss: 9.7173\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0062 - val_loss: 9.7918\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 9.9781 - val_loss: 9.9392\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0083 - val_loss: 9.8273\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 10.0063 - val_loss: 9.7817\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 9.9961 - val_loss: 9.8402\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 9.9871 - val_loss: 9.9962\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 9.9819 - val_loss: 10.0294\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 9.9717 - val_loss: 10.0759\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 9.9729 - val_loss: 9.8276\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 9.9811 - val_loss: 9.8199\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 9.9722 - val_loss: 10.0074\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 9.9741 - val_loss: 9.8790\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 9.9625 - val_loss: 9.8103\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 9.9582 - val_loss: 10.1329\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 9.9627 - val_loss: 9.8561\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 9.9502 - val_loss: 11.0053\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 9.9711 - val_loss: 10.1603\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 9.9502 - val_loss: 10.5019\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 9.9568 - val_loss: 10.2506\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 9.9587 - val_loss: 9.9871\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 9.9458 - val_loss: 10.5517\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 9.9444 - val_loss: 10.5042\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 9.9421 - val_loss: 10.2365\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 9.9486 - val_loss: 10.3524\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 9.9303 - val_loss: 9.9650\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 9.9366 - val_loss: 10.0743\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 9.9314 - val_loss: 10.1151\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 9.9165 - val_loss: 9.9236\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 9.9143 - val_loss: 10.2608\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 9.9022 - val_loss: 9.9122\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 9.9094 - val_loss: 9.8492\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 9.9177 - val_loss: 9.9290\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 9.9017 - val_loss: 10.1409\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 9.9108 - val_loss: 10.0185\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 9.8976 - val_loss: 10.1007\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 9.9047 - val_loss: 9.9807\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 9.9075 - val_loss: 10.0701\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 9.9014 - val_loss: 10.2669\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 9.9272 - val_loss: 10.2150\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 9.8902 - val_loss: 10.1678\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 9.9057 - val_loss: 10.4535\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 9.8745 - val_loss: 10.4460\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 9.8858 - val_loss: 10.2552\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 9.8730 - val_loss: 10.2335\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 9.8839 - val_loss: 10.9530\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 9.8761 - val_loss: 11.0745\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 9.8771 - val_loss: 11.1098\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 9.8819 - val_loss: 10.7504\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 9.8640 - val_loss: 11.2165\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 9.8703 - val_loss: 11.3806\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 9.8866 - val_loss: 11.4042\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 9.8840 - val_loss: 11.6174\n",
      "\n",
      "Loss: 1161.74%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 256)       2560      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 92, 92, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 92, 92, 256)       0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 121)       278905    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 90, 90, 121)      484       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 119)       129710    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 88, 88, 119)      476       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 117)       125424    \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 86, 86, 117)      468       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 84, 84, 136)       143344    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,272,479\n",
      "Trainable params: 1,271,251\n",
      "Non-trainable params: 1,228\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 38s 146ms/step - loss: 1261.9241 - val_loss: 95.5607\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 11.3756 - val_loss: 13.3437\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.3162 - val_loss: 10.7402\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.1845 - val_loss: 10.9054\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.2174 - val_loss: 11.2103\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.1562 - val_loss: 10.2009\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.1366 - val_loss: 10.2313\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.1266 - val_loss: 10.4010\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.1314 - val_loss: 10.2623\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.1364 - val_loss: 10.1968\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.1112 - val_loss: 10.2350\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.1138 - val_loss: 9.9854\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.1402 - val_loss: 10.1033\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.1185 - val_loss: 10.0119\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.1006 - val_loss: 9.9258\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.0955 - val_loss: 9.9325\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.1081 - val_loss: 10.0029\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.0926 - val_loss: 9.8581\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.1068 - val_loss: 9.9397\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.0933 - val_loss: 10.2495\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.0931 - val_loss: 10.2326\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.1170 - val_loss: 10.4297\n",
      "Epoch 23/128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 35s 141ms/step - loss: 10.0923 - val_loss: 10.0877\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.1149 - val_loss: 10.0010\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.0725 - val_loss: 9.7691\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.1017 - val_loss: 10.2435\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.0947 - val_loss: 10.2302\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.0933 - val_loss: 10.0572\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.0845 - val_loss: 10.7658\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.0953 - val_loss: 10.5108\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.0994 - val_loss: 10.1762\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.0870 - val_loss: 10.0060\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.0666 - val_loss: 10.6706\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.0710 - val_loss: 10.2803\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.0941 - val_loss: 9.8781\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.0927 - val_loss: 9.9784\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.0691 - val_loss: 9.8814\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.0949 - val_loss: 9.9281\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.0694 - val_loss: 10.0087\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.0790 - val_loss: 9.8884\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.0542 - val_loss: 9.7453\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.0840 - val_loss: 10.1068\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.0595 - val_loss: 10.5548\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.0372 - val_loss: 10.4862\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.0555 - val_loss: 9.7309\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.0518 - val_loss: 9.9341\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.0672 - val_loss: 9.9404\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.0594 - val_loss: 10.0468\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.0384 - val_loss: 9.9775\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.0447 - val_loss: 9.9673\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.0475 - val_loss: 10.0344\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.0605 - val_loss: 10.0400\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.0623 - val_loss: 9.8332\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.0198 - val_loss: 10.1448\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.0841 - val_loss: 10.2677\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.0440 - val_loss: 10.4821\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.0579 - val_loss: 9.7529\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.0580 - val_loss: 9.8732\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.0305 - val_loss: 9.9408\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.0469 - val_loss: 10.0440\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.0109 - val_loss: 10.3498\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.0504 - val_loss: 9.8357\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.0313 - val_loss: 10.1489\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.0276 - val_loss: 9.9017\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.0171 - val_loss: 9.9348\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.0319 - val_loss: 10.2675\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.0127 - val_loss: 9.8580\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.0041 - val_loss: 9.9120\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.0184 - val_loss: 10.2446\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.0396 - val_loss: 9.8074\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 9.9974 - val_loss: 9.8681\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.0312 - val_loss: 9.9561\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 9.9858 - val_loss: 10.1166\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.0161 - val_loss: 22.1060\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.0018 - val_loss: 9.9645\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.0101 - val_loss: 9.9289\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 9.9928 - val_loss: 10.2235\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.0225 - val_loss: 10.0334\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 9.9791 - val_loss: 9.8514\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 9.9967 - val_loss: 10.1151\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.0143 - val_loss: 9.7681\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.0036 - val_loss: 10.0154\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 9.9785 - val_loss: 9.8685\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 9.9717 - val_loss: 9.7786\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 9.9913 - val_loss: 9.7592\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 9.9847 - val_loss: 9.8783\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 9.9759 - val_loss: 9.8956\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 9.9972 - val_loss: 9.7957\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 9.9929 - val_loss: 9.9087\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 9.9911 - val_loss: 10.6018\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 9.9704 - val_loss: 10.0415\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.0045 - val_loss: 9.9597\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 9.9957 - val_loss: 10.2378\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 9.9602 - val_loss: 9.8999\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 9.9641 - val_loss: 9.9950\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 9.9893 - val_loss: 9.7476\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 9.9787 - val_loss: 10.0365\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 9.9605 - val_loss: 9.9640\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 9.9555 - val_loss: 9.8840\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 9.9983 - val_loss: 9.8098\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 9.9640 - val_loss: 9.8192\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 9.9525 - val_loss: 10.0480\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 9.9938 - val_loss: 10.1562\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 10.0035 - val_loss: 10.2454\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 9.9781 - val_loss: 9.9995\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 9.9896 - val_loss: 9.9181\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 9.9513 - val_loss: 9.9474\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 9.9664 - val_loss: 10.0059\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 9.9666 - val_loss: 9.9099\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 9.9478 - val_loss: 18.6377\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 9.9681 - val_loss: 9.9257\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 9.9505 - val_loss: 9.8369\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 9.9486 - val_loss: 10.2795\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 9.9409 - val_loss: 9.7803\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 9.9373 - val_loss: 9.9699\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 9.9687 - val_loss: 9.9726\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 9.9606 - val_loss: 10.1308\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 9.9577 - val_loss: 10.1759\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 9.9467 - val_loss: 9.9560\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 9.9574 - val_loss: 9.8092\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 9.9679 - val_loss: 10.0120\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 9.9481 - val_loss: 9.8594\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 9.9401 - val_loss: 9.8733\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 9.9443 - val_loss: 10.0441\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 9.9414 - val_loss: 10.0962\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 9.9568 - val_loss: 9.9010\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 9.9280 - val_loss: 10.0122\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 9.9298 - val_loss: 9.8575\n",
      "\n",
      "Loss: 985.75%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 143)       1430      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 143)       184184    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 92, 92, 143)      572       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 92, 92, 143)       0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 256)       329728    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 90, 90, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 90, 90, 256)       0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 256)       590080    \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 86, 86, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 86, 86, 256)       0         \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 84, 84, 75)        172875    \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 82, 82, 75)        50700     \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 82, 82, 75)       300       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 80, 80, 136)       91936     \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 80, 80, 136)      544       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 78, 78, 136)       166600    \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 78, 78, 136)      544       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 76, 76, 136)       166600    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,348,225\n",
      "Trainable params: 2,346,219\n",
      "Non-trainable params: 2,006\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 67s 253ms/step - loss: 1194.5876 - val_loss: 25.4403\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 61s 246ms/step - loss: 11.1971 - val_loss: 11.8297\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 61s 246ms/step - loss: 10.3025 - val_loss: 9.8535\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 61s 246ms/step - loss: 10.3262 - val_loss: 10.9065\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 61s 246ms/step - loss: 10.2432 - val_loss: 9.8500\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 61s 246ms/step - loss: 10.2337 - val_loss: 10.2143\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 61s 246ms/step - loss: 10.2315 - val_loss: 10.6955\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 61s 246ms/step - loss: 10.2857 - val_loss: 10.6614\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 61s 246ms/step - loss: 10.1715 - val_loss: 9.9409\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 61s 246ms/step - loss: 10.1589 - val_loss: 9.9158\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 61s 246ms/step - loss: 10.1909 - val_loss: 9.8423\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 61s 246ms/step - loss: 10.2252 - val_loss: 10.0978\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 61s 246ms/step - loss: 10.2217 - val_loss: 10.5574\n",
      "Epoch 14/128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 61s 246ms/step - loss: 10.2800 - val_loss: 9.9212\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 61s 246ms/step - loss: 10.1507 - val_loss: 9.9363\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 61s 246ms/step - loss: 10.2262 - val_loss: 10.0205\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 61s 246ms/step - loss: 10.1672 - val_loss: 10.3307\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 61s 246ms/step - loss: 10.1530 - val_loss: 9.8595\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 61s 246ms/step - loss: 10.1690 - val_loss: 10.0185\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 61s 246ms/step - loss: 10.1704 - val_loss: 9.7524\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 61s 246ms/step - loss: 10.1600 - val_loss: 10.0209\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 61s 246ms/step - loss: 10.1686 - val_loss: 9.8942\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 61s 246ms/step - loss: 10.1755 - val_loss: 10.4275\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 61s 246ms/step - loss: 10.1539 - val_loss: 9.9460\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 10.1567 - val_loss: 10.0456\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 10.1330 - val_loss: 10.1111\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 10.1783 - val_loss: 9.9726\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 10.1418 - val_loss: 11.2337\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 10.1267 - val_loss: 10.0390\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 10.1176 - val_loss: 9.8327\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 10.2310 - val_loss: 10.3103\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 10.1222 - val_loss: 9.9178\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 10.1115 - val_loss: 10.2353\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 10.1236 - val_loss: 10.3970\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 10.1238 - val_loss: 10.2643\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 10.0887 - val_loss: 10.5909\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 10.1560 - val_loss: 10.2915\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 10.0981 - val_loss: 9.9118\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 10.0916 - val_loss: 10.0515\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 10.1172 - val_loss: 11.0129\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 10.0982 - val_loss: 10.9745\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 10.0945 - val_loss: 9.9079\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 10.0621 - val_loss: 10.3361\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 10.0892 - val_loss: 10.8155\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 61s 246ms/step - loss: 10.0568 - val_loss: 10.2943\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 10.0944 - val_loss: 9.9914\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 10.0340 - val_loss: 11.3015\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 10.0589 - val_loss: 10.2402\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 10.0285 - val_loss: 11.3191\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 10.0209 - val_loss: 10.0293\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 10.0443 - val_loss: 10.0077\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 10.0210 - val_loss: 9.8593\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 10.0050 - val_loss: 10.0153\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 10.0080 - val_loss: 9.9139\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 9.9975 - val_loss: 10.0097\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 9.9781 - val_loss: 11.0670\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 9.9557 - val_loss: 10.3189\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 9.9236 - val_loss: 9.9428\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 9.9042 - val_loss: 9.8285\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 9.9197 - val_loss: 9.8248\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 9.8694 - val_loss: 10.1195\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 9.8464 - val_loss: 10.3236\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 9.8610 - val_loss: 10.1331\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 9.8578 - val_loss: 9.8319\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 9.8102 - val_loss: 9.7135\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 9.8075 - val_loss: 10.0688\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 9.7497 - val_loss: 10.2510\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 9.7757 - val_loss: 9.8068\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 9.7381 - val_loss: 10.2761\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 9.6920 - val_loss: 9.7280\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 9.6818 - val_loss: 9.9679\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 9.6835 - val_loss: 10.3825\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 9.6181 - val_loss: 10.0611\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 9.6195 - val_loss: 11.0612\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 9.6162 - val_loss: 9.7220\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 9.5846 - val_loss: 9.7748\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 9.5391 - val_loss: 9.9720\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 9.5124 - val_loss: 10.2899\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 9.5765 - val_loss: 15.1189\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 9.5371 - val_loss: 9.7931\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 9.5123 - val_loss: 10.7002\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 9.4497 - val_loss: 10.5943\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 9.4469 - val_loss: 11.5531\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 9.4403 - val_loss: 10.3166\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 9.4181 - val_loss: 10.0098\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 9.4103 - val_loss: 10.3403\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 9.4046 - val_loss: 10.4842\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 9.3835 - val_loss: 11.0680\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 9.3515 - val_loss: 10.8407\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 9.3304 - val_loss: 10.4583\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 9.3136 - val_loss: 11.2621\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 9.2893 - val_loss: 10.4991\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 9.3253 - val_loss: 10.2759\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 9.3011 - val_loss: 9.9428\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 9.2747 - val_loss: 10.6032\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 9.2780 - val_loss: 9.9818\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 9.2393 - val_loss: 9.6798\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 9.2430 - val_loss: 10.0495\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 9.2121 - val_loss: 9.9243\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 9.2098 - val_loss: 9.9386\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 9.1697 - val_loss: 10.9521\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 9.1820 - val_loss: 9.6296\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 9.1737 - val_loss: 10.4699\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 9.1508 - val_loss: 10.2986\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 9.1454 - val_loss: 9.9677\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 9.1667 - val_loss: 9.9207\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 9.1086 - val_loss: 10.6096\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 9.0970 - val_loss: 9.9280\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 9.0943 - val_loss: 10.6106\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 9.0922 - val_loss: 10.3399\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 9.0842 - val_loss: 11.0347\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 9.0725 - val_loss: 10.0574\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 9.0716 - val_loss: 10.1951\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 9.0162 - val_loss: 12.0952\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 9.0250 - val_loss: 10.7838\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 9.0001 - val_loss: 10.3736\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 8.9733 - val_loss: 11.5962\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 9.0103 - val_loss: 10.0665\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 9.0006 - val_loss: 10.5039\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 8.9865 - val_loss: 9.6288\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 8.9844 - val_loss: 9.6726\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 8.9963 - val_loss: 11.0660\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 8.9415 - val_loss: 10.8707\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 8.9654 - val_loss: 9.5624\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 8.9489 - val_loss: 9.9155\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 8.8981 - val_loss: 9.8413\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 8.9354 - val_loss: 9.8925\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 8.8975 - val_loss: 12.9495\n",
      "\n",
      "Loss: 1294.95%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 41)        410       \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 94, 94, 41)       164       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 94, 94, 41)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 25)        9250      \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 25)        5650      \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 90, 90, 25)       100       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 90, 90, 25)        0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 131)       29606     \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 88, 88, 131)      524       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 88, 88, 131)       0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 121)       142780    \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 84, 84, 121)       131890    \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 84, 84, 121)      484       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 82, 82, 27)        29430     \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 82, 82, 27)       108       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 80, 80, 54)        13176     \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 80, 80, 54)       216       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 78, 78, 136)       66232     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 430,024\n",
      "Trainable params: 429,224\n",
      "Non-trainable params: 800\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 25s 89ms/step - loss: 1723.0360 - val_loss: 186.2829\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 34.4625 - val_loss: 10.9924\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 10.3368 - val_loss: 10.3718\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 10.1238 - val_loss: 9.9879\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 10.1030 - val_loss: 9.8083\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 10.0246 - val_loss: 9.8027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 10.0359 - val_loss: 9.8602\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 10.0309 - val_loss: 9.8789\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 10.0254 - val_loss: 9.9154\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 10.0318 - val_loss: 9.7671\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 10.0098 - val_loss: 10.0756\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 10.0243 - val_loss: 9.9557\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 10.0082 - val_loss: 10.2166\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 10.0486 - val_loss: 9.7930\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 10.0317 - val_loss: 9.8055\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 10.0331 - val_loss: 10.6748\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 10.0238 - val_loss: 10.0831\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 10.0420 - val_loss: 9.8216\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 10.0422 - val_loss: 9.8889\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 10.0419 - val_loss: 10.0931\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 10.0222 - val_loss: 9.7512\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 10.0331 - val_loss: 9.8008\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 10.0182 - val_loss: 9.8042\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 10.0418 - val_loss: 9.8250\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 10.0542 - val_loss: 10.1106\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 10.0432 - val_loss: 10.0012\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 10.0390 - val_loss: 9.9071\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 10.0382 - val_loss: 9.9881\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 10.0515 - val_loss: 10.0592\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 10.0515 - val_loss: 9.7635\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 10.0199 - val_loss: 9.9495\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 10.0308 - val_loss: 9.8578\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 10.0510 - val_loss: 9.9668\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 10.0317 - val_loss: 9.9715\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 10.0434 - val_loss: 10.0319\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 10.0485 - val_loss: 10.0863\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 10.0543 - val_loss: 9.8403\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 10.0310 - val_loss: 9.9917\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 10.0474 - val_loss: 9.8080\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 10.0507 - val_loss: 9.9202\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 10.0539 - val_loss: 9.9925\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 10.0205 - val_loss: 9.9593\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 10.0256 - val_loss: 10.5083\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 10.0293 - val_loss: 9.8527\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 10.0115 - val_loss: 9.7811\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 10.0364 - val_loss: 9.7971\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 10.0224 - val_loss: 9.8728\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 10.0164 - val_loss: 9.8280\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 10.0226 - val_loss: 9.7861\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 10.0212 - val_loss: 9.7427\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 10.0042 - val_loss: 9.7050\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.9914 - val_loss: 9.8024\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.9944 - val_loss: 9.7465\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.9902 - val_loss: 9.7694\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.9923 - val_loss: 9.9773\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.9783 - val_loss: 9.7567\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 10.0059 - val_loss: 10.1268\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.9831 - val_loss: 10.3292\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.9358 - val_loss: 9.8924\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.9553 - val_loss: 9.8466\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.9306 - val_loss: 9.6627\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.9638 - val_loss: 9.8003\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.9316 - val_loss: 9.7108\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.9294 - val_loss: 9.7231\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.9149 - val_loss: 10.2495\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.9096 - val_loss: 9.9464\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.9135 - val_loss: 10.1342\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.9104 - val_loss: 9.8275\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.8897 - val_loss: 9.7002\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.8924 - val_loss: 9.8803\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.8763 - val_loss: 9.7950\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.8793 - val_loss: 9.8682\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.8715 - val_loss: 11.0077\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.8510 - val_loss: 10.0132\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.8609 - val_loss: 9.9740\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.8520 - val_loss: 10.0825\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.8331 - val_loss: 9.6477\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.8485 - val_loss: 9.7151\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.8313 - val_loss: 10.1034\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.8078 - val_loss: 9.9923\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.8388 - val_loss: 10.0613\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.8108 - val_loss: 9.9619\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.8228 - val_loss: 9.8960\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.8002 - val_loss: 9.6665\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.8000 - val_loss: 10.2598\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.7736 - val_loss: 10.5987\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.7707 - val_loss: 9.7230\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.7673 - val_loss: 9.8782\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.7558 - val_loss: 9.7689\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.7589 - val_loss: 9.7274\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.7624 - val_loss: 9.9201\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.7428 - val_loss: 9.7842\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.7256 - val_loss: 9.7058\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.7151 - val_loss: 9.7770\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.7100 - val_loss: 10.4129\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.7265 - val_loss: 10.5904\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.7238 - val_loss: 10.5318\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.6920 - val_loss: 9.7844\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.7100 - val_loss: 9.7190\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.6797 - val_loss: 9.6436\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.6932 - val_loss: 9.6499\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.6673 - val_loss: 10.0287\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.6901 - val_loss: 9.8819\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.6649 - val_loss: 9.8825\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.6390 - val_loss: 10.7187\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.6591 - val_loss: 10.0480\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.6262 - val_loss: 10.0663\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.6239 - val_loss: 10.1476\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.6272 - val_loss: 9.7941\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.6233 - val_loss: 9.9669\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.6261 - val_loss: 9.6964\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.6085 - val_loss: 9.8564\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.6189 - val_loss: 9.9751\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.6054 - val_loss: 9.6955\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.5982 - val_loss: 10.1702\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.5919 - val_loss: 10.0018\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.5647 - val_loss: 9.7288\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.5746 - val_loss: 9.9977\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.5643 - val_loss: 10.2277\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.5517 - val_loss: 10.2312\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.5648 - val_loss: 9.7781\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.5462 - val_loss: 9.7947\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.5659 - val_loss: 9.8883\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.5313 - val_loss: 10.0034\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.5467 - val_loss: 9.8920\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.5289 - val_loss: 9.7769\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.5302 - val_loss: 9.6685\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.5185 - val_loss: 9.9804\n",
      "\n",
      "Loss: 998.04%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 221)       2210      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 221)       439790    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 92, 92, 221)      884       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 92, 92, 221)       0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 255)       507450    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 90, 90, 255)      1020      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 11)        25256     \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 88, 88, 11)       44        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 24)        2400      \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 86, 86, 24)       96        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 84, 84, 136)       29512     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,008,666\n",
      "Trainable params: 1,007,642\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 36s 132ms/step - loss: 2250.0435 - val_loss: 1231.8661\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 249.0532 - val_loss: 56.2400\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 15.2715 - val_loss: 16.2822\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.2081 - val_loss: 10.4913\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0618 - val_loss: 10.6006\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0337 - val_loss: 10.1517\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0021 - val_loss: 9.8426\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0000 - val_loss: 9.7697\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 10.0086 - val_loss: 10.0297\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9969 - val_loss: 9.8224\n",
      "Epoch 11/128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9823 - val_loss: 9.9502\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9876 - val_loss: 9.9677\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9728 - val_loss: 9.8074\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9836 - val_loss: 9.7798\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9763 - val_loss: 9.8402\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9808 - val_loss: 9.9086\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9738 - val_loss: 9.8872\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9766 - val_loss: 9.8892\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9756 - val_loss: 9.8894\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9694 - val_loss: 9.8457\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9791 - val_loss: 9.8789\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9790 - val_loss: 9.8506\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9718 - val_loss: 9.8547\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9733 - val_loss: 9.8021\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9721 - val_loss: 9.8679\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9845 - val_loss: 9.8038\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9729 - val_loss: 9.6955\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9869 - val_loss: 9.8454\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9766 - val_loss: 9.7802\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9615 - val_loss: 9.8131\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9791 - val_loss: 9.7326\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9734 - val_loss: 9.9773\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9719 - val_loss: 9.7242\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9597 - val_loss: 9.7178\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9643 - val_loss: 9.7946\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9588 - val_loss: 9.8882\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9848 - val_loss: 9.8486\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9697 - val_loss: 9.8769\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9605 - val_loss: 9.7796\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9593 - val_loss: 9.8190\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9676 - val_loss: 9.7911\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9485 - val_loss: 10.2633\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9558 - val_loss: 10.0073\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9493 - val_loss: 9.8466\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9629 - val_loss: 9.6981\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9508 - val_loss: 9.8446\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9601 - val_loss: 9.7898\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9265 - val_loss: 9.7739\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9397 - val_loss: 9.7964\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9321 - val_loss: 9.7856\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9380 - val_loss: 10.0447\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9337 - val_loss: 9.7140\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9297 - val_loss: 9.7428\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9291 - val_loss: 9.7841\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9276 - val_loss: 9.7184\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9257 - val_loss: 9.8032\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9184 - val_loss: 9.8015\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9224 - val_loss: 9.7102\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9280 - val_loss: 9.7025\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9168 - val_loss: 9.6469\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9250 - val_loss: 9.6903\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9063 - val_loss: 9.7589\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9113 - val_loss: 9.7064\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9022 - val_loss: 9.9293\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9079 - val_loss: 9.6629\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9079 - val_loss: 9.7516\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9065 - val_loss: 9.6832\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.8946 - val_loss: 9.8416\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9023 - val_loss: 9.8383\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.9070 - val_loss: 9.6534\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.8908 - val_loss: 9.7732\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.8803 - val_loss: 9.7540\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.8769 - val_loss: 9.6693\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.8897 - val_loss: 9.6392\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.8791 - val_loss: 9.7101\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.8888 - val_loss: 9.7317\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.8849 - val_loss: 9.6711\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.8753 - val_loss: 9.7114\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.8774 - val_loss: 9.8215\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.8698 - val_loss: 9.6915\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.8758 - val_loss: 9.6432\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.8536 - val_loss: 9.5891\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.8517 - val_loss: 9.7979\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.8448 - val_loss: 9.7602\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.8699 - val_loss: 9.6901\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.8433 - val_loss: 9.5990\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.8553 - val_loss: 9.6075\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.8345 - val_loss: 9.6995\n",
      "Epoch 89/128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 32s 127ms/step - loss: 9.8593 - val_loss: 9.6486\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.8205 - val_loss: 9.5480\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.8231 - val_loss: 9.5985\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.8163 - val_loss: 9.6158\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.8209 - val_loss: 9.5719\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.8159 - val_loss: 9.5689\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.8157 - val_loss: 9.8824\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.8099 - val_loss: 9.6631\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.8131 - val_loss: 9.5636\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.8074 - val_loss: 9.7640\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.7979 - val_loss: 9.7661\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.7998 - val_loss: 9.5570\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.7754 - val_loss: 9.5911\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.7788 - val_loss: 9.5819\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.7799 - val_loss: 9.6654\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.7869 - val_loss: 9.6242\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.7729 - val_loss: 9.5582\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.7804 - val_loss: 9.5902\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.7752 - val_loss: 9.5504\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.7628 - val_loss: 9.5313\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.7566 - val_loss: 9.6266\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.7501 - val_loss: 9.5406\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.7459 - val_loss: 9.4755\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.7598 - val_loss: 9.6452\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.7570 - val_loss: 9.5697\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.7444 - val_loss: 9.5475\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.7134 - val_loss: 9.5456\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.7360 - val_loss: 9.5513\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.7354 - val_loss: 9.4482\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.7235 - val_loss: 9.5714\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.7293 - val_loss: 9.4990\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.7354 - val_loss: 9.5626\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.7389 - val_loss: 9.4774\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.7213 - val_loss: 9.5791\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.7042 - val_loss: 9.4890\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.7158 - val_loss: 9.5273\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.7013 - val_loss: 9.8100\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.7143 - val_loss: 9.6831\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.7058 - val_loss: 9.4621\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 32s 127ms/step - loss: 9.7077 - val_loss: 9.4598\n",
      "\n",
      "Loss: 945.98%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 97)        970       \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 94, 94, 97)       388       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 104)       90896     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 92, 92, 104)      416       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 8)         7496      \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 90, 90, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 136)       9928      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 110,130\n",
      "Trainable params: 109,710\n",
      "Non-trainable params: 420\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 11s 40ms/step - loss: 2735.6187 - val_loss: 2079.5493\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 1204.1293 - val_loss: 517.1361\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 219.3453 - val_loss: 69.9117\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 33.6442 - val_loss: 16.3976\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 12.6208 - val_loss: 10.4652\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 10.2392 - val_loss: 9.7799\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9544 - val_loss: 9.7197\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9246 - val_loss: 9.6822\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9159 - val_loss: 9.6907\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9196 - val_loss: 9.6852\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9187 - val_loss: 9.6949\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9217 - val_loss: 9.6924\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9151 - val_loss: 9.6867\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9202 - val_loss: 9.6864\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9204 - val_loss: 9.7401\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9234 - val_loss: 9.6961\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9197 - val_loss: 9.6843\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9210 - val_loss: 9.6862\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9203 - val_loss: 9.6718\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9182 - val_loss: 9.6822\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9252 - val_loss: 9.7291\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9254 - val_loss: 9.6914\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9174 - val_loss: 9.7461\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9381 - val_loss: 9.6730\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9345 - val_loss: 9.6989\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9219 - val_loss: 9.7509\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9405 - val_loss: 9.6954\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9348 - val_loss: 9.7140\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9346 - val_loss: 9.6822\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9368 - val_loss: 9.6801\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9318 - val_loss: 9.6979\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9361 - val_loss: 9.6974\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9373 - val_loss: 9.7453\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9339 - val_loss: 9.8434\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9372 - val_loss: 9.7866\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9329 - val_loss: 9.6916\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9197 - val_loss: 9.7453\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9397 - val_loss: 9.7047\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9444 - val_loss: 9.7413\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9333 - val_loss: 9.7158\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 9s 37ms/step - loss: 9.9247 - val_loss: 9.7671\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9340 - val_loss: 9.7454\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9370 - val_loss: 9.7114\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9391 - val_loss: 9.7223\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9343 - val_loss: 9.7274\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9272 - val_loss: 9.6952\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9481 - val_loss: 9.7306\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9318 - val_loss: 9.7200\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9371 - val_loss: 9.7021\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9343 - val_loss: 9.6722\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9432 - val_loss: 9.7332\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9408 - val_loss: 9.7059\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9419 - val_loss: 9.7194\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9373 - val_loss: 9.6914\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9429 - val_loss: 9.7323\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9419 - val_loss: 9.7024\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9265 - val_loss: 9.8605\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9362 - val_loss: 9.6938\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9397 - val_loss: 9.7069\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9292 - val_loss: 9.7387\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9435 - val_loss: 9.7173\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9318 - val_loss: 9.7570\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9392 - val_loss: 9.6963\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9382 - val_loss: 9.7677\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9392 - val_loss: 9.7169\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9360 - val_loss: 9.6951\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9359 - val_loss: 9.7140\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9404 - val_loss: 9.7139\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9359 - val_loss: 9.7343\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9403 - val_loss: 9.7512\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9411 - val_loss: 9.7285\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9430 - val_loss: 9.7058\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9358 - val_loss: 9.7182\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9453 - val_loss: 9.7263\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9336 - val_loss: 9.6874\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9250 - val_loss: 9.8027\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9301 - val_loss: 9.7675\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9379 - val_loss: 9.7839\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9338 - val_loss: 9.6872\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9304 - val_loss: 9.7035\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9291 - val_loss: 9.6986\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9408 - val_loss: 9.6985\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9352 - val_loss: 9.7409\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9355 - val_loss: 9.7176\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9342 - val_loss: 9.7697\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9342 - val_loss: 9.7799\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9296 - val_loss: 9.7064\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9477 - val_loss: 9.7426\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9371 - val_loss: 9.7051\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9368 - val_loss: 9.6837\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9314 - val_loss: 9.7025\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9350 - val_loss: 9.6983\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9315 - val_loss: 9.7563\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9369 - val_loss: 9.6966\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9340 - val_loss: 9.7026\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9392 - val_loss: 9.7268\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9369 - val_loss: 9.7369\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9300 - val_loss: 9.7131\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9332 - val_loss: 9.6915\n",
      "Epoch 100/128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9315 - val_loss: 9.7011\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9380 - val_loss: 9.6992\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9332 - val_loss: 9.6979\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9424 - val_loss: 9.7411\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9302 - val_loss: 9.7061\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9319 - val_loss: 9.7387\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9480 - val_loss: 9.6890\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9379 - val_loss: 9.6938\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9357 - val_loss: 9.7015\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9354 - val_loss: 9.7188\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9332 - val_loss: 9.7498\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9233 - val_loss: 9.7213\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9286 - val_loss: 9.6777\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9361 - val_loss: 9.7351\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9403 - val_loss: 9.6937\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9272 - val_loss: 9.6919\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9326 - val_loss: 9.7530\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9282 - val_loss: 9.6801\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9280 - val_loss: 9.7223\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9395 - val_loss: 9.6949\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9343 - val_loss: 9.6702\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9250 - val_loss: 9.7640\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9348 - val_loss: 9.7779\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9409 - val_loss: 9.6784\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9435 - val_loss: 9.6963\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9313 - val_loss: 9.6841\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9414 - val_loss: 9.6808\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9286 - val_loss: 9.7019\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 9s 38ms/step - loss: 9.9329 - val_loss: 9.7082\n",
      "\n",
      "Loss: 970.82%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 8)         80        \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 94, 94, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 136)       9928      \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 92, 92, 136)      544       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 8)         9800      \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 90, 90, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 136)       9928      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,348\n",
      "Trainable params: 30,042\n",
      "Non-trainable params: 306\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 8s 28ms/step - loss: 2734.8044 - val_loss: 2004.0266\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 1199.8866 - val_loss: 620.7921\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 217.3233 - val_loss: 85.4991\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 33.5612 - val_loss: 28.4438\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 12.8335 - val_loss: 15.0293\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 10.4374 - val_loss: 10.5984\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 10.1547 - val_loss: 11.9436\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 10.0872 - val_loss: 10.3886\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 10.0719 - val_loss: 10.0333\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 10.0613 - val_loss: 10.4213\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 10.0597 - val_loss: 10.3078\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 10.0357 - val_loss: 10.4399\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 10.0388 - val_loss: 10.3695\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 10.0442 - val_loss: 10.1424\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9982 - val_loss: 9.9908\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 10.0054 - val_loss: 10.0212\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 10.0163 - val_loss: 10.1846\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 10.0147 - val_loss: 9.9411\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 10.0081 - val_loss: 9.8855\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 10.0082 - val_loss: 9.8940\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9935 - val_loss: 9.7944\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 10.0082 - val_loss: 9.8123\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9989 - val_loss: 10.0702\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 10.0138 - val_loss: 9.9963\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 10.0090 - val_loss: 9.8164\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9903 - val_loss: 9.7612\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9897 - val_loss: 9.8201\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9996 - val_loss: 10.0217\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 10.0020 - val_loss: 9.8152\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 10.0158 - val_loss: 9.8111\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9886 - val_loss: 9.7515\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9874 - val_loss: 9.7755\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9843 - val_loss: 9.8329\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 10.0016 - val_loss: 9.7308\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9985 - val_loss: 9.7330\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9840 - val_loss: 9.7528\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 10.0021 - val_loss: 10.2401\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9844 - val_loss: 9.6879\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9824 - val_loss: 9.7758\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9914 - val_loss: 9.7997\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9922 - val_loss: 9.7476\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9939 - val_loss: 9.7751\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9856 - val_loss: 9.7705\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9728 - val_loss: 9.7748\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9892 - val_loss: 9.8664\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9857 - val_loss: 9.7754\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9700 - val_loss: 9.7354\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9852 - val_loss: 9.8293\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9903 - val_loss: 9.7399\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9757 - val_loss: 9.7708\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9704 - val_loss: 9.8486\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9822 - val_loss: 9.8714\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9658 - val_loss: 9.8080\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9807 - val_loss: 9.7740\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9844 - val_loss: 9.7587\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9831 - val_loss: 9.8254\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9739 - val_loss: 9.7919\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9839 - val_loss: 9.7528\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9897 - val_loss: 9.7325\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9820 - val_loss: 9.7841\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9685 - val_loss: 9.7883\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9700 - val_loss: 9.8048\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9558 - val_loss: 9.7767\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9805 - val_loss: 9.7579\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9708 - val_loss: 9.7303\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9644 - val_loss: 9.7812\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9618 - val_loss: 9.7208\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9585 - val_loss: 9.7337\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9695 - val_loss: 9.7625\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9805 - val_loss: 9.8474\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9727 - val_loss: 9.7512\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9691 - val_loss: 9.7451\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9787 - val_loss: 9.7022\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9676 - val_loss: 9.7875\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9625 - val_loss: 9.7526\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9639 - val_loss: 9.7121\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9582 - val_loss: 9.8211\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9734 - val_loss: 9.7663\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9673 - val_loss: 9.7125\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9736 - val_loss: 9.7261\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9583 - val_loss: 9.7520\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9586 - val_loss: 9.8189\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9543 - val_loss: 9.7492\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9627 - val_loss: 9.7381\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9606 - val_loss: 9.7312\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9587 - val_loss: 9.7090\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9640 - val_loss: 9.8578\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9638 - val_loss: 9.7148\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9550 - val_loss: 9.7331\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9605 - val_loss: 9.7699\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9634 - val_loss: 9.7915\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9704 - val_loss: 9.7919\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9690 - val_loss: 9.7359\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9652 - val_loss: 9.7454\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9666 - val_loss: 9.6956\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9480 - val_loss: 9.7998\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9570 - val_loss: 9.7059\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9590 - val_loss: 9.6973\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9609 - val_loss: 9.7107\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9671 - val_loss: 9.7662\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9655 - val_loss: 9.7829\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9662 - val_loss: 9.7983\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9447 - val_loss: 9.7331\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9666 - val_loss: 9.6985\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9579 - val_loss: 9.6932\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9595 - val_loss: 9.7831\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9470 - val_loss: 9.7566\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9552 - val_loss: 9.7183\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9712 - val_loss: 9.7847\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9576 - val_loss: 9.8275\n",
      "Epoch 111/128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9622 - val_loss: 9.7128\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9536 - val_loss: 9.7800\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9595 - val_loss: 9.7594\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9496 - val_loss: 9.6800\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9555 - val_loss: 9.8025\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9524 - val_loss: 9.7415\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9651 - val_loss: 9.7324\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9502 - val_loss: 9.7483\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9505 - val_loss: 9.7915\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9576 - val_loss: 9.6894\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9497 - val_loss: 9.7424\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9693 - val_loss: 9.7790\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9661 - val_loss: 9.7348\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9487 - val_loss: 9.8182\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9545 - val_loss: 9.6823\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9530 - val_loss: 9.7349\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9572 - val_loss: 9.7531\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 9.9508 - val_loss: 9.6970\n",
      "\n",
      "Loss: 969.70%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 8)         80        \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 8)         584       \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 92, 92, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 92, 92, 8)         0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 169)       12337     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 90, 90, 169)      676       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 127)       193294    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 88, 88, 127)      508       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 8)         9152      \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 86, 86, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 84, 84, 136)       9928      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 226,627\n",
      "Trainable params: 226,001\n",
      "Non-trainable params: 626\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 14s 52ms/step - loss: 2735.2769 - val_loss: 2271.2249\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 1204.0466 - val_loss: 571.2426\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 219.9463 - val_loss: 76.8360\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 34.1203 - val_loss: 17.9776\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 12.8608 - val_loss: 11.1818\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 10.3521 - val_loss: 9.9085\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 10.0492 - val_loss: 9.7954\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9823 - val_loss: 9.7395\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9762 - val_loss: 9.8394\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 10.0009 - val_loss: 9.7166\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9638 - val_loss: 9.7434\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 12s 49ms/step - loss: 9.9774 - val_loss: 9.7684\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9711 - val_loss: 9.8991\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9658 - val_loss: 9.8225\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9755 - val_loss: 9.7817\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9702 - val_loss: 9.8155\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9684 - val_loss: 9.7470\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9565 - val_loss: 9.7358\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9538 - val_loss: 9.7571\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9569 - val_loss: 9.7306\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9731 - val_loss: 9.7116\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9638 - val_loss: 9.7728\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9527 - val_loss: 9.7260\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9569 - val_loss: 9.7532\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 12s 49ms/step - loss: 9.9575 - val_loss: 9.6939\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9581 - val_loss: 9.7016\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9492 - val_loss: 9.7588\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9533 - val_loss: 9.7472\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9707 - val_loss: 9.7145\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9602 - val_loss: 9.7261\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9434 - val_loss: 9.7781\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9604 - val_loss: 9.8576\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 12s 49ms/step - loss: 9.9485 - val_loss: 9.7074\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9543 - val_loss: 9.7048\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9588 - val_loss: 9.6943\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9578 - val_loss: 9.7128\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9669 - val_loss: 9.7076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9617 - val_loss: 9.7640\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9521 - val_loss: 9.7285\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9460 - val_loss: 9.7472\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9765 - val_loss: 9.7556\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9413 - val_loss: 9.7369\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9567 - val_loss: 9.7106\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9390 - val_loss: 9.8230\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9490 - val_loss: 9.6875\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 12s 49ms/step - loss: 9.9506 - val_loss: 9.7128\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9600 - val_loss: 9.7390\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 12s 49ms/step - loss: 9.9581 - val_loss: 9.7197\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9542 - val_loss: 9.6994\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 12s 49ms/step - loss: 9.9410 - val_loss: 9.6753\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9433 - val_loss: 9.7018\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9485 - val_loss: 9.7893\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9466 - val_loss: 9.6845\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 12s 49ms/step - loss: 9.9389 - val_loss: 9.7579\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9480 - val_loss: 9.7137\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9500 - val_loss: 9.6978\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9495 - val_loss: 9.7283\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9419 - val_loss: 9.7321\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9386 - val_loss: 9.6928\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9426 - val_loss: 9.7196\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9390 - val_loss: 9.7246\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9393 - val_loss: 9.7103\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9485 - val_loss: 9.6910\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 12s 49ms/step - loss: 9.9448 - val_loss: 9.6877\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 12s 49ms/step - loss: 9.9409 - val_loss: 9.7766\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 12s 49ms/step - loss: 9.9518 - val_loss: 9.6905\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 12s 49ms/step - loss: 9.9421 - val_loss: 9.7206\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9403 - val_loss: 9.6976\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 12s 49ms/step - loss: 9.9459 - val_loss: 9.6933\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9348 - val_loss: 9.6800\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9315 - val_loss: 9.7993\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 12s 49ms/step - loss: 9.9452 - val_loss: 9.7293\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 12s 49ms/step - loss: 9.9298 - val_loss: 9.7399\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9249 - val_loss: 9.7719\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 12s 49ms/step - loss: 9.9421 - val_loss: 9.7230\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 12s 49ms/step - loss: 9.9395 - val_loss: 9.7334\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 12s 49ms/step - loss: 9.9357 - val_loss: 9.7183\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9322 - val_loss: 9.6834\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 12s 49ms/step - loss: 9.9338 - val_loss: 9.6858\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9386 - val_loss: 9.7005\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9475 - val_loss: 9.6719\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9362 - val_loss: 9.7943\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9470 - val_loss: 9.7673\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9497 - val_loss: 9.6785\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9384 - val_loss: 9.6692\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9408 - val_loss: 9.7089\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9399 - val_loss: 9.7280\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9299 - val_loss: 9.7079\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9351 - val_loss: 9.7001\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9402 - val_loss: 9.6683\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9319 - val_loss: 9.7371\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9402 - val_loss: 9.6838\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9329 - val_loss: 9.8023\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9286 - val_loss: 9.7836\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9326 - val_loss: 9.7221\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9351 - val_loss: 9.7513\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9334 - val_loss: 9.8367\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9396 - val_loss: 9.7116\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9304 - val_loss: 9.7351\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9355 - val_loss: 9.6760\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9315 - val_loss: 9.7102\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9254 - val_loss: 9.7222\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9275 - val_loss: 9.6809\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9302 - val_loss: 9.7673\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9237 - val_loss: 9.7452\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9308 - val_loss: 9.6826\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9315 - val_loss: 9.6896\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9262 - val_loss: 9.7143\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9263 - val_loss: 9.6928\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9267 - val_loss: 9.7045\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9338 - val_loss: 9.6738\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9300 - val_loss: 9.6776\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 12s 49ms/step - loss: 9.9280 - val_loss: 9.6964\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9200 - val_loss: 9.6737\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 12s 49ms/step - loss: 9.9306 - val_loss: 9.7124\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 12s 49ms/step - loss: 9.9310 - val_loss: 9.7238\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 12s 49ms/step - loss: 9.9214 - val_loss: 9.7581\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9167 - val_loss: 9.7590\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 12s 49ms/step - loss: 9.9126 - val_loss: 9.6547\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9147 - val_loss: 9.7761\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 12s 49ms/step - loss: 9.9285 - val_loss: 9.7168\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9286 - val_loss: 9.6807\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9179 - val_loss: 9.7308\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9140 - val_loss: 9.7508\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 12s 49ms/step - loss: 9.9154 - val_loss: 9.7807\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 12s 50ms/step - loss: 9.9203 - val_loss: 9.6842\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 12s 49ms/step - loss: 9.9216 - val_loss: 9.7120\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 12s 49ms/step - loss: 9.9259 - val_loss: 9.7184\n",
      "\n",
      "Loss: 971.84%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 8)         80        \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 94, 94, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 94, 94, 8)         0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 256)       18688     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 92, 92, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 132)       304260    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 90, 90, 132)      528       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 8)         9512      \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 88, 88, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 136)       9928      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 344,088\n",
      "Trainable params: 343,278\n",
      "Non-trainable params: 810\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 19s 69ms/step - loss: 2738.2881 - val_loss: 1997.8584\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 16s 66ms/step - loss: 1204.4087 - val_loss: 493.4671\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 16s 66ms/step - loss: 218.7559 - val_loss: 69.4563\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 33.6358 - val_loss: 16.3845\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 12.6287 - val_loss: 10.5418\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 10.2386 - val_loss: 9.7920\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 16s 66ms/step - loss: 9.9516 - val_loss: 9.7505\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.9232 - val_loss: 9.6871\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.9136 - val_loss: 9.7300\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 16s 66ms/step - loss: 9.9153 - val_loss: 9.6858\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.9155 - val_loss: 9.6802\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.9218 - val_loss: 9.6834\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.9169 - val_loss: 9.6939\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.9176 - val_loss: 9.7302\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.9169 - val_loss: 9.7182\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 16s 66ms/step - loss: 9.9148 - val_loss: 9.7091\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.9170 - val_loss: 9.7213\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.9224 - val_loss: 9.7300\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.9210 - val_loss: 9.7091\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.9183 - val_loss: 9.6995\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.9217 - val_loss: 9.7274\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.9253 - val_loss: 9.7024\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.9246 - val_loss: 9.7220\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 16s 66ms/step - loss: 9.9291 - val_loss: 9.6968\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 16s 66ms/step - loss: 9.9153 - val_loss: 9.7597\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.9257 - val_loss: 9.7513\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.9278 - val_loss: 9.7013\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.9217 - val_loss: 9.7836\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 16s 66ms/step - loss: 9.9254 - val_loss: 9.7007\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 16s 66ms/step - loss: 9.9288 - val_loss: 9.7243\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.9232 - val_loss: 9.7071\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.9259 - val_loss: 9.6849\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.9223 - val_loss: 9.6942\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 16s 66ms/step - loss: 9.9245 - val_loss: 9.6804\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 16s 66ms/step - loss: 9.9186 - val_loss: 9.6862\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.9247 - val_loss: 9.7157\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.9101 - val_loss: 9.7058\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 16s 66ms/step - loss: 9.9214 - val_loss: 9.6591\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.9247 - val_loss: 9.7067\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.9161 - val_loss: 9.6614\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.9186 - val_loss: 9.7011\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.9071 - val_loss: 9.7003\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.9096 - val_loss: 9.6700\n",
      "Epoch 44/128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 17s 66ms/step - loss: 9.9085 - val_loss: 9.6801\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.9074 - val_loss: 9.6667\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.9163 - val_loss: 9.6902\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.9128 - val_loss: 9.7031\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.9060 - val_loss: 9.6848\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.9158 - val_loss: 9.7008\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.9134 - val_loss: 9.6680\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.9051 - val_loss: 9.6679\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.8933 - val_loss: 9.6888\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.9083 - val_loss: 9.6882\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.8959 - val_loss: 9.6485\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.9054 - val_loss: 9.6800\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.8963 - val_loss: 9.7015\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.9129 - val_loss: 9.7126\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.9036 - val_loss: 9.6670\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.8943 - val_loss: 9.6783\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.9049 - val_loss: 9.7229\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.9083 - val_loss: 9.6799\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.8924 - val_loss: 9.6638\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.8988 - val_loss: 9.7141\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.8918 - val_loss: 9.6657\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.9000 - val_loss: 9.6708\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.9019 - val_loss: 9.6464\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.8943 - val_loss: 9.6810\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.8968 - val_loss: 9.6938\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.8930 - val_loss: 9.6300\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.9046 - val_loss: 9.6855\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.8850 - val_loss: 9.7160\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.8851 - val_loss: 9.6603\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.8776 - val_loss: 9.9010\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.9003 - val_loss: 9.6447\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.8870 - val_loss: 9.6512\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.8933 - val_loss: 9.7055\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.8898 - val_loss: 9.6466\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.8726 - val_loss: 9.7089\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.8722 - val_loss: 9.6302\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.8782 - val_loss: 9.6311\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.8774 - val_loss: 9.6395\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.8729 - val_loss: 9.6912\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.8713 - val_loss: 9.6821\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.8794 - val_loss: 9.6410\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.8588 - val_loss: 9.6026\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.8526 - val_loss: 9.6468\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.8568 - val_loss: 9.6393\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.8754 - val_loss: 9.6511\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 16s 66ms/step - loss: 9.8618 - val_loss: 9.6908\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.8532 - val_loss: 9.6497\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.8623 - val_loss: 9.6921\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.8604 - val_loss: 9.6277\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.8463 - val_loss: 9.5996\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.8514 - val_loss: 9.6454\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.8392 - val_loss: 9.6561\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.8524 - val_loss: 9.6058\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.8489 - val_loss: 9.7634\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 16s 66ms/step - loss: 9.8407 - val_loss: 9.6814\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 16s 66ms/step - loss: 9.8479 - val_loss: 9.6430\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.8415 - val_loss: 9.7040\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.8386 - val_loss: 9.6383\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.8285 - val_loss: 9.5902\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.8253 - val_loss: 9.5909\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.8231 - val_loss: 9.6484\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.8240 - val_loss: 9.5888\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.8180 - val_loss: 9.6830\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.8226 - val_loss: 9.5941\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.8213 - val_loss: 9.7759\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.8285 - val_loss: 9.6041\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.8130 - val_loss: 9.6391\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.8188 - val_loss: 9.6534\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.8171 - val_loss: 9.5579\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.8086 - val_loss: 10.9215\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.8140 - val_loss: 9.6167\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.8139 - val_loss: 9.5956\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 16s 66ms/step - loss: 9.8113 - val_loss: 9.6458\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.7964 - val_loss: 9.6484\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 16s 66ms/step - loss: 9.7942 - val_loss: 9.8125\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.7927 - val_loss: 9.7746\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.8079 - val_loss: 9.6482\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.7988 - val_loss: 9.6837\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.8553 - val_loss: 9.5846\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 16s 66ms/step - loss: 9.8101 - val_loss: 9.6014\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.8036 - val_loss: 9.6084\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 16s 66ms/step - loss: 9.7920 - val_loss: 9.5841\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.7824 - val_loss: 9.6214\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.7935 - val_loss: 9.6138\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 9.7944 - val_loss: 9.5815\n",
      "\n",
      "Loss: 958.15%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 111)       1110      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 94, 94, 111)      444       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 62)        62000     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 92, 92, 62)       248       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 8)         4472      \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 90, 90, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 136)       9928      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 78,238\n",
      "Trainable params: 77,874\n",
      "Non-trainable params: 364\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 10s 35ms/step - loss: 2736.4517 - val_loss: 2162.8706\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 1203.8257 - val_loss: 553.1824\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 219.0923 - val_loss: 74.7049\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 33.7908 - val_loss: 17.2476\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 12.7338 - val_loss: 10.8245\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 10.3565 - val_loss: 9.8758\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 10.0740 - val_loss: 9.8958\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 10.0062 - val_loss: 9.7192\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 10.0154 - val_loss: 9.7354\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 10.0165 - val_loss: 9.9093\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9896 - val_loss: 9.7536\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 10.0026 - val_loss: 9.8073\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9895 - val_loss: 9.7675\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9917 - val_loss: 9.7344\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9818 - val_loss: 9.7447\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9852 - val_loss: 9.7440\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9866 - val_loss: 9.7533\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9920 - val_loss: 9.7958\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9802 - val_loss: 9.8725\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9866 - val_loss: 9.7064\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9725 - val_loss: 9.7314\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9736 - val_loss: 9.7627\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9792 - val_loss: 9.6981\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9879 - val_loss: 9.7637\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9782 - val_loss: 9.7100\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9690 - val_loss: 9.7778\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9781 - val_loss: 9.7302\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9833 - val_loss: 9.7611\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9885 - val_loss: 9.7923\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9756 - val_loss: 9.6948\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9732 - val_loss: 9.7057\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9743 - val_loss: 9.7265\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9758 - val_loss: 9.8179\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9665 - val_loss: 9.7118\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9836 - val_loss: 9.7106\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9872 - val_loss: 9.7274\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9828 - val_loss: 9.7337\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9782 - val_loss: 9.7611\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9718 - val_loss: 9.7370\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9752 - val_loss: 9.7814\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9753 - val_loss: 9.7874\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9809 - val_loss: 9.7873\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9761 - val_loss: 9.7242\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9802 - val_loss: 9.7082\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9855 - val_loss: 9.7731\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9692 - val_loss: 9.7467\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9761 - val_loss: 9.7290\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9753 - val_loss: 9.8373\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9752 - val_loss: 9.7342\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9723 - val_loss: 9.7044\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9739 - val_loss: 9.7341\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9768 - val_loss: 9.7444\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9633 - val_loss: 9.7834\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9745 - val_loss: 9.7475\n",
      "Epoch 55/128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9640 - val_loss: 9.7996\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9656 - val_loss: 9.7418\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9675 - val_loss: 9.7699\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9520 - val_loss: 9.6990\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9677 - val_loss: 9.7634\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9720 - val_loss: 9.7617\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9703 - val_loss: 9.7112\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9694 - val_loss: 9.7969\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9706 - val_loss: 9.7455\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9689 - val_loss: 9.7768\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9701 - val_loss: 9.7435\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9677 - val_loss: 9.7499\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9673 - val_loss: 9.7162\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9575 - val_loss: 9.7392\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9489 - val_loss: 9.7019\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9769 - val_loss: 9.7296\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9643 - val_loss: 9.7402\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9616 - val_loss: 9.6968\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9636 - val_loss: 9.7061\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9721 - val_loss: 9.7798\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9611 - val_loss: 9.7053\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9544 - val_loss: 9.6850\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9617 - val_loss: 9.6711\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9651 - val_loss: 9.7991\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9561 - val_loss: 9.7318\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9552 - val_loss: 9.7442\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9513 - val_loss: 9.7596\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9606 - val_loss: 9.7714\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9533 - val_loss: 9.7229\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9588 - val_loss: 9.7925\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9537 - val_loss: 9.7501\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9524 - val_loss: 9.7647\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9594 - val_loss: 9.7358\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9686 - val_loss: 9.7398\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9677 - val_loss: 9.7570\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9520 - val_loss: 9.7537\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9530 - val_loss: 9.7394\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9503 - val_loss: 9.7553\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9606 - val_loss: 9.7504\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9581 - val_loss: 9.7338\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 8s 33ms/step - loss: 9.9675 - val_loss: 9.6956\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9538 - val_loss: 9.7291\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9732 - val_loss: 9.7694\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9600 - val_loss: 9.7372\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9552 - val_loss: 9.6948\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9566 - val_loss: 9.6939\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9536 - val_loss: 9.6943\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9523 - val_loss: 9.7017\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9452 - val_loss: 9.7269\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9480 - val_loss: 9.7266\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9570 - val_loss: 9.7167\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9534 - val_loss: 9.7508\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9447 - val_loss: 9.7192\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9574 - val_loss: 9.6991\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9509 - val_loss: 9.7688\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9513 - val_loss: 9.7152\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9557 - val_loss: 9.7649\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9439 - val_loss: 9.8311\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9515 - val_loss: 9.7118\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9468 - val_loss: 9.6890\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9496 - val_loss: 9.7489\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9542 - val_loss: 9.7637\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9464 - val_loss: 9.7266\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9521 - val_loss: 9.7182\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9496 - val_loss: 9.7474\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9449 - val_loss: 9.7524\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9596 - val_loss: 9.7309\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9505 - val_loss: 9.6916\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9521 - val_loss: 9.7185\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9539 - val_loss: 9.6973\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9467 - val_loss: 9.7192\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9549 - val_loss: 9.8304\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9657 - val_loss: 9.6879\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 9.9495 - val_loss: 9.7386\n",
      "\n",
      "Loss: 973.86%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " conv2d (Conv2D)             (None, 94, 94, 256)       2560      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 92, 92, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 136)       313480    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 90, 90, 136)      544       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 8)         9800      \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 88, 88, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 136)       9928      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 927,452\n",
      "Trainable params: 926,650\n",
      "Non-trainable params: 802\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 28s 105ms/step - loss: 2734.9099 - val_loss: 2214.9836\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 1204.4952 - val_loss: 579.5800\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 219.9922 - val_loss: 83.0657\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 33.9277 - val_loss: 19.0121\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 12.7664 - val_loss: 11.0037\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 10.3482 - val_loss: 10.0244\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 10.0174 - val_loss: 9.8421\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.9955 - val_loss: 9.7751\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.9932 - val_loss: 9.7888\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.9891 - val_loss: 9.8083\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.9642 - val_loss: 9.7516\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.9876 - val_loss: 9.7287\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.9838 - val_loss: 9.7919\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.9680 - val_loss: 9.8190\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.9682 - val_loss: 9.8901\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.9630 - val_loss: 9.7412\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.9534 - val_loss: 9.7462\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.9603 - val_loss: 9.7657\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.9581 - val_loss: 9.7219\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.9490 - val_loss: 9.7665\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.9573 - val_loss: 9.7587\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.9571 - val_loss: 9.7470\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.9613 - val_loss: 9.6909\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.9505 - val_loss: 9.8521\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.9459 - val_loss: 9.7441\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.9525 - val_loss: 9.8279\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.9418 - val_loss: 9.7188\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.9587 - val_loss: 9.7494\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.9526 - val_loss: 9.7951\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.9525 - val_loss: 9.7462\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.9499 - val_loss: 9.7507\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.9579 - val_loss: 9.6983\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.9476 - val_loss: 9.8068\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.9393 - val_loss: 9.7700\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.9479 - val_loss: 9.7078\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.9428 - val_loss: 9.7439\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.9421 - val_loss: 9.7010\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.9437 - val_loss: 9.7204\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.9469 - val_loss: 9.7111\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.9450 - val_loss: 9.7449\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.9432 - val_loss: 9.6780\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.9416 - val_loss: 9.7878\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.9443 - val_loss: 9.7294\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.9446 - val_loss: 9.6884\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.9387 - val_loss: 9.7277\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.9489 - val_loss: 9.6955\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.9480 - val_loss: 9.7110\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.9385 - val_loss: 9.8634\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.9399 - val_loss: 9.7184\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.9456 - val_loss: 9.8486\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.9394 - val_loss: 9.7605\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.9421 - val_loss: 9.7449\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.9411 - val_loss: 9.7340\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.9364 - val_loss: 9.6801\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.9363 - val_loss: 9.7083\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.9293 - val_loss: 9.7037\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.9384 - val_loss: 9.6907\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.9355 - val_loss: 9.7062\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.9376 - val_loss: 9.7315\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 25s 102ms/step - loss: 9.9438 - val_loss: 9.7132\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 25s 102ms/step - loss: 9.9403 - val_loss: 9.7404\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 25s 102ms/step - loss: 9.9316 - val_loss: 9.6918\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.9312 - val_loss: 9.6977\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.9325 - val_loss: 9.7099\n",
      "Epoch 65/128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 26s 102ms/step - loss: 9.9272 - val_loss: 9.6682\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.9316 - val_loss: 9.7014\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.9237 - val_loss: 9.6933\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.9188 - val_loss: 9.7320\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.9267 - val_loss: 9.7461\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.9297 - val_loss: 9.7988\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.9288 - val_loss: 9.6618\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.9295 - val_loss: 9.7227\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.9295 - val_loss: 9.7370\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.9205 - val_loss: 9.6821\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.9183 - val_loss: 9.6789\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.9227 - val_loss: 9.7014\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.9299 - val_loss: 9.6624\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.9238 - val_loss: 9.7018\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 25s 102ms/step - loss: 9.9146 - val_loss: 9.6651\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 25s 102ms/step - loss: 9.9150 - val_loss: 9.7885\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.9227 - val_loss: 9.6796\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.9104 - val_loss: 9.7012\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.9093 - val_loss: 9.7036\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 25s 102ms/step - loss: 9.9161 - val_loss: 9.7238\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 25s 102ms/step - loss: 9.9067 - val_loss: 9.6930\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 25s 102ms/step - loss: 9.9165 - val_loss: 9.7063\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 25s 102ms/step - loss: 9.9183 - val_loss: 9.6907\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 25s 102ms/step - loss: 9.9176 - val_loss: 9.6889\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 25s 102ms/step - loss: 9.9185 - val_loss: 9.6924\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 25s 102ms/step - loss: 9.9118 - val_loss: 9.6585\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 25s 102ms/step - loss: 9.9147 - val_loss: 9.7119\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 25s 102ms/step - loss: 9.9039 - val_loss: 9.7495\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.9196 - val_loss: 9.7172\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.9049 - val_loss: 9.7135\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 25s 102ms/step - loss: 9.9037 - val_loss: 9.6606\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 25s 102ms/step - loss: 9.9026 - val_loss: 9.6744\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 25s 102ms/step - loss: 9.9064 - val_loss: 9.8626\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 25s 102ms/step - loss: 9.9078 - val_loss: 9.6542\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 25s 102ms/step - loss: 9.9078 - val_loss: 9.6768\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 25s 102ms/step - loss: 9.9045 - val_loss: 9.6593\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 25s 102ms/step - loss: 9.8920 - val_loss: 9.6758\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.9011 - val_loss: 9.6816\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 25s 102ms/step - loss: 9.9009 - val_loss: 9.6739\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.8981 - val_loss: 9.6577\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.8935 - val_loss: 9.6795\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 25s 102ms/step - loss: 9.9005 - val_loss: 9.6937\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 25s 102ms/step - loss: 9.8959 - val_loss: 9.7379\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 25s 102ms/step - loss: 9.8983 - val_loss: 9.6877\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 25s 102ms/step - loss: 9.8973 - val_loss: 9.7064\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 25s 102ms/step - loss: 9.9084 - val_loss: 9.6977\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 25s 102ms/step - loss: 9.8924 - val_loss: 9.7468\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 25s 102ms/step - loss: 9.8930 - val_loss: 9.6914\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 25s 102ms/step - loss: 9.8833 - val_loss: 9.6817\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.8918 - val_loss: 9.6547\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.8882 - val_loss: 9.7008\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.8946 - val_loss: 9.6491\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 25s 102ms/step - loss: 9.8843 - val_loss: 9.6869\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 25s 102ms/step - loss: 9.8902 - val_loss: 9.7109\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 25s 102ms/step - loss: 9.8833 - val_loss: 9.6454\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 25s 102ms/step - loss: 9.8904 - val_loss: 9.6783\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 25s 102ms/step - loss: 9.8865 - val_loss: 9.6949\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 25s 102ms/step - loss: 9.8887 - val_loss: 9.6525\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 25s 102ms/step - loss: 9.8961 - val_loss: 9.6872\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 25s 102ms/step - loss: 9.8898 - val_loss: 9.6896\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 9.8846 - val_loss: 9.6905\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 25s 102ms/step - loss: 9.8878 - val_loss: 9.6924\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 25s 102ms/step - loss: 9.8814 - val_loss: 9.6610\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 25s 102ms/step - loss: 9.8886 - val_loss: 9.6915\n",
      "\n",
      "Loss: 969.15%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 8)         80        \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 8)         584       \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 92, 92, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 92, 92, 8)         0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 256)       18688     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 90, 90, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dropout_2 (Dropout)         (None, 90, 90, 256)       0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 8)         18440     \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 8)         584       \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 86, 86, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 86, 86, 8)         0         \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 84, 84, 256)       18688     \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 82, 82, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 82, 82, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 80, 80, 8)         18440     \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 80, 80, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 78, 78, 8)         584       \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 78, 78, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 76, 76, 136)       9928      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 678,276\n",
      "Trainable params: 677,186\n",
      "Non-trainable params: 1,090\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 26s 98ms/step - loss: 2735.6411 - val_loss: 2090.0986\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 1203.8887 - val_loss: 519.6715\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 219.9374 - val_loss: 68.4544\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 34.0126 - val_loss: 16.3967\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 12.7056 - val_loss: 10.5348\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.2635 - val_loss: 9.7987\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.9735 - val_loss: 9.7455\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.9241 - val_loss: 9.7304\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.9190 - val_loss: 9.7148\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.9118 - val_loss: 9.7159\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.9161 - val_loss: 9.6680\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.9040 - val_loss: 9.7107\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.8991 - val_loss: 9.7634\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.8949 - val_loss: 9.6694\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.8892 - val_loss: 9.7117\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.8885 - val_loss: 9.6836\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.8847 - val_loss: 9.6703\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.8834 - val_loss: 9.6353\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.8677 - val_loss: 9.6436\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.8679 - val_loss: 9.6777\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.8614 - val_loss: 9.6214\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.8488 - val_loss: 9.7012\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.8356 - val_loss: 9.6084\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.8222 - val_loss: 9.5757\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.8209 - val_loss: 9.5230\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.8004 - val_loss: 9.6238\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.7921 - val_loss: 9.5680\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.7789 - val_loss: 9.5403\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.7658 - val_loss: 9.5558\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.7499 - val_loss: 9.5098\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.7463 - val_loss: 9.4728\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.7272 - val_loss: 9.5633\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.7303 - val_loss: 9.6168\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.7058 - val_loss: 9.6010\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.7001 - val_loss: 9.4385\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.6944 - val_loss: 9.5047\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.6814 - val_loss: 9.5281\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.6745 - val_loss: 9.3938\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.6718 - val_loss: 9.6746\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.6633 - val_loss: 192.4051\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.7240 - val_loss: 9.4687\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.6598 - val_loss: 9.4724\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.6511 - val_loss: 9.4219\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.6277 - val_loss: 10.0061\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.6207 - val_loss: 9.5006\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.6267 - val_loss: 9.6888\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.6114 - val_loss: 9.6208\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.6364 - val_loss: 9.7245\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.6545 - val_loss: 11.6161\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.6399 - val_loss: 9.4381\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.6181 - val_loss: 9.3916\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.5894 - val_loss: 9.3805\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.5875 - val_loss: 9.5511\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.6938 - val_loss: 9.7046\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.6294 - val_loss: 9.3895\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.6161 - val_loss: 9.5261\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.5947 - val_loss: 9.7429\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.6365 - val_loss: 9.6636\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.6455 - val_loss: 9.5568\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.6478 - val_loss: 9.7796\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.6056 - val_loss: 10.8678\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.5963 - val_loss: 10.8958\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.5874 - val_loss: 17.1891\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.5833 - val_loss: 9.6901\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.5677 - val_loss: 10.7244\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.5621 - val_loss: 11.3321\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.5522 - val_loss: 10.2941\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.6058 - val_loss: 10.1067\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.5681 - val_loss: 11.6636\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.5432 - val_loss: 11.8955\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.5524 - val_loss: 9.6359\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.5402 - val_loss: 9.3757\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.5495 - val_loss: 9.4648\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.5426 - val_loss: 9.8950\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.5207 - val_loss: 9.9891\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.5412 - val_loss: 9.7130\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.5216 - val_loss: 9.6368\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.5329 - val_loss: 9.8243\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.5130 - val_loss: 9.5418\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.5303 - val_loss: 9.9763\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.5210 - val_loss: 10.6367\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.5305 - val_loss: 9.7287\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.5332 - val_loss: 11.9687\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.5245 - val_loss: 9.6242\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.5040 - val_loss: 10.0750\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.4974 - val_loss: 20.0786\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.5067 - val_loss: 47.8461\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.4899 - val_loss: 21.8531\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.4947 - val_loss: 39.1972\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.4964 - val_loss: 11.8248\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.5011 - val_loss: 22.0726\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.4912 - val_loss: 19.5085\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.4855 - val_loss: 23.9914\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.4919 - val_loss: 17.2014\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.4844 - val_loss: 18.8141\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.4798 - val_loss: 17.4483\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.4848 - val_loss: 20.8381\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.4808 - val_loss: 11.1531\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.4733 - val_loss: 13.1612\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.4776 - val_loss: 17.6730\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.4616 - val_loss: 13.9541\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.4726 - val_loss: 13.4216\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.4641 - val_loss: 12.4610\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.4564 - val_loss: 12.5555\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.4478 - val_loss: 12.2785\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.4559 - val_loss: 11.9357\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.4644 - val_loss: 9.6246\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.4468 - val_loss: 9.5265\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.4397 - val_loss: 9.3515\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.4357 - val_loss: 10.4299\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.4391 - val_loss: 10.1354\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.4456 - val_loss: 10.3931\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.4537 - val_loss: 9.8929\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.4281 - val_loss: 9.7385\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.4191 - val_loss: 9.5832\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.4314 - val_loss: 10.5666\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.4282 - val_loss: 10.4457\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.4266 - val_loss: 9.4154\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.3899 - val_loss: 10.3562\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.3660 - val_loss: 10.0526\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.3497 - val_loss: 10.3119\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.3299 - val_loss: 9.4242\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.3009 - val_loss: 9.2615\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.3440 - val_loss: 10.2296\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.2883 - val_loss: 9.6008\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.2900 - val_loss: 9.2432\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.2633 - val_loss: 9.3965\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.2479 - val_loss: 9.1713\n",
      "\n",
      "Loss: 917.13%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 256)       2560      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 256)       590080    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 92, 92, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 92, 92, 256)       0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 256)       590080    \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 88, 88, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 8)         18440     \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 86, 86, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 84, 84, 8)         584       \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 84, 84, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 82, 82, 136)       9928      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,803,868\n",
      "Trainable params: 1,802,810\n",
      "Non-trainable params: 1,058\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 41s 160ms/step - loss: 2735.5032 - val_loss: 2225.5325\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 1203.1090 - val_loss: 563.7737\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 219.3251 - val_loss: 72.9946\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 33.8244 - val_loss: 16.9704\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 12.6515 - val_loss: 10.5527\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 10.2517 - val_loss: 9.8182\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.9594 - val_loss: 9.7168\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.9264 - val_loss: 9.7872\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.9187 - val_loss: 9.7487\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.9228 - val_loss: 9.6917\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.9169 - val_loss: 9.6921\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.9170 - val_loss: 9.7099\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.9203 - val_loss: 9.7039\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.9097 - val_loss: 9.6833\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.9136 - val_loss: 9.6811\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.9087 - val_loss: 9.7691\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.9145 - val_loss: 9.7616\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.9167 - val_loss: 9.8232\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.9190 - val_loss: 9.6956\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.9208 - val_loss: 9.6996\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.9165 - val_loss: 9.7326\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.9132 - val_loss: 9.6678\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.9092 - val_loss: 9.7470\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.9070 - val_loss: 9.7420\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.9123 - val_loss: 9.7342\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.9109 - val_loss: 9.7256\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.9020 - val_loss: 9.6854\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.8960 - val_loss: 9.6869\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.9022 - val_loss: 9.6756\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.8970 - val_loss: 9.7306\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.8913 - val_loss: 9.7531\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.8843 - val_loss: 9.6747\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.8883 - val_loss: 9.6284\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.8747 - val_loss: 9.6888\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.8699 - val_loss: 9.6634\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.8612 - val_loss: 9.6162\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.8596 - val_loss: 9.5980\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.8483 - val_loss: 9.6161\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.8231 - val_loss: 9.5818\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.8242 - val_loss: 9.6802\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.8036 - val_loss: 9.6133\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.7931 - val_loss: 9.5151\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.7827 - val_loss: 9.6506\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.7651 - val_loss: 9.4953\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.7586 - val_loss: 9.4875\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.7338 - val_loss: 9.5301\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.7259 - val_loss: 9.4405\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.7092 - val_loss: 9.5054\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.7057 - val_loss: 9.4599\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.6782 - val_loss: 9.4823\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.6651 - val_loss: 9.4670\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.6462 - val_loss: 9.4699\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.6308 - val_loss: 9.4595\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.6097 - val_loss: 9.4439\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.6200 - val_loss: 9.3746\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.6060 - val_loss: 9.3417\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.5931 - val_loss: 9.3472\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.5764 - val_loss: 9.3992\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.5655 - val_loss: 9.4155\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.5579 - val_loss: 9.2948\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.5351 - val_loss: 9.2810\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.5271 - val_loss: 9.3427\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.5164 - val_loss: 9.3460\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.5056 - val_loss: 9.3050\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.4905 - val_loss: 9.3085\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.4687 - val_loss: 9.3597\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.4854 - val_loss: 9.3179\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.4801 - val_loss: 9.2266\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.4486 - val_loss: 9.3823\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.4407 - val_loss: 9.4399\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.4495 - val_loss: 9.4253\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.4410 - val_loss: 9.3421\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.4219 - val_loss: 9.2117\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.4005 - val_loss: 9.1825\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.3923 - val_loss: 9.2493\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.4035 - val_loss: 9.3013\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.3983 - val_loss: 9.1097\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.3867 - val_loss: 9.2342\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.3735 - val_loss: 9.2948\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.3690 - val_loss: 9.1659\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.3754 - val_loss: 9.2263\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.3482 - val_loss: 9.1918\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.3578 - val_loss: 9.1376\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.3612 - val_loss: 9.1492\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.3328 - val_loss: 9.2636\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.3486 - val_loss: 9.1477\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.3158 - val_loss: 9.1096\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.3305 - val_loss: 9.1158\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.3146 - val_loss: 9.1563\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.3067 - val_loss: 9.1368\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.3101 - val_loss: 9.2200\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.2935 - val_loss: 9.2311\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.2940 - val_loss: 9.1439\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.2742 - val_loss: 9.1419\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.2755 - val_loss: 9.1047\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.2915 - val_loss: 9.2022\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.2764 - val_loss: 9.1273\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.2563 - val_loss: 9.1460\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.2793 - val_loss: 9.1370\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.2491 - val_loss: 9.2068\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.2522 - val_loss: 9.1018\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.2440 - val_loss: 9.0947\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.2276 - val_loss: 9.1484\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.2330 - val_loss: 9.2199\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.2407 - val_loss: 9.1486\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.2366 - val_loss: 9.0683\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.1992 - val_loss: 9.1339\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.2044 - val_loss: 9.2638\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.1970 - val_loss: 9.0994\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.2120 - val_loss: 9.1439\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.2019 - val_loss: 9.1292\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.1989 - val_loss: 9.3351\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.1997 - val_loss: 9.0677\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.1782 - val_loss: 9.1140\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.1585 - val_loss: 9.1552\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.1649 - val_loss: 9.1665\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.1728 - val_loss: 9.1774\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.1683 - val_loss: 9.1215\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.1732 - val_loss: 9.2502\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.1670 - val_loss: 9.0375\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.1544 - val_loss: 9.1661\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.1532 - val_loss: 9.4295\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.1468 - val_loss: 9.1365\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.1365 - val_loss: 9.0906\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.1344 - val_loss: 9.0991\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.1276 - val_loss: 9.1367\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.1217 - val_loss: 9.1598\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.1354 - val_loss: 9.0818\n",
      "\n",
      "Loss: 908.18%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 151)       1510      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 151)       205360    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 92, 92, 151)      604       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 92, 92, 151)       0         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 256)       348160    \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 88, 88, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 88, 88, 256)       0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 77)        177485    \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 84, 84, 77)        53438     \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 84, 84, 77)       308       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 82, 82, 52)        36088     \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 82, 82, 52)       208       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 80, 80, 104)       48776     \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 80, 80, 104)      416       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 78, 78, 136)       127432    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,590,893\n",
      "Trainable params: 1,589,611\n",
      "Non-trainable params: 1,282\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 50s 191ms/step - loss: 1324.0126 - val_loss: 100.5274\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 12.4426 - val_loss: 18.2988\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.5156 - val_loss: 15.8415\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.4369 - val_loss: 10.7699\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.3098 - val_loss: 10.2341\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.2245 - val_loss: 10.0098\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.1444 - val_loss: 10.5118\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.1912 - val_loss: 10.6067\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.1460 - val_loss: 10.8909\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.1878 - val_loss: 10.8343\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.1708 - val_loss: 11.4405\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.1308 - val_loss: 9.8333\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.1510 - val_loss: 10.7335\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.1506 - val_loss: 10.9328\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.1317 - val_loss: 10.2035\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.1116 - val_loss: 9.9020\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.1107 - val_loss: 9.8901\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.1174 - val_loss: 10.0059\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.1050 - val_loss: 10.6814\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.1436 - val_loss: 10.0099\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.1068 - val_loss: 9.8709\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.1369 - val_loss: 9.9341\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.1725 - val_loss: 10.0880\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.0942 - val_loss: 10.1824\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.1151 - val_loss: 9.8478\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.1062 - val_loss: 10.0362\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.0833 - val_loss: 9.9631\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.1042 - val_loss: 9.9910\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.0790 - val_loss: 10.4826\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.0690 - val_loss: 9.8824\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.0347 - val_loss: 10.0848\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.0423 - val_loss: 10.0594\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.0272 - val_loss: 10.3575\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.0005 - val_loss: 9.8613\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.9790 - val_loss: 10.2123\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.9657 - val_loss: 9.8601\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.9560 - val_loss: 10.1584\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.9672 - val_loss: 9.7550\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.9638 - val_loss: 9.6059\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.9568 - val_loss: 9.8519\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.9235 - val_loss: 9.6357\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.8880 - val_loss: 9.8813\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.8924 - val_loss: 9.6069\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.8693 - val_loss: 9.7057\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.8688 - val_loss: 9.6628\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.8721 - val_loss: 9.8544\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.8421 - val_loss: 9.7914\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.8045 - val_loss: 9.8701\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.7963 - val_loss: 10.2721\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.8017 - val_loss: 9.5353\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.7814 - val_loss: 10.3423\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.7566 - val_loss: 9.8489\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.6915 - val_loss: 9.9322\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.6832 - val_loss: 9.5022\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.6971 - val_loss: 9.3731\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.6229 - val_loss: 9.4197\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.6125 - val_loss: 9.5563\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.5355 - val_loss: 9.6048\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.5745 - val_loss: 10.4343\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.4734 - val_loss: 9.2085\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.5115 - val_loss: 9.5952\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.4212 - val_loss: 9.4793\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.4141 - val_loss: 9.3009\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.3798 - val_loss: 9.1029\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.3894 - val_loss: 9.1841\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.3340 - val_loss: 10.5929\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.3178 - val_loss: 9.2160\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.2990 - val_loss: 9.3086\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.2695 - val_loss: 8.9388\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.2368 - val_loss: 9.5236\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.2299 - val_loss: 9.2124\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.1911 - val_loss: 8.9824\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.1608 - val_loss: 9.6131\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.1840 - val_loss: 9.0691\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.1806 - val_loss: 9.2175\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.1439 - val_loss: 8.8858\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.1167 - val_loss: 9.7350\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.1455 - val_loss: 10.1123\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.0814 - val_loss: 8.9063\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.0816 - val_loss: 9.2072\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.0578 - val_loss: 8.9359\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.0426 - val_loss: 9.2300\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.0464 - val_loss: 8.9060\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.0124 - val_loss: 9.3056\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.0374 - val_loss: 8.8715\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.0163 - val_loss: 9.1575\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 8.9835 - val_loss: 9.2533\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 8.9562 - val_loss: 8.9672\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 8.9377 - val_loss: 8.9926\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 8.9235 - val_loss: 8.8367\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 8.9354 - val_loss: 9.2264\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 8.8942 - val_loss: 8.7859\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 8.8865 - val_loss: 8.9258\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 8.8914 - val_loss: 8.7044\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 8.8979 - val_loss: 8.8140\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 8.8060 - val_loss: 9.7731\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 8.8227 - val_loss: 8.9129\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 8.8436 - val_loss: 8.9009\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 8.8112 - val_loss: 8.8460\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 8.7953 - val_loss: 8.8086\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 8.7914 - val_loss: 8.7885\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 8.7874 - val_loss: 8.8138\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 8.7661 - val_loss: 8.6425\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 8.7885 - val_loss: 8.6874\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 8.7889 - val_loss: 8.8713\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 8.7415 - val_loss: 8.7113\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 8.7797 - val_loss: 8.8229\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 8.7277 - val_loss: 8.7339\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 8.7085 - val_loss: 8.8501\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 8.7148 - val_loss: 8.7491\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 8.7057 - val_loss: 9.1464\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 8.7085 - val_loss: 9.0943\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 8.6679 - val_loss: 8.7974\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 8.6760 - val_loss: 9.1725\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 8.6527 - val_loss: 8.6516\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 8.6553 - val_loss: 8.8576\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 8.6438 - val_loss: 8.6170\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 8.6195 - val_loss: 8.6949\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 8.6268 - val_loss: 8.6049\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 8.6573 - val_loss: 8.8367\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 8.6289 - val_loss: 8.7821\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 8.6037 - val_loss: 8.9648\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 8.5959 - val_loss: 9.2111\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 8.5853 - val_loss: 8.8432\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 8.6284 - val_loss: 9.1446\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 8.5629 - val_loss: 8.8918\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 8.5734 - val_loss: 8.9358\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 8.5531 - val_loss: 8.5916\n",
      "\n",
      "Loss: 859.16%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 256)       2560      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 94, 94, 256)      1024      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 94, 94, 256)       0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 26)        59930     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 92, 92, 26)       104       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 136)       31960     \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 90, 90, 136)      544       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 73)        89425     \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 88, 88, 73)       292       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 136)       89488     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 275,331\n",
      "Trainable params: 274,347\n",
      "Non-trainable params: 984\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 23s 85ms/step - loss: 1527.9033 - val_loss: 162.1758\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 17.9705 - val_loss: 18.0324\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.1585 - val_loss: 10.3584\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.1088 - val_loss: 11.1392\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.1074 - val_loss: 10.1239\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0618 - val_loss: 10.1756\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0604 - val_loss: 9.8794\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0402 - val_loss: 9.9384\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0447 - val_loss: 10.4511\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0553 - val_loss: 11.0709\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0801 - val_loss: 10.1578\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0436 - val_loss: 9.9025\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0377 - val_loss: 9.8577\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0466 - val_loss: 10.2098\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0501 - val_loss: 10.0708\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0709 - val_loss: 9.7980\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0496 - val_loss: 10.2806\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0739 - val_loss: 9.9672\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0288 - val_loss: 9.9458\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0735 - val_loss: 10.5721\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0683 - val_loss: 9.8749\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0428 - val_loss: 10.0877\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0540 - val_loss: 9.7588\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0524 - val_loss: 10.0877\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0723 - val_loss: 9.7908\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0408 - val_loss: 9.9293\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0456 - val_loss: 9.9067\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0689 - val_loss: 10.0660\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0676 - val_loss: 10.0649\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0441 - val_loss: 10.6152\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0543 - val_loss: 9.8669\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0340 - val_loss: 9.8505\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0546 - val_loss: 10.0535\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0511 - val_loss: 9.8701\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0556 - val_loss: 9.9321\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0461 - val_loss: 9.8289\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0566 - val_loss: 9.9996\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0445 - val_loss: 10.1049\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0566 - val_loss: 9.7897\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0510 - val_loss: 10.0261\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0777 - val_loss: 9.7671\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0563 - val_loss: 9.9653\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0389 - val_loss: 10.0128\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0551 - val_loss: 9.7629\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0267 - val_loss: 9.8170\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0710 - val_loss: 10.0161\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0387 - val_loss: 9.7581\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0525 - val_loss: 9.7723\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0444 - val_loss: 10.1114\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0477 - val_loss: 9.9014\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0614 - val_loss: 9.8494\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0580 - val_loss: 9.8058\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0778 - val_loss: 9.9265\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0350 - val_loss: 9.7639\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0303 - val_loss: 9.8680\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0314 - val_loss: 9.8977\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0404 - val_loss: 9.8352\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0367 - val_loss: 10.0551\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0556 - val_loss: 9.8918\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0285 - val_loss: 9.8128\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0448 - val_loss: 9.8467\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0771 - val_loss: 9.9367\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0318 - val_loss: 9.8322\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0382 - val_loss: 10.0796\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0478 - val_loss: 9.8964\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0575 - val_loss: 9.7565\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0401 - val_loss: 10.2216\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0554 - val_loss: 9.8626\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0215 - val_loss: 10.3115\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0493 - val_loss: 9.7774\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0526 - val_loss: 9.8410\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0505 - val_loss: 9.9109\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0473 - val_loss: 9.8163\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0155 - val_loss: 9.6992\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0191 - val_loss: 9.9611\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0236 - val_loss: 9.7267\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0389 - val_loss: 9.7613\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0438 - val_loss: 9.8462\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0191 - val_loss: 9.9001\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0386 - val_loss: 9.8049\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0381 - val_loss: 9.9001\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0098 - val_loss: 9.8604\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0170 - val_loss: 9.8922\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0499 - val_loss: 9.7745\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0072 - val_loss: 9.7257\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0101 - val_loss: 9.8216\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0112 - val_loss: 9.7841\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0240 - val_loss: 9.9003\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0418 - val_loss: 9.8150\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0259 - val_loss: 9.8124\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0300 - val_loss: 9.8870\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0079 - val_loss: 9.7614\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0277 - val_loss: 9.7721\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0310 - val_loss: 9.8793\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0206 - val_loss: 9.7722\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0160 - val_loss: 9.7979\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0190 - val_loss: 9.8958\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0144 - val_loss: 9.7724\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0146 - val_loss: 9.8007\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0276 - val_loss: 9.7662\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 9.9958 - val_loss: 9.7554\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0097 - val_loss: 9.8365\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0045 - val_loss: 9.8204\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0121 - val_loss: 9.8251\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0009 - val_loss: 9.7212\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0059 - val_loss: 9.7960\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 9.9995 - val_loss: 9.8406\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0062 - val_loss: 9.7665\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0132 - val_loss: 9.7992\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 9.9951 - val_loss: 9.7799\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0117 - val_loss: 9.8495\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0225 - val_loss: 9.8825\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 9.9840 - val_loss: 9.9955\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 9.9931 - val_loss: 9.7725\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 9.9811 - val_loss: 9.7168\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0003 - val_loss: 9.7738\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 9.9955 - val_loss: 9.7091\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 9.9724 - val_loss: 9.8252\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 9.9927 - val_loss: 9.7626\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 9.9728 - val_loss: 9.7143\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 9.9750 - val_loss: 9.8127\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 9.9656 - val_loss: 9.9678\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 9.9748 - val_loss: 9.7825\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 9.9801 - val_loss: 9.8468\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 9.9791 - val_loss: 9.8737\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 9.9921 - val_loss: 9.8115\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 10.0017 - val_loss: 9.6828\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 20s 81ms/step - loss: 9.9794 - val_loss: 9.9198\n",
      "\n",
      "Loss: 991.98%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 256)       2560      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 92, 92, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 92, 92, 256)       0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 72)        165960    \n",
      "                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " conv2d_3 (Conv2D)           (None, 88, 88, 72)        46728     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 88, 88, 72)       288       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 88, 88, 72)        0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 94)        61006     \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 84, 84, 94)        79618     \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 84, 84, 94)       376       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 84, 84, 94)        0         \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 82, 82, 256)       216832    \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 80, 80, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 80, 80, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 78, 78, 8)         18440     \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 78, 78, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 76, 76, 75)        5475      \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 76, 76, 75)       300       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 74, 74, 136)       91936     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,871,763\n",
      "Trainable params: 1,870,239\n",
      "Non-trainable params: 1,524\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 54s 204ms/step - loss: 1510.9924 - val_loss: 165.0629\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 17.1283 - val_loss: 14.2521\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 10.1483 - val_loss: 11.8299\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 10.0570 - val_loss: 12.1736\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 10.0578 - val_loss: 12.3495\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 10.0449 - val_loss: 11.3566\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 10.0386 - val_loss: 12.1914\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 10.0314 - val_loss: 10.2290\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 10.0076 - val_loss: 11.0111\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 10.0228 - val_loss: 10.6123\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 10.0216 - val_loss: 11.0698\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 10.0199 - val_loss: 11.6085\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 10.0299 - val_loss: 10.3794\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 49s 198ms/step - loss: 10.0202 - val_loss: 10.7445\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 10.0257 - val_loss: 10.6239\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 10.0055 - val_loss: 10.4161\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 10.0133 - val_loss: 10.3446\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 9.9243 - val_loss: 9.7495\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 9.9243 - val_loss: 9.7881\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 9.8951 - val_loss: 10.3158\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 9.8662 - val_loss: 10.2094\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 9.8522 - val_loss: 9.8055\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 9.7985 - val_loss: 10.9515\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 9.7667 - val_loss: 9.6063\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 9.7541 - val_loss: 9.5510\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 9.7198 - val_loss: 9.3924\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 9.7002 - val_loss: 11.0838\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 9.6757 - val_loss: 10.6490\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 9.6475 - val_loss: 9.4971\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 9.6464 - val_loss: 9.9949\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 9.5850 - val_loss: 10.4629\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 9.5658 - val_loss: 9.6896\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 9.5508 - val_loss: 10.4067\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 9.5173 - val_loss: 9.8326\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 9.4798 - val_loss: 10.8606\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 9.4640 - val_loss: 9.7627\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 9.4584 - val_loss: 9.4344\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 9.4442 - val_loss: 9.2971\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 9.4319 - val_loss: 9.8829\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 9.3945 - val_loss: 10.3078\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 9.3544 - val_loss: 9.9490\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 9.2601 - val_loss: 9.9455\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 9.1903 - val_loss: 10.9086\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 9.1346 - val_loss: 9.7694\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 9.0793 - val_loss: 9.1087\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 9.0218 - val_loss: 9.6339\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 8.9629 - val_loss: 10.1601\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 8.9473 - val_loss: 9.0936\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 8.8902 - val_loss: 9.5286\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 8.8217 - val_loss: 9.0034\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 8.7978 - val_loss: 10.1580\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 8.7780 - val_loss: 8.7427\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 8.7790 - val_loss: 10.1802\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 8.6996 - val_loss: 9.2362\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 8.7048 - val_loss: 9.3218\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 8.6553 - val_loss: 9.4591\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 8.6803 - val_loss: 9.5522\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 8.6271 - val_loss: 8.6182\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 8.6015 - val_loss: 8.5209\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 8.5639 - val_loss: 8.7592\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 8.5595 - val_loss: 8.3318\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 8.5928 - val_loss: 9.5962\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 8.5162 - val_loss: 9.6116\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 8.5109 - val_loss: 9.3006\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 8.5327 - val_loss: 8.5484\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 8.4777 - val_loss: 9.1124\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 8.4419 - val_loss: 8.7014\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 8.4915 - val_loss: 8.3566\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 8.4436 - val_loss: 8.4534\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 8.5160 - val_loss: 8.7938\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 8.4054 - val_loss: 8.5326\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 8.4304 - val_loss: 8.5372\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 8.4130 - val_loss: 8.5102\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 8.3901 - val_loss: 8.8230\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 8.3791 - val_loss: 9.1338\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 8.3486 - val_loss: 8.6066\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 8.3410 - val_loss: 8.1173\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 8.3468 - val_loss: 8.9849\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 8.3101 - val_loss: 8.4420\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 8.3452 - val_loss: 8.2876\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 8.2905 - val_loss: 9.6826\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 8.2970 - val_loss: 10.0502\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 8.2790 - val_loss: 8.6516\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 8.2635 - val_loss: 9.4141\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 8.2460 - val_loss: 8.5874\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 8.2639 - val_loss: 9.2912\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 8.2215 - val_loss: 8.8561\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 8.2307 - val_loss: 8.3786\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 8.2322 - val_loss: 9.0785\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 8.2524 - val_loss: 8.5011\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 8.2041 - val_loss: 9.2887\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 8.1926 - val_loss: 8.5939\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 8.2040 - val_loss: 8.8258\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 8.2357 - val_loss: 8.1516\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 8.1797 - val_loss: 9.6135\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 49s 198ms/step - loss: 8.2359 - val_loss: 8.7908\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 8.1627 - val_loss: 8.2175\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 8.1851 - val_loss: 8.8222\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 8.1060 - val_loss: 8.3455\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 8.0994 - val_loss: 9.1219\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 8.1157 - val_loss: 8.5220\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 8.1620 - val_loss: 8.9607\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 8.0999 - val_loss: 8.5235\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 8.0909 - val_loss: 8.1506\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 8.0847 - val_loss: 8.4667\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 8.0959 - val_loss: 11.1269\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 49s 198ms/step - loss: 8.0910 - val_loss: 8.2799\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 49s 198ms/step - loss: 8.0631 - val_loss: 8.3717\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 49s 198ms/step - loss: 8.0841 - val_loss: 10.4621\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 49s 198ms/step - loss: 8.0522 - val_loss: 9.0417\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 8.0647 - val_loss: 8.3733\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 8.0384 - val_loss: 8.8006\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 8.1293 - val_loss: 9.0582\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 49s 198ms/step - loss: 8.0393 - val_loss: 8.2822\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 49s 198ms/step - loss: 8.0657 - val_loss: 9.2318\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 8.0523 - val_loss: 8.4975\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 8.0199 - val_loss: 8.3245\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 8.0625 - val_loss: 9.0099\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 49s 198ms/step - loss: 7.9774 - val_loss: 8.5671\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 7.9736 - val_loss: 8.2136\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 49s 198ms/step - loss: 7.9489 - val_loss: 9.0712\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 49s 198ms/step - loss: 7.9956 - val_loss: 8.2287\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 7.9673 - val_loss: 8.1912\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 7.9406 - val_loss: 8.1949\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 49s 198ms/step - loss: 7.9532 - val_loss: 8.3752\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 49s 198ms/step - loss: 7.9469 - val_loss: 8.0879\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 50s 198ms/step - loss: 7.8936 - val_loss: 8.9654\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 49s 198ms/step - loss: 7.9573 - val_loss: 8.9251\n",
      "\n",
      "Loss: 892.51%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 256)       2560      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 94, 94, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 94, 94, 256)       0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 8)         18440     \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 8)         584       \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 90, 90, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 90, 90, 8)         0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 256)       18688     \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 88, 88, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 88, 88, 256)       0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 158)       364190    \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 84, 84, 158)       224834    \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 84, 84, 158)      632       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 84, 84, 158)       0         \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 82, 82, 256)       364288    \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 82, 82, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 80, 80, 8)         18440     \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 80, 80, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 78, 78, 136)       9928      \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 78, 78, 136)      544       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 76, 76, 136)       166600    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,192,868\n",
      "Trainable params: 1,190,710\n",
      "Non-trainable params: 2,158\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 47s 180ms/step - loss: 1191.8102 - val_loss: 186.6657\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 11.0000 - val_loss: 79.4233\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.2724 - val_loss: 17.5317\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.1916 - val_loss: 19.6571\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.1823 - val_loss: 29.3592\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.1290 - val_loss: 14.7924\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.1827 - val_loss: 18.0809\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.1713 - val_loss: 15.3852\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.1685 - val_loss: 14.5902\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.1087 - val_loss: 17.9127\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.1289 - val_loss: 14.3460\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.1278 - val_loss: 12.4400\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.1234 - val_loss: 11.0563\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.1373 - val_loss: 12.7507\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.1217 - val_loss: 12.1924\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.1367 - val_loss: 21.7194\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.2173 - val_loss: 40.7441\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.1600 - val_loss: 11.3873\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.1269 - val_loss: 9.9012\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.1423 - val_loss: 11.8938\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.1034 - val_loss: 13.6543\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.1363 - val_loss: 10.6789\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.1143 - val_loss: 11.0507\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.1147 - val_loss: 12.2600\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.1233 - val_loss: 11.7336\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.1178 - val_loss: 10.9905\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.1491 - val_loss: 10.3112\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.1410 - val_loss: 11.7334\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.1253 - val_loss: 10.6443\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.1483 - val_loss: 10.3853\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.1428 - val_loss: 11.8931\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.0921 - val_loss: 10.1212\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.1421 - val_loss: 11.1777\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.1165 - val_loss: 10.9143\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.0958 - val_loss: 10.8488\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.1140 - val_loss: 10.5988\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.0932 - val_loss: 10.9544\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.1235 - val_loss: 10.7252\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.1189 - val_loss: 10.1370\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.2062 - val_loss: 14.9096\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.1290 - val_loss: 13.7706\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.1134 - val_loss: 10.6092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.1177 - val_loss: 10.2633\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.0733 - val_loss: 10.6464\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.0844 - val_loss: 10.8560\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.1139 - val_loss: 9.9303\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.0921 - val_loss: 10.0737\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.1123 - val_loss: 10.0459\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.0794 - val_loss: 10.0012\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.1046 - val_loss: 9.9845\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.0586 - val_loss: 71.4039\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.1288 - val_loss: 9.9542\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.1264 - val_loss: 9.8813\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.0488 - val_loss: 9.8342\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.0560 - val_loss: 10.0970\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.0832 - val_loss: 10.1279\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.1067 - val_loss: 10.6158\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.0548 - val_loss: 9.8301\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.0749 - val_loss: 10.0864\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.0906 - val_loss: 10.3597\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.0685 - val_loss: 10.1422\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.0660 - val_loss: 10.3512\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.0601 - val_loss: 10.1301\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.0225 - val_loss: 9.7274\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.0605 - val_loss: 9.8348\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.0659 - val_loss: 9.8127\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.0630 - val_loss: 10.2294\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.0367 - val_loss: 10.5020\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.0198 - val_loss: 9.8866\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.0601 - val_loss: 12.8127\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.0834 - val_loss: 10.6312\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.0317 - val_loss: 10.5534\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.0330 - val_loss: 11.4685\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.0174 - val_loss: 10.0932\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.0347 - val_loss: 10.1514\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.0258 - val_loss: 10.7655\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 9.9993 - val_loss: 9.9120\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.0035 - val_loss: 10.2100\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.0165 - val_loss: 11.7423\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.0314 - val_loss: 10.8277\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 9.9873 - val_loss: 9.8112\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 9.9838 - val_loss: 9.9019\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.0193 - val_loss: 13.5426\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 9.9857 - val_loss: 9.7508\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 9.9957 - val_loss: 12.9442\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 9.9914 - val_loss: 10.2776\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 9.9586 - val_loss: 10.1970\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 9.9656 - val_loss: 544.1764\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 10.0088 - val_loss: 11.0252\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 9.9978 - val_loss: 9.7291\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 9.9354 - val_loss: 10.1366\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 9.9724 - val_loss: 10.2067\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 9.9595 - val_loss: 9.7347\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 9.9545 - val_loss: 9.6525\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 9.9732 - val_loss: 10.0771\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 9.9541 - val_loss: 9.8384\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 9.9637 - val_loss: 10.0359\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 9.9801 - val_loss: 9.7127\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 9.9369 - val_loss: 10.0865\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 9.9549 - val_loss: 9.7665\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 9.9403 - val_loss: 9.7347\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 9.9606 - val_loss: 9.8246\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 9.9428 - val_loss: 9.9049\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 9.9435 - val_loss: 10.0559\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 9.9275 - val_loss: 10.0083\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 9.9130 - val_loss: 9.9583\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 9.9304 - val_loss: 10.5740\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 9.9306 - val_loss: 10.0686\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 9.9180 - val_loss: 10.0472\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 9.9432 - val_loss: 9.8592\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 9.9390 - val_loss: 9.8261\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 9.8807 - val_loss: 9.9649\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 9.9415 - val_loss: 10.5998\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 9.8964 - val_loss: 10.3313\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 9.8975 - val_loss: 10.0428\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 9.8946 - val_loss: 9.8470\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 9.9015 - val_loss: 9.6690\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 9.9295 - val_loss: 9.7219\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 9.8776 - val_loss: 10.3429\n",
      "Epoch 120/128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 44s 176ms/step - loss: 9.8821 - val_loss: 11.1082\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 9.8988 - val_loss: 9.8107\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 9.8916 - val_loss: 10.0433\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 9.8653 - val_loss: 9.8423\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 9.8625 - val_loss: 9.6791\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 9.8733 - val_loss: 10.1536\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 9.8701 - val_loss: 9.8672\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 9.8519 - val_loss: 9.6525\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 9.8496 - val_loss: 9.7409\n",
      "\n",
      "Loss: 974.09%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 8)         80        \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 8)         584       \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 92, 92, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 92, 92, 8)         0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 256)       18688     \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 88, 88, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 88, 88, 256)       0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 256)       590080    \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 84, 84, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 84, 84, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 84, 84, 256)       0         \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 82, 82, 245)       564725    \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 80, 80, 245)       540470    \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 80, 80, 245)      980       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 78, 78, 136)       300016    \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 78, 78, 136)      544       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 76, 76, 134)       164150    \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 76, 76, 134)      536       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 74, 74, 136)       164152    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,527,249\n",
      "Trainable params: 3,525,177\n",
      "Non-trainable params: 2,072\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 72s 276ms/step - loss: 1205.4093 - val_loss: 28.4370\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 10.6424 - val_loss: 10.0731\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 10.2041 - val_loss: 10.5826\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 10.0976 - val_loss: 10.0120\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 10.1723 - val_loss: 11.4571\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 10.0977 - val_loss: 9.9555\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 10.0648 - val_loss: 10.1276\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 10.0889 - val_loss: 9.8876\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 10.0769 - val_loss: 9.9925\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 10.0795 - val_loss: 9.8175\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 10.0886 - val_loss: 10.2814\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 10.0793 - val_loss: 10.3076\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 10.1017 - val_loss: 10.0702\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 10.0891 - val_loss: 10.3323\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 10.0792 - val_loss: 10.6755\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 10.0832 - val_loss: 10.0761\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 10.1213 - val_loss: 9.9252\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 10.0889 - val_loss: 9.8550\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 10.0767 - val_loss: 10.6001\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 10.0791 - val_loss: 10.4077\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 10.0553 - val_loss: 9.9907\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 10.0659 - val_loss: 10.0383\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 10.0405 - val_loss: 9.9654\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 10.0285 - val_loss: 10.0958\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 10.0314 - val_loss: 10.4912\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 9.9890 - val_loss: 10.8425\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 9.9617 - val_loss: 10.0936\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 9.9279 - val_loss: 10.3983\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 9.9082 - val_loss: 10.7117\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 9.8824 - val_loss: 9.7194\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 9.8650 - val_loss: 9.8957\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 9.8697 - val_loss: 11.8287\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 9.7828 - val_loss: 9.7468\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 9.7276 - val_loss: 11.3795\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 9.6890 - val_loss: 9.8183\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 9.6317 - val_loss: 9.3157\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 9.5489 - val_loss: 9.0888\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 9.4465 - val_loss: 10.1231\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 9.4094 - val_loss: 9.7955\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 9.3464 - val_loss: 9.3510\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 9.2328 - val_loss: 9.2457\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 9.2601 - val_loss: 9.2259\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 9.1698 - val_loss: 9.3161\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 9.0981 - val_loss: 8.8759\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 9.0880 - val_loss: 9.5326\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 9.0409 - val_loss: 8.9195\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 8.9582 - val_loss: 9.2332\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 8.9587 - val_loss: 8.9051\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 8.9084 - val_loss: 9.2896\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 8.7953 - val_loss: 8.5129\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 8.7920 - val_loss: 9.3856\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 8.7183 - val_loss: 8.6245\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 8.7445 - val_loss: 8.5181\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 8.7122 - val_loss: 8.4523\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 8.6795 - val_loss: 8.4288\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 8.6259 - val_loss: 9.6147\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 8.6239 - val_loss: 8.4641\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 8.5344 - val_loss: 9.0349\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 8.5902 - val_loss: 8.8751\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 8.5143 - val_loss: 8.6024\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 8.5280 - val_loss: 8.3282\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 8.4832 - val_loss: 8.3923\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 8.4216 - val_loss: 8.6501\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 8.4342 - val_loss: 8.9789\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 8.3780 - val_loss: 8.9014\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 8.3403 - val_loss: 8.2659\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 8.3130 - val_loss: 8.2313\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 8.2857 - val_loss: 8.2763\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 8.2604 - val_loss: 8.5796\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 8.2549 - val_loss: 8.2425\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 8.2534 - val_loss: 8.2280\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 8.2050 - val_loss: 8.3174\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 8.2122 - val_loss: 8.3513\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 8.1196 - val_loss: 8.1089\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 8.1520 - val_loss: 8.1879\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 8.1416 - val_loss: 8.0994\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 8.1150 - val_loss: 8.1432\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 8.1300 - val_loss: 8.0908\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 8.0883 - val_loss: 8.3340\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 8.0340 - val_loss: 8.0793\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 8.0086 - val_loss: 8.8657\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 7.9807 - val_loss: 8.2448\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 7.9718 - val_loss: 8.2250\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 8.0021 - val_loss: 8.2407\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 8.8478 - val_loss: 10.4340\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 8.9935 - val_loss: 8.3840\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 8.4668 - val_loss: 8.2191\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 8.2230 - val_loss: 8.2283\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 8.1575 - val_loss: 8.1751\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 8.0344 - val_loss: 8.1559\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 8.0026 - val_loss: 7.9785\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 7.9463 - val_loss: 304109952.0000\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 7.9368 - val_loss: 108306576.0000\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 7.9053 - val_loss: 8.0569\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 7.9349 - val_loss: 8.0687\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 7.8522 - val_loss: 8.3764\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 7.8363 - val_loss: 8.4938\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 7.8159 - val_loss: 8.2108\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 7.8361 - val_loss: 8.7649\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 7.8382 - val_loss: 743.3509\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 7.8016 - val_loss: 130.6681\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 7.7845 - val_loss: 115929.6406\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 7.7369 - val_loss: 596268.7500\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 8.2231 - val_loss: 8.8828\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 7.8340 - val_loss: 8.1254\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 7.7966 - val_loss: 8.3084\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 7.7283 - val_loss: 8.2166\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 7.7319 - val_loss: 7.8894\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 7.7301 - val_loss: 7.9154\n",
      "Epoch 110/128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 67s 269ms/step - loss: 7.7048 - val_loss: 7.9281\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 7.6881 - val_loss: 8.1798\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 7.6251 - val_loss: 8.0625\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 7.6960 - val_loss: 7.9377\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 7.6401 - val_loss: 8.2146\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 7.6421 - val_loss: 7.9785\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 7.6177 - val_loss: 8.2729\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 7.6030 - val_loss: 7.8919\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 7.5815 - val_loss: 8.0164\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 7.6423 - val_loss: 8.2387\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 7.6022 - val_loss: 8.6032\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 7.6082 - val_loss: 8.0175\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 7.5534 - val_loss: 8.0612\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 7.5582 - val_loss: 8.0319\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 7.5188 - val_loss: 8.0287\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 7.5275 - val_loss: 8.2503\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 7.5350 - val_loss: 7.9283\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 7.5416 - val_loss: 7.8762\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 7.4826 - val_loss: 7.9849\n",
      "\n",
      "Loss: 798.49%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 169)       1690      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 94, 94, 169)      676       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 94, 94, 169)       0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 8)         12176     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 92, 92, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 92, 92, 8)         0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 63)        4599      \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 90, 90, 63)       252       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 8)         4544      \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 88, 88, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 75)        5475      \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 86, 86, 75)       300       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 84, 84, 136)       91936     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 121,716\n",
      "Trainable params: 121,068\n",
      "Non-trainable params: 648\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 17s 60ms/step - loss: 1496.4154 - val_loss: 172.0861\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 17.6111 - val_loss: 23.7877\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.3703 - val_loss: 15.9777\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.2564 - val_loss: 12.3927\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.2173 - val_loss: 11.4642\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.2079 - val_loss: 10.7711\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.1869 - val_loss: 10.9037\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.1603 - val_loss: 10.2295\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.1480 - val_loss: 10.4455\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.1374 - val_loss: 10.3174\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.1689 - val_loss: 10.2712\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.1075 - val_loss: 10.1484\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.1235 - val_loss: 10.5299\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.1410 - val_loss: 10.0417\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.1159 - val_loss: 10.0242\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.1141 - val_loss: 11.0071\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.1070 - val_loss: 10.3390\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.1013 - val_loss: 10.1289\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0878 - val_loss: 9.9708\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0995 - val_loss: 10.2181\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.1029 - val_loss: 10.3188\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.1239 - val_loss: 10.7191\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.1295 - val_loss: 10.1769\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0738 - val_loss: 9.9351\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0910 - val_loss: 10.0001\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.1025 - val_loss: 9.8516\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0827 - val_loss: 10.1375\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0653 - val_loss: 10.0624\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0810 - val_loss: 10.1080\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0827 - val_loss: 9.9952\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0685 - val_loss: 9.8951\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0871 - val_loss: 10.4589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0830 - val_loss: 10.2976\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0706 - val_loss: 9.8765\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0958 - val_loss: 9.8150\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0602 - val_loss: 10.0970\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0864 - val_loss: 9.8761\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0928 - val_loss: 10.2509\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.1060 - val_loss: 10.2064\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0382 - val_loss: 9.8883\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0640 - val_loss: 9.7678\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0458 - val_loss: 9.9707\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0693 - val_loss: 9.8534\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0564 - val_loss: 9.8608\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0618 - val_loss: 9.8375\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0644 - val_loss: 9.9243\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0719 - val_loss: 10.0051\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0545 - val_loss: 9.8890\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0476 - val_loss: 9.8184\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0457 - val_loss: 10.4462\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0243 - val_loss: 10.0518\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0532 - val_loss: 9.7766\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0423 - val_loss: 9.7997\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0566 - val_loss: 9.7353\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0507 - val_loss: 10.2952\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0744 - val_loss: 10.1331\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0493 - val_loss: 9.7657\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0779 - val_loss: 9.9344\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0402 - val_loss: 9.8481\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0236 - val_loss: 10.0506\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0543 - val_loss: 10.1065\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0682 - val_loss: 9.9178\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0561 - val_loss: 9.9333\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0365 - val_loss: 9.7827\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0520 - val_loss: 9.8573\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0411 - val_loss: 9.7910\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0422 - val_loss: 9.8715\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0358 - val_loss: 9.7794\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0625 - val_loss: 9.9334\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0597 - val_loss: 9.9881\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0503 - val_loss: 9.7947\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0773 - val_loss: 9.8448\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0410 - val_loss: 9.8879\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0365 - val_loss: 10.1826\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0556 - val_loss: 9.7550\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0265 - val_loss: 9.8860\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0444 - val_loss: 9.9825\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0553 - val_loss: 9.8116\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0353 - val_loss: 9.8398\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0483 - val_loss: 9.9142\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0235 - val_loss: 9.7363\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0311 - val_loss: 9.8975\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0481 - val_loss: 9.9524\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0371 - val_loss: 9.8559\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0292 - val_loss: 9.7758\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0476 - val_loss: 9.9408\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0318 - val_loss: 9.7771\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0421 - val_loss: 9.7554\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0256 - val_loss: 9.7939\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0452 - val_loss: 9.8716\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0073 - val_loss: 10.0079\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0444 - val_loss: 9.9269\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0439 - val_loss: 9.8111\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0069 - val_loss: 9.8106\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0244 - val_loss: 9.8338\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0386 - val_loss: 9.8182\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0300 - val_loss: 9.9235\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0303 - val_loss: 10.1924\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0270 - val_loss: 9.9247\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0476 - val_loss: 9.7666\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0279 - val_loss: 9.9200\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0170 - val_loss: 9.7790\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0338 - val_loss: 9.7816\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0284 - val_loss: 9.8627\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.9975 - val_loss: 9.7979\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0403 - val_loss: 9.7804\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0308 - val_loss: 9.9177\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0302 - val_loss: 9.7762\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0528 - val_loss: 9.7929\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0353 - val_loss: 9.7963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0365 - val_loss: 9.9737\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0407 - val_loss: 9.8999\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0345 - val_loss: 9.7675\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0207 - val_loss: 9.8921\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0120 - val_loss: 9.7222\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0304 - val_loss: 9.7963\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0272 - val_loss: 9.8426\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0201 - val_loss: 9.8614\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0066 - val_loss: 9.8632\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 9.9994 - val_loss: 9.8293\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0197 - val_loss: 9.9269\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0346 - val_loss: 9.8328\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0362 - val_loss: 9.8339\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0216 - val_loss: 9.9785\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0121 - val_loss: 9.8893\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0361 - val_loss: 9.7880\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0575 - val_loss: 9.7620\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 10.0113 - val_loss: 9.7253\n",
      "\n",
      "Loss: 972.53%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 8)         80        \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 94, 94, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 94, 94, 8)         0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 8)         584       \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 8)         584       \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 90, 90, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 90, 90, 8)         0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 256)       18688     \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 86, 86, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 86, 86, 256)       0         \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 84, 84, 8)         18440     \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 82, 82, 8)         584       \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 82, 82, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 80, 80, 58)        4234      \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 80, 80, 58)       232       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 78, 78, 8)         4184      \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 78, 78, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 76, 76, 136)       9928      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 648,774\n",
      "Trainable params: 648,080\n",
      "Non-trainable params: 694\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 22s 85ms/step - loss: 2734.4910 - val_loss: 2229.3577\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 1203.9619 - val_loss: 514.2822\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 220.5852 - val_loss: 69.5799\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 34.2269 - val_loss: 16.3117\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 12.7268 - val_loss: 10.4581\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 10.2644 - val_loss: 9.7985\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.9775 - val_loss: 9.7168\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.9333 - val_loss: 9.7095\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.9238 - val_loss: 9.7373\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.9217 - val_loss: 9.7278\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.9251 - val_loss: 9.7176\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.9213 - val_loss: 9.7375\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.9174 - val_loss: 9.8377\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.9140 - val_loss: 9.7052\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.9131 - val_loss: 9.6707\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.9145 - val_loss: 9.6875\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.9112 - val_loss: 9.6749\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.9175 - val_loss: 10.1500\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.9094 - val_loss: 9.7508\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.8998 - val_loss: 9.7994\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.8937 - val_loss: 9.7204\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.8845 - val_loss: 9.7946\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.8835 - val_loss: 9.8851\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.8855 - val_loss: 9.8595\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.8660 - val_loss: 9.6868\n",
      "Epoch 26/128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 21s 83ms/step - loss: 9.8678 - val_loss: 9.7709\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.8636 - val_loss: 9.6702\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.8565 - val_loss: 9.6235\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.8452 - val_loss: 9.5865\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.8392 - val_loss: 9.5996\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.8329 - val_loss: 9.6302\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.8090 - val_loss: 9.7001\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.8181 - val_loss: 9.5860\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.7937 - val_loss: 9.7662\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.7796 - val_loss: 9.6130\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.7777 - val_loss: 9.7891\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.7545 - val_loss: 9.7116\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.7650 - val_loss: 9.5838\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.7272 - val_loss: 9.5106\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.7334 - val_loss: 9.5540\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.7116 - val_loss: 9.5700\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.7123 - val_loss: 9.5711\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.6965 - val_loss: 9.5912\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.6867 - val_loss: 9.5163\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.6632 - val_loss: 9.6281\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.6724 - val_loss: 9.7040\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.6395 - val_loss: 9.4448\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.6387 - val_loss: 9.7329\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.6183 - val_loss: 9.4714\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.6041 - val_loss: 9.4476\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.5950 - val_loss: 9.3514\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.5990 - val_loss: 9.3057\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.5715 - val_loss: 9.5219\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.5662 - val_loss: 9.3143\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.5654 - val_loss: 9.3050\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.5406 - val_loss: 9.3115\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.5388 - val_loss: 9.2769\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.5367 - val_loss: 9.3076\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.5107 - val_loss: 9.4559\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.5260 - val_loss: 9.2923\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.4944 - val_loss: 9.2276\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.5037 - val_loss: 9.2632\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.4784 - val_loss: 9.4126\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.4758 - val_loss: 9.2347\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.4728 - val_loss: 9.2987\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.4676 - val_loss: 9.2422\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.4427 - val_loss: 9.2205\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.4513 - val_loss: 9.1997\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.4443 - val_loss: 9.2642\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.4178 - val_loss: 9.3151\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.4150 - val_loss: 9.1491\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.4184 - val_loss: 9.2446\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.3997 - val_loss: 9.2732\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.4078 - val_loss: 9.3121\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.3975 - val_loss: 9.0985\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.3943 - val_loss: 9.2838\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.3905 - val_loss: 9.1206\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.3723 - val_loss: 9.2524\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.3626 - val_loss: 9.2438\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.3460 - val_loss: 9.1361\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.3602 - val_loss: 9.1679\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.3419 - val_loss: 9.3025\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.3324 - val_loss: 9.1869\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.3452 - val_loss: 9.2667\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.3265 - val_loss: 9.5078\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.3194 - val_loss: 9.1058\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.3145 - val_loss: 9.0730\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.3140 - val_loss: 9.3180\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.3052 - val_loss: 9.0709\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.3020 - val_loss: 9.1877\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.2979 - val_loss: 9.2327\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.2773 - val_loss: 9.2394\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.2773 - val_loss: 9.2882\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.2723 - val_loss: 9.8627\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.2942 - val_loss: 13.8165\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.2977 - val_loss: 9.4104\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.2743 - val_loss: 9.0575\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.2598 - val_loss: 9.2263\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.2423 - val_loss: 9.0280\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.2598 - val_loss: 9.1745\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.2415 - val_loss: 9.1612\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.2686 - val_loss: 9.2161\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.2735 - val_loss: 9.1243\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.2449 - val_loss: 9.1888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 105/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.2502 - val_loss: 9.0843\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.2332 - val_loss: 9.1014\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.2420 - val_loss: 9.0002\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.4190 - val_loss: 10.3456\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.6919 - val_loss: 9.4254\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.5666 - val_loss: 9.4212\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.4601 - val_loss: 9.1456\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.4002 - val_loss: 9.2719\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.3481 - val_loss: 9.2361\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.3126 - val_loss: 9.1197\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.2911 - val_loss: 9.3400\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.2831 - val_loss: 9.0270\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.2754 - val_loss: 9.0663\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.2519 - val_loss: 9.1580\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.2376 - val_loss: 9.5133\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.2283 - val_loss: 9.0784\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.2348 - val_loss: 9.1620\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.2274 - val_loss: 9.0809\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.2338 - val_loss: 9.1785\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.2248 - val_loss: 9.0518\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.1949 - val_loss: 9.0670\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.2113 - val_loss: 9.1223\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.1976 - val_loss: 9.0095\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 9.2030 - val_loss: 9.1072\n",
      "\n",
      "Loss: 910.72%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 202)       2020      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 94, 94, 202)      808       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 94, 94, 202)       0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 8)         14552     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 92, 92, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 136)       9928      \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 90, 90, 136)      544       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 108)       132300    \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 88, 88, 108)      432       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 136)       132328    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 292,948\n",
      "Trainable params: 292,038\n",
      "Non-trainable params: 910\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 22s 81ms/step - loss: 1299.3503 - val_loss: 21.9787\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 11.5154 - val_loss: 10.6006\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0629 - val_loss: 10.7086\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0445 - val_loss: 10.3699\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0306 - val_loss: 10.0202\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0295 - val_loss: 10.2036\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 19s 77ms/step - loss: 10.0452 - val_loss: 10.5724\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0357 - val_loss: 10.2349\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0404 - val_loss: 9.9274\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0450 - val_loss: 10.3098\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0428 - val_loss: 10.5018\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0413 - val_loss: 9.8555\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0533 - val_loss: 9.8410\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0553 - val_loss: 9.8944\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0819 - val_loss: 10.4183\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0516 - val_loss: 9.9388\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0489 - val_loss: 10.2841\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0689 - val_loss: 9.9276\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0742 - val_loss: 9.8373\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0769 - val_loss: 11.0085\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0488 - val_loss: 10.0153\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0769 - val_loss: 9.8067\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0749 - val_loss: 10.1034\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0752 - val_loss: 10.3438\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0699 - val_loss: 10.0403\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0678 - val_loss: 10.0643\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0850 - val_loss: 10.2059\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0788 - val_loss: 9.9209\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0590 - val_loss: 10.2968\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0458 - val_loss: 9.8296\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0878 - val_loss: 9.8695\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0555 - val_loss: 10.1092\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0536 - val_loss: 9.7908\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0848 - val_loss: 9.8865\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0796 - val_loss: 10.1418\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0622 - val_loss: 9.8204\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0632 - val_loss: 9.7420\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0600 - val_loss: 10.1870\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0938 - val_loss: 9.8108\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0655 - val_loss: 10.0534\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0736 - val_loss: 10.1290\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0705 - val_loss: 9.7581\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.1045 - val_loss: 10.0599\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0867 - val_loss: 9.9671\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0556 - val_loss: 9.8928\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0506 - val_loss: 9.9049\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0743 - val_loss: 9.8305\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0592 - val_loss: 10.4192\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0479 - val_loss: 9.9529\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0567 - val_loss: 9.8948\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0961 - val_loss: 9.8582\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0449 - val_loss: 9.8437\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0783 - val_loss: 9.9985\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0652 - val_loss: 10.0625\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0616 - val_loss: 9.9130\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0760 - val_loss: 9.9651\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0275 - val_loss: 10.1857\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0782 - val_loss: 9.8639\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0591 - val_loss: 9.8655\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0392 - val_loss: 9.9647\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0322 - val_loss: 9.7230\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0800 - val_loss: 9.8329\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0541 - val_loss: 9.8811\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0791 - val_loss: 10.1263\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0512 - val_loss: 9.8885\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0649 - val_loss: 9.9463\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0544 - val_loss: 9.8020\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0514 - val_loss: 9.9249\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0481 - val_loss: 9.7301\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0558 - val_loss: 10.0683\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0564 - val_loss: 9.9890\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0395 - val_loss: 10.1652\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0529 - val_loss: 9.7649\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0423 - val_loss: 9.7693\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0426 - val_loss: 9.8620\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0439 - val_loss: 10.0882\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0684 - val_loss: 9.7991\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0598 - val_loss: 9.8110\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0473 - val_loss: 10.3717\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0781 - val_loss: 9.9831\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0279 - val_loss: 9.9431\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0902 - val_loss: 9.9536\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0529 - val_loss: 9.8467\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0500 - val_loss: 9.8087\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0266 - val_loss: 10.1414\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0346 - val_loss: 9.7355\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0677 - val_loss: 9.8954\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0375 - val_loss: 10.1771\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0689 - val_loss: 9.9395\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0630 - val_loss: 9.7854\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0284 - val_loss: 9.8694\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0262 - val_loss: 9.8635\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0523 - val_loss: 9.9201\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0578 - val_loss: 9.8377\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0598 - val_loss: 9.7715\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0228 - val_loss: 9.8738\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0335 - val_loss: 10.0555\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0357 - val_loss: 9.9061\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0498 - val_loss: 9.8057\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0476 - val_loss: 9.7753\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0206 - val_loss: 9.7746\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0257 - val_loss: 9.7516\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0675 - val_loss: 10.1294\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0685 - val_loss: 9.8310\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0231 - val_loss: 9.8664\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0251 - val_loss: 9.7181\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0086 - val_loss: 9.9036\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0420 - val_loss: 9.7474\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0306 - val_loss: 9.8650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 110/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0503 - val_loss: 9.7855\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0356 - val_loss: 9.8350\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0630 - val_loss: 9.9086\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0147 - val_loss: 9.7212\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0120 - val_loss: 10.1316\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0584 - val_loss: 9.8900\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0161 - val_loss: 9.9167\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 9.9976 - val_loss: 9.9135\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0249 - val_loss: 9.9137\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0267 - val_loss: 9.7689\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0450 - val_loss: 9.8730\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0155 - val_loss: 9.7437\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0302 - val_loss: 10.0634\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0161 - val_loss: 9.7688\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0256 - val_loss: 10.0013\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0269 - val_loss: 9.8233\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0240 - val_loss: 9.8185\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0415 - val_loss: 9.7416\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 10.0133 - val_loss: 9.9379\n",
      "\n",
      "Loss: 993.79%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 128)       1280      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 94, 94, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 94, 94, 128)       0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 256)       295168    \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 90, 90, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 74)        170570    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 88, 88, 74)       296       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 11)        7337      \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 86, 86, 11)       44        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 84, 84, 136)       13600     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,079,915\n",
      "Trainable params: 1,078,975\n",
      "Non-trainable params: 940\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 32s 121ms/step - loss: 2630.8079 - val_loss: 2165.4014\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 866.5045 - val_loss: 303.9241\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 98.4252 - val_loss: 33.8785\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 16.9018 - val_loss: 11.9550\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 10.6412 - val_loss: 10.3820\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 10.1237 - val_loss: 9.9325\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 10.0827 - val_loss: 10.0425\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 10.0647 - val_loss: 9.9244\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 10.0516 - val_loss: 9.9334\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 10.0350 - val_loss: 9.7723\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 10.0229 - val_loss: 9.8453\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 10.0182 - val_loss: 9.8125\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 10.0073 - val_loss: 9.8476\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 10.0173 - val_loss: 9.7642\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 10.0083 - val_loss: 9.7457\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.9963 - val_loss: 9.8533\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 10.0020 - val_loss: 9.7342\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 10.0082 - val_loss: 9.9875\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.9934 - val_loss: 9.9272\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 10.0024 - val_loss: 9.8105\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 10.0105 - val_loss: 9.9777\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 10.0111 - val_loss: 9.8051\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 10.0081 - val_loss: 10.0232\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.9940 - val_loss: 9.7947\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 10.0122 - val_loss: 9.7927\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.9786 - val_loss: 9.7791\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.9849 - val_loss: 9.8855\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.9924 - val_loss: 9.8126\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.9909 - val_loss: 9.7473\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.9772 - val_loss: 9.7793\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.9851 - val_loss: 9.7468\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.9745 - val_loss: 9.7497\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.9879 - val_loss: 9.9281\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.9993 - val_loss: 9.7319\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.9806 - val_loss: 9.7601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.9761 - val_loss: 9.7723\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.9917 - val_loss: 9.8369\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.9649 - val_loss: 9.7972\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.9809 - val_loss: 9.7163\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.9622 - val_loss: 9.8210\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.9646 - val_loss: 9.7592\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.9834 - val_loss: 9.7732\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.9654 - val_loss: 9.7022\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.9782 - val_loss: 9.7337\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.9771 - val_loss: 9.8759\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.9715 - val_loss: 9.7634\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.9770 - val_loss: 9.7548\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.9670 - val_loss: 9.9840\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.9771 - val_loss: 9.7914\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.9529 - val_loss: 9.7628\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.9589 - val_loss: 9.7548\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.9675 - val_loss: 9.7568\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.9534 - val_loss: 9.7302\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.9661 - val_loss: 9.6866\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.9638 - val_loss: 9.7377\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.9766 - val_loss: 9.8073\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.9639 - val_loss: 9.7453\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.9614 - val_loss: 9.8267\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.9646 - val_loss: 9.7603\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.9475 - val_loss: 9.8216\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.9561 - val_loss: 9.7256\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.9502 - val_loss: 9.7893\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.9474 - val_loss: 9.7046\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.9493 - val_loss: 9.7404\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.9635 - val_loss: 9.7010\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.9479 - val_loss: 9.7261\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.9369 - val_loss: 9.7536\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.9490 - val_loss: 9.7376\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.9545 - val_loss: 9.7415\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.9354 - val_loss: 9.7259\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.9457 - val_loss: 9.7636\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.9310 - val_loss: 9.6940\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.9447 - val_loss: 9.7273\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.9248 - val_loss: 9.7611\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.9222 - val_loss: 9.7226\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.9259 - val_loss: 9.8055\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.9141 - val_loss: 9.6760\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.9177 - val_loss: 9.7107\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.9185 - val_loss: 9.7295\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.9102 - val_loss: 9.6982\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.9367 - val_loss: 9.6875\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.9040 - val_loss: 9.6567\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.8950 - val_loss: 9.7606\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.8984 - val_loss: 9.7812\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.9007 - val_loss: 9.6844\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.8973 - val_loss: 9.7443\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.8849 - val_loss: 9.6510\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.8839 - val_loss: 9.6575\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.8850 - val_loss: 9.7126\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.8783 - val_loss: 9.7526\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.8716 - val_loss: 9.8484\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.8708 - val_loss: 9.6790\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.8832 - val_loss: 9.6287\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.8628 - val_loss: 9.7831\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.8637 - val_loss: 9.6429\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.8604 - val_loss: 9.6085\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.8601 - val_loss: 9.7355\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.8555 - val_loss: 9.6517\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.8467 - val_loss: 9.6085\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.8362 - val_loss: 9.5847\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.8502 - val_loss: 9.6109\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.8291 - val_loss: 9.7092\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.8478 - val_loss: 9.7125\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.8309 - val_loss: 9.6088\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.8283 - val_loss: 9.6514\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.8187 - val_loss: 9.6087\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.8178 - val_loss: 9.5849\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.8140 - val_loss: 9.6319\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.8142 - val_loss: 9.5796\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.8198 - val_loss: 9.7217\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.7993 - val_loss: 9.5672\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.7944 - val_loss: 9.6827\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.7925 - val_loss: 9.5290\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 114/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.7882 - val_loss: 9.5195\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.7868 - val_loss: 9.6297\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.7931 - val_loss: 9.5611\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.7837 - val_loss: 9.5814\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.8024 - val_loss: 9.6159\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.7661 - val_loss: 9.6373\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.7757 - val_loss: 9.6853\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.7766 - val_loss: 9.6176\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.7594 - val_loss: 9.6336\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.7704 - val_loss: 9.6128\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.7592 - val_loss: 9.4924\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.7518 - val_loss: 9.7951\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.7621 - val_loss: 9.5907\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.7573 - val_loss: 9.6217\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 29s 117ms/step - loss: 9.7496 - val_loss: 9.5966\n",
      "\n",
      "Loss: 959.66%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 112)       1120      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 112)       113008    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 92, 92, 112)      448       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 92, 92, 112)       0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 8)         8072      \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 90, 90, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 84)        6132      \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 88, 88, 84)       336       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 136)       102952    \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 86, 86, 136)      544       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 84, 84, 136)       166600    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 399,248\n",
      "Trainable params: 398,566\n",
      "Non-trainable params: 682\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 22s 80ms/step - loss: 1191.3152 - val_loss: 22.7047\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 10.9123 - val_loss: 11.9536\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 10.3683 - val_loss: 11.5391\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 10.2886 - val_loss: 10.3437\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 19s 77ms/step - loss: 10.2629 - val_loss: 11.4429\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 19s 77ms/step - loss: 10.2360 - val_loss: 10.7760\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 10.2030 - val_loss: 9.9880\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 10.1613 - val_loss: 10.3358\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 19s 77ms/step - loss: 10.2100 - val_loss: 10.1129\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 19s 77ms/step - loss: 10.1559 - val_loss: 10.2726\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 10.1420 - val_loss: 32.3223\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 10.1292 - val_loss: 10.3712\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 10.1250 - val_loss: 10.3417\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 10.1441 - val_loss: 9.8614\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 10.0969 - val_loss: 10.2065\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 10.1064 - val_loss: 9.8469\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 10.1284 - val_loss: 10.5784\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 10.1222 - val_loss: 11.5759\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 10.1495 - val_loss: 9.9704\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 10.1513 - val_loss: 11.1025\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 19s 77ms/step - loss: 10.1457 - val_loss: 10.2111\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 19s 77ms/step - loss: 10.1426 - val_loss: 9.8118\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 10.1161 - val_loss: 10.5846\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 10.1163 - val_loss: 9.9535\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 10.1169 - val_loss: 9.9449\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 19s 77ms/step - loss: 10.1300 - val_loss: 10.1186\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 10.1457 - val_loss: 9.9334\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 19s 77ms/step - loss: 10.1226 - val_loss: 10.1004\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 10.0943 - val_loss: 9.9171\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 10.0894 - val_loss: 10.1851\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 10.1106 - val_loss: 9.9676\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 10.1237 - val_loss: 9.7718\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 19s 77ms/step - loss: 10.0800 - val_loss: 9.8681\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 10.1098 - val_loss: 9.8319\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 19s 77ms/step - loss: 10.0806 - val_loss: 9.9464\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 10.0947 - val_loss: 9.7583\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 10.0799 - val_loss: 9.9557\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 10.0832 - val_loss: 10.2218\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 10.0998 - val_loss: 10.1904\n",
      "Epoch 40/128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 19s 77ms/step - loss: 10.1433 - val_loss: 9.8592\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 10.0659 - val_loss: 9.8422\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 19s 77ms/step - loss: 10.0686 - val_loss: 10.1868\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 10.0935 - val_loss: 9.7624\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 10.0793 - val_loss: 10.3011\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 19s 77ms/step - loss: 10.0585 - val_loss: 9.8514\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 10.0411 - val_loss: 9.9057\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 10.0588 - val_loss: 10.0073\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 19s 77ms/step - loss: 10.0618 - val_loss: 9.7512\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 19s 77ms/step - loss: 10.0156 - val_loss: 9.9791\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 19s 77ms/step - loss: 10.0354 - val_loss: 10.1267\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 10.0335 - val_loss: 10.2997\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 10.0219 - val_loss: 10.0996\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 10.0340 - val_loss: 10.0560\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 19s 77ms/step - loss: 10.0543 - val_loss: 11.5436\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 19s 77ms/step - loss: 10.0266 - val_loss: 10.0393\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 19s 77ms/step - loss: 10.0356 - val_loss: 10.1695\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 10.0221 - val_loss: 10.0544\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 10.0297 - val_loss: 10.0997\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 10.0574 - val_loss: 9.8384\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 10.0416 - val_loss: 9.9471\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 19s 77ms/step - loss: 10.0094 - val_loss: 10.2105\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 19s 77ms/step - loss: 10.0474 - val_loss: 9.7416\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 19s 77ms/step - loss: 10.0082 - val_loss: 10.0049\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 19s 77ms/step - loss: 10.0001 - val_loss: 9.8254\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 10.0093 - val_loss: 9.9361\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 19s 77ms/step - loss: 10.0245 - val_loss: 9.9085\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 10.0231 - val_loss: 9.7523\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 19s 77ms/step - loss: 10.0116 - val_loss: 10.0201\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 10.0170 - val_loss: 9.7985\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 9.9978 - val_loss: 9.7938\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 9.9918 - val_loss: 9.8822\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 10.0008 - val_loss: 10.0075\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 9.9972 - val_loss: 10.3068\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 19s 77ms/step - loss: 10.0085 - val_loss: 9.7827\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 10.0031 - val_loss: 9.9727\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 9.9958 - val_loss: 10.0749\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 9.9866 - val_loss: 9.7408\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 19s 77ms/step - loss: 9.9758 - val_loss: 9.9409\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 10.0112 - val_loss: 9.8430\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 9.9904 - val_loss: 9.8409\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 10.0073 - val_loss: 10.1216\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 9.9972 - val_loss: 9.9754\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 19s 77ms/step - loss: 9.9868 - val_loss: 9.8220\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 19s 77ms/step - loss: 9.9732 - val_loss: 10.1112\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 9.9844 - val_loss: 9.9490\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 9.9903 - val_loss: 9.8548\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 10.0986 - val_loss: 9.8423\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 10.0192 - val_loss: 9.7594\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 19s 77ms/step - loss: 9.9915 - val_loss: 9.8116\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 10.0047 - val_loss: 9.6622\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 10.0058 - val_loss: 9.7213\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 9.9789 - val_loss: 9.8137\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 9.9993 - val_loss: 9.8043\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 10.0106 - val_loss: 9.9551\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 9.9827 - val_loss: 9.9333\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 19s 77ms/step - loss: 9.9656 - val_loss: 9.8102\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 19s 77ms/step - loss: 9.9564 - val_loss: 9.7677\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 19s 77ms/step - loss: 9.9785 - val_loss: 9.7452\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 9.9540 - val_loss: 9.8428\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 9.9684 - val_loss: 9.8109\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 19s 77ms/step - loss: 9.9730 - val_loss: 9.7821\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 9.9598 - val_loss: 9.9077\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 9.9914 - val_loss: 9.9976\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 9.9620 - val_loss: 9.9043\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 9.9673 - val_loss: 9.7793\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 9.9539 - val_loss: 9.8665\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 9.9370 - val_loss: 9.8550\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 9.9821 - val_loss: 9.8323\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 9.9615 - val_loss: 9.7105\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 9.9734 - val_loss: 9.9878\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 9.9534 - val_loss: 9.7850\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 19s 77ms/step - loss: 9.9725 - val_loss: 9.8415\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 9.9418 - val_loss: 9.7981\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 9.9725 - val_loss: 9.7432\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 9.9706 - val_loss: 9.9289\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 9.9682 - val_loss: 9.9357\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 19s 77ms/step - loss: 9.9461 - val_loss: 9.8369\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 9.9643 - val_loss: 10.0564\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 9.9547 - val_loss: 10.0017\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 9.9441 - val_loss: 9.8274\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 9.9490 - val_loss: 9.8376\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 9.9647 - val_loss: 9.7796\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 9.9646 - val_loss: 9.7181\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 9.9518 - val_loss: 9.9259\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 9.9622 - val_loss: 9.6625\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 9.9313 - val_loss: 9.9119\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 9.9369 - val_loss: 10.1754\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 9.9558 - val_loss: 9.8000\n",
      "\n",
      "Loss: 980.00%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 73)        730       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 73)        48034     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 92, 92, 73)       292       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 92, 92, 73)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 8)         5264      \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 90, 90, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 90, 90, 8)         0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 80)        5840      \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 88, 88, 80)       320       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 88, 88, 80)        0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 102)       73542     \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 86, 86, 102)      408       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 84, 84, 136)       124984    \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 84, 84, 136)      544       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 82, 82, 8)         9800      \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 82, 82, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 80, 80, 136)       9928      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 279,754\n",
      "Trainable params: 278,938\n",
      "Non-trainable params: 816\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 22s 80ms/step - loss: 2734.2283 - val_loss: 1964.5753\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 1202.1337 - val_loss: 525.5821\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 219.0292 - val_loss: 75.1514\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 34.0444 - val_loss: 17.1758\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 12.7753 - val_loss: 10.6955\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 10.3290 - val_loss: 9.8139\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 10.0174 - val_loss: 9.7696\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.9876 - val_loss: 9.7711\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.9686 - val_loss: 9.7662\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.9527 - val_loss: 9.6932\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.9526 - val_loss: 9.7172\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.9579 - val_loss: 9.7144\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.9445 - val_loss: 9.6901\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.9393 - val_loss: 9.7679\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.9538 - val_loss: 9.6840\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.9337 - val_loss: 9.7050\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.9365 - val_loss: 9.9023\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.9439 - val_loss: 9.7179\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.9354 - val_loss: 9.7108\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.9374 - val_loss: 9.7038\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.9364 - val_loss: 9.6822\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.9410 - val_loss: 9.7087\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.9323 - val_loss: 9.7211\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.9280 - val_loss: 9.7878\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.9397 - val_loss: 9.7510\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.9375 - val_loss: 9.7087\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.9341 - val_loss: 9.7320\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.9260 - val_loss: 9.7004\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.9220 - val_loss: 9.7440\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.9272 - val_loss: 9.7280\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.9242 - val_loss: 9.6970\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.9245 - val_loss: 9.7383\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.9271 - val_loss: 9.6605\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.9275 - val_loss: 9.7116\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.9236 - val_loss: 9.7026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.9284 - val_loss: 9.6946\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.9200 - val_loss: 9.7117\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.9176 - val_loss: 9.7175\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.9325 - val_loss: 9.7614\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.9168 - val_loss: 9.6955\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.9266 - val_loss: 9.8154\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.9128 - val_loss: 9.7065\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.9048 - val_loss: 9.7142\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.9032 - val_loss: 9.6799\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.9065 - val_loss: 9.7094\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.9033 - val_loss: 9.6564\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.9096 - val_loss: 9.6761\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.9047 - val_loss: 9.7367\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.9198 - val_loss: 9.7653\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.9092 - val_loss: 9.7217\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.8993 - val_loss: 9.7052\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.8952 - val_loss: 9.7054\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.8984 - val_loss: 9.6645\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.8910 - val_loss: 9.7267\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.9033 - val_loss: 9.6856\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.8969 - val_loss: 9.6443\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.8791 - val_loss: 9.6518\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.8845 - val_loss: 9.6511\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.8853 - val_loss: 9.6189\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.8772 - val_loss: 9.6931\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.8912 - val_loss: 9.7784\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.8665 - val_loss: 9.6783\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.8546 - val_loss: 9.6254\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.8708 - val_loss: 9.6736\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.8711 - val_loss: 9.6277\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.8693 - val_loss: 9.6588\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.8523 - val_loss: 9.7128\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.8536 - val_loss: 9.6405\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.8578 - val_loss: 9.7512\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.8530 - val_loss: 9.6732\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.8603 - val_loss: 9.6054\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.8276 - val_loss: 9.6755\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.8382 - val_loss: 9.5796\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.8498 - val_loss: 9.6045\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.8332 - val_loss: 9.6152\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.8223 - val_loss: 9.5547\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.8270 - val_loss: 9.6016\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.8230 - val_loss: 9.5755\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.8249 - val_loss: 9.6441\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.8207 - val_loss: 9.5723\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.8166 - val_loss: 9.6643\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.8162 - val_loss: 9.6035\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.8130 - val_loss: 9.5656\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.8078 - val_loss: 9.5316\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.7985 - val_loss: 9.6006\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.7933 - val_loss: 9.5919\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.7984 - val_loss: 9.5809\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.7966 - val_loss: 9.6688\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.7984 - val_loss: 9.5608\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.7925 - val_loss: 9.5202\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.7844 - val_loss: 9.5634\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.7869 - val_loss: 9.5012\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.7859 - val_loss: 9.5374\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.7723 - val_loss: 9.5594\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.7824 - val_loss: 9.5198\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.7784 - val_loss: 9.5164\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.7699 - val_loss: 9.5347\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.7817 - val_loss: 9.5285\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.7709 - val_loss: 9.5200\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.7794 - val_loss: 9.5075\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.7762 - val_loss: 9.5304\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.7549 - val_loss: 9.5034\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.7745 - val_loss: 9.5479\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.7616 - val_loss: 9.4781\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.7531 - val_loss: 9.5706\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.7646 - val_loss: 9.4795\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.7493 - val_loss: 9.6685\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.7394 - val_loss: 9.6123\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.7499 - val_loss: 9.5447\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.7527 - val_loss: 9.5297\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.7685 - val_loss: 9.5339\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.7407 - val_loss: 9.5098\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.7360 - val_loss: 9.4253\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.7440 - val_loss: 9.4614\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.7333 - val_loss: 9.4346\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.7327 - val_loss: 9.4982\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.7376 - val_loss: 9.4608\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.7203 - val_loss: 9.4482\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.7307 - val_loss: 9.4741\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.7280 - val_loss: 9.4811\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.7296 - val_loss: 9.4576\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.7293 - val_loss: 9.5379\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.7194 - val_loss: 9.5034\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.7196 - val_loss: 9.5036\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.7380 - val_loss: 9.4651\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.7174 - val_loss: 9.4091\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.7158 - val_loss: 9.4348\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 9.7122 - val_loss: 9.4419\n",
      "\n",
      "Loss: 944.19%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 256)       2560      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 92, 92, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 92, 92, 256)       0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 256)       590080    \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 88, 88, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 8)         18440     \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 86, 86, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 84, 84, 8)         584       \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 84, 84, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 82, 82, 136)       9928      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,803,868\n",
      "Trainable params: 1,802,810\n",
      "Non-trainable params: 1,058\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 41s 160ms/step - loss: 2734.4805 - val_loss: 2004.1801\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 1203.5771 - val_loss: 465.5938\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 220.0828 - val_loss: 67.8381\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 34.0655 - val_loss: 15.8906\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 12.7301 - val_loss: 10.5005\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 10.3001 - val_loss: 9.8877\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.9972 - val_loss: 9.7862\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.9514 - val_loss: 9.7018\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.9461 - val_loss: 9.7909\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.9587 - val_loss: 9.7106\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.9364 - val_loss: 9.6785\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.9362 - val_loss: 9.7119\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.9363 - val_loss: 9.7590\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.9383 - val_loss: 9.7411\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.9423 - val_loss: 9.6901\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.9354 - val_loss: 9.6939\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.9340 - val_loss: 9.6996\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.9238 - val_loss: 9.7256\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.9332 - val_loss: 9.6817\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.9340 - val_loss: 9.6768\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.9312 - val_loss: 9.7413\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.9257 - val_loss: 9.8174\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.9294 - val_loss: 9.7053\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.9189 - val_loss: 9.6677\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.9251 - val_loss: 9.7738\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.9132 - val_loss: 9.6588\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.9108 - val_loss: 9.6873\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.8959 - val_loss: 9.7039\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.8914 - val_loss: 9.7027\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.8877 - val_loss: 9.7137\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.8847 - val_loss: 9.6726\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.8701 - val_loss: 9.7172\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.8680 - val_loss: 9.5920\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.8684 - val_loss: 9.6203\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.8525 - val_loss: 9.8160\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.8545 - val_loss: 9.7547\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.8512 - val_loss: 9.7446\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.8332 - val_loss: 9.5692\n",
      "Epoch 39/128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 40s 159ms/step - loss: 9.8283 - val_loss: 9.7179\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.8234 - val_loss: 9.6238\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.8163 - val_loss: 9.5586\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.8196 - val_loss: 9.5397\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.8083 - val_loss: 9.6689\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.7952 - val_loss: 9.5643\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.7774 - val_loss: 9.5089\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.7601 - val_loss: 9.4928\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.7380 - val_loss: 9.5025\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.7254 - val_loss: 9.5960\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.7098 - val_loss: 9.7228\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.7062 - val_loss: 9.4199\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.6718 - val_loss: 9.3847\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.6516 - val_loss: 9.5263\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.6439 - val_loss: 9.4936\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.6316 - val_loss: 9.5902\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.6285 - val_loss: 9.5770\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.6036 - val_loss: 9.3615\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.5955 - val_loss: 9.3441\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.5582 - val_loss: 9.7830\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.5740 - val_loss: 9.3900\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.5559 - val_loss: 9.4388\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.5519 - val_loss: 9.2692\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.5422 - val_loss: 9.3692\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.5338 - val_loss: 9.3414\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.5161 - val_loss: 9.3662\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.5316 - val_loss: 9.3776\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.4946 - val_loss: 9.3365\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.4901 - val_loss: 9.3915\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.4740 - val_loss: 9.2781\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.4759 - val_loss: 9.2741\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.4648 - val_loss: 9.3385\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.4753 - val_loss: 9.2927\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.4631 - val_loss: 9.2951\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.4513 - val_loss: 9.3205\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.4508 - val_loss: 9.1417\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.4369 - val_loss: 9.4911\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.4322 - val_loss: 9.4703\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.4260 - val_loss: 9.3371\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.4223 - val_loss: 9.1964\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.4022 - val_loss: 9.3249\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.4018 - val_loss: 9.2180\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.3968 - val_loss: 9.1508\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.3955 - val_loss: 9.3034\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.3876 - val_loss: 9.2459\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.3654 - val_loss: 9.2431\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.3644 - val_loss: 9.3365\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.3733 - val_loss: 9.2237\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.3467 - val_loss: 9.1684\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.3462 - val_loss: 9.2134\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.3606 - val_loss: 9.2889\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.3377 - val_loss: 9.2023\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.3347 - val_loss: 9.1841\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.3235 - val_loss: 9.1216\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.3326 - val_loss: 9.1528\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.3295 - val_loss: 9.1214\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.3200 - val_loss: 9.2220\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.3026 - val_loss: 9.2962\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.3064 - val_loss: 9.4078\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.2924 - val_loss: 9.1263\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.3109 - val_loss: 9.1103\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.2888 - val_loss: 9.2615\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.3035 - val_loss: 9.2997\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.2904 - val_loss: 9.2227\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.2817 - val_loss: 9.2514\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.2827 - val_loss: 9.1866\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.2764 - val_loss: 9.1073\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.2725 - val_loss: 9.2070\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.2565 - val_loss: 9.1891\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.2539 - val_loss: 9.2854\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.2560 - val_loss: 9.1168\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.2506 - val_loss: 9.0638\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.2516 - val_loss: 9.2303\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.2345 - val_loss: 9.1887\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.2353 - val_loss: 9.3392\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.2410 - val_loss: 9.4702\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.2218 - val_loss: 9.1123\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.2366 - val_loss: 9.1854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.2260 - val_loss: 9.0949\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.2196 - val_loss: 9.2624\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.2090 - val_loss: 9.3394\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.2073 - val_loss: 9.2208\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.2160 - val_loss: 9.1085\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.2159 - val_loss: 9.1959\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.2086 - val_loss: 9.1228\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.2025 - val_loss: 9.1892\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.1790 - val_loss: 9.2381\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.1881 - val_loss: 9.1555\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.1841 - val_loss: 9.1226\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 9.1838 - val_loss: 9.0875\n",
      "\n",
      "Loss: 908.75%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 64)        640       \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 94, 94, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 94, 94, 64)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 132)       76164     \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 132)       156948    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 90, 90, 132)      528       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 90, 90, 132)       0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 256)       304384    \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 86, 86, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 86, 86, 256)       0         \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 84, 84, 8)         18440     \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 82, 82, 8)         584       \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 82, 82, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 80, 80, 73)        5329      \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 80, 80, 73)       292       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 78, 78, 61)        40138     \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 78, 78, 61)       244       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 76, 76, 136)       74800     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,269,887\n",
      "Trainable params: 1,268,697\n",
      "Non-trainable params: 1,190\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 44s 163ms/step - loss: 1646.3833 - val_loss: 142.2049\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 26.0449 - val_loss: 12.5208\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 10.4947 - val_loss: 11.8421\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 10.5106 - val_loss: 13.2036\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 10.3189 - val_loss: 11.7576\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 10.2418 - val_loss: 11.5224\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 10.1905 - val_loss: 10.6491\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 10.1707 - val_loss: 10.5770\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 10.1831 - val_loss: 10.5559\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 10.1238 - val_loss: 10.7592\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 10.0975 - val_loss: 10.3702\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 10.0758 - val_loss: 10.0998\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 10.0967 - val_loss: 10.6858\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 10.0867 - val_loss: 10.2337\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 10.0997 - val_loss: 10.9741\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 10.0815 - val_loss: 10.4259\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 10.0602 - val_loss: 10.2522\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 10.1100 - val_loss: 10.7344\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 10.0742 - val_loss: 10.0593\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 10.0622 - val_loss: 10.0736\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 10.0777 - val_loss: 11.1552\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 10.0844 - val_loss: 10.2429\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 10.0567 - val_loss: 9.8995\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 10.0589 - val_loss: 9.9310\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 10.0463 - val_loss: 10.2301\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 10.0669 - val_loss: 9.9687\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 10.0450 - val_loss: 10.0001\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 10.0437 - val_loss: 9.9258\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 10.0551 - val_loss: 10.0356\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 10.0637 - val_loss: 10.1204\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 10.0599 - val_loss: 10.2178\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 10.0610 - val_loss: 9.8828\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 10.0414 - val_loss: 9.7099\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 10.0445 - val_loss: 9.8990\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 10.0148 - val_loss: 9.9124\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 10.0203 - val_loss: 9.8537\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 10.0324 - val_loss: 9.7522\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 10.0069 - val_loss: 10.0649\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 10.0104 - val_loss: 9.7369\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 9.9896 - val_loss: 9.7677\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 10.0148 - val_loss: 10.0275\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 10.0016 - val_loss: 9.7596\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 9.9783 - val_loss: 10.1672\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 9.9687 - val_loss: 9.8406\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 9.9328 - val_loss: 9.8260\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 9.9308 - val_loss: 9.7679\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 9.9249 - val_loss: 9.7865\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 9.9058 - val_loss: 9.7995\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 9.8897 - val_loss: 9.5915\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 9.8873 - val_loss: 9.9117\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 9.8955 - val_loss: 9.5963\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 9.8646 - val_loss: 9.6992\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 9.8599 - val_loss: 9.5508\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 9.8067 - val_loss: 9.7505\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 9.7985 - val_loss: 9.5284\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 9.7765 - val_loss: 9.8078\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 9.7756 - val_loss: 9.5024\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 9.7341 - val_loss: 9.8406\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 9.7365 - val_loss: 9.5806\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 9.7054 - val_loss: 9.5528\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 9.6873 - val_loss: 9.7215\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 9.6682 - val_loss: 9.4438\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 9.6341 - val_loss: 9.5578\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 9.6167 - val_loss: 9.4825\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 9.6323 - val_loss: 9.5020\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 9.5808 - val_loss: 9.7730\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 9.5475 - val_loss: 9.5475\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 9.5204 - val_loss: 9.3192\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 9.5387 - val_loss: 9.7294\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 9.4754 - val_loss: 9.5217\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 9.4486 - val_loss: 9.4256\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 9.4284 - val_loss: 9.3572\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 9.4098 - val_loss: 9.4035\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 9.3674 - val_loss: 9.3299\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 9.3250 - val_loss: 9.1480\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 9.3072 - val_loss: 9.2375\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 9.2661 - val_loss: 9.1049\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 9.2555 - val_loss: 8.9606\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 9.2188 - val_loss: 9.1056\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 9.1935 - val_loss: 8.8994\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 9.1538 - val_loss: 8.8492\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 9.1380 - val_loss: 8.8435\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 9.0854 - val_loss: 8.8768\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 9.0503 - val_loss: 8.8900\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 9.0446 - val_loss: 9.1168\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 9.0300 - val_loss: 8.9890\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 9.0215 - val_loss: 9.0734\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 8.9989 - val_loss: 9.2107\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 8.9388 - val_loss: 9.1448\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 8.9261 - val_loss: 9.0638\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 8.8826 - val_loss: 8.7665\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 8.8795 - val_loss: 8.7908\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 8.8824 - val_loss: 9.1420\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 8.8464 - val_loss: 8.8193\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 8.7962 - val_loss: 8.7632\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 8.7906 - val_loss: 8.7104\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 8.7674 - val_loss: 8.7763\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 8.7674 - val_loss: 8.7503\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 8.7152 - val_loss: 8.8379\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 8.6827 - val_loss: 8.6245\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 8.6861 - val_loss: 8.6645\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 8.6618 - val_loss: 9.0318\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 8.6727 - val_loss: 9.2512\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 8.6237 - val_loss: 8.7967\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 8.6170 - val_loss: 8.6701\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 8.6152 - val_loss: 8.6824\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 8.5773 - val_loss: 8.5461\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 8.5437 - val_loss: 9.0212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 109/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 8.5496 - val_loss: 8.7769\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 8.5481 - val_loss: 8.5080\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 8.5381 - val_loss: 8.9229\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 8.5118 - val_loss: 8.6684\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 8.4808 - val_loss: 8.8322\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 8.5147 - val_loss: 8.7460\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 8.4253 - val_loss: 8.5864\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 8.4022 - val_loss: 8.9449\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 8.4405 - val_loss: 8.8219\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 8.4218 - val_loss: 9.3940\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 8.3734 - val_loss: 8.6656\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 8.3982 - val_loss: 9.0129\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 8.3731 - val_loss: 8.8072\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 8.3631 - val_loss: 8.7201\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 8.3660 - val_loss: 8.6859\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 8.3591 - val_loss: 9.0378\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 8.2945 - val_loss: 8.5558\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 8.2833 - val_loss: 8.6197\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 8.3004 - val_loss: 8.8402\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 8.2777 - val_loss: 8.5700\n",
      "\n",
      "Loss: 857.00%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 8)         80        \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 94, 94, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 94, 94, 8)         0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 256)       18688     \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 90, 90, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 90, 90, 256)       0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 88, 88, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 8)         18440     \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 86, 86, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 84, 84, 136)       9928      \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 84, 84, 136)      544       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 82, 82, 136)       166600    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,396,556\n",
      "Trainable params: 1,395,226\n",
      "Non-trainable params: 1,330\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 36s 143ms/step - loss: 1190.6190 - val_loss: 58.4389\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 10.8717 - val_loss: 15.2539\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 10.2062 - val_loss: 10.9643\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 10.2156 - val_loss: 11.9613\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 10.1458 - val_loss: 10.9971\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 10.1350 - val_loss: 10.1321\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 10.1223 - val_loss: 10.0336\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 10.0749 - val_loss: 10.2034\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 10.1001 - val_loss: 10.2479\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 10.1194 - val_loss: 10.2953\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 10.1281 - val_loss: 10.0621\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 10.1187 - val_loss: 10.1879\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 10.0802 - val_loss: 10.1491\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 10.0891 - val_loss: 10.2896\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 10.1222 - val_loss: 9.8841\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 10.1174 - val_loss: 9.9828\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 10.1261 - val_loss: 10.1965\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 10.0902 - val_loss: 10.0782\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 10.0878 - val_loss: 9.8645\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 10.1496 - val_loss: 10.5107\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 10.1079 - val_loss: 10.9797\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 10.1304 - val_loss: 10.2355\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 10.1084 - val_loss: 10.1067\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 10.1334 - val_loss: 10.1707\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 10.1087 - val_loss: 10.8209\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 10.1283 - val_loss: 10.0488\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 10.1451 - val_loss: 10.7826\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 10.0902 - val_loss: 10.0096\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 10.0991 - val_loss: 10.1121\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 10.0879 - val_loss: 10.1100\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 10.0703 - val_loss: 10.0266\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 10.0753 - val_loss: 9.8478\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 10.0613 - val_loss: 10.7293\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 10.0568 - val_loss: 10.2358\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 10.0740 - val_loss: 10.2093\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 10.0493 - val_loss: 9.9966\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 10.0525 - val_loss: 10.0940\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 10.0348 - val_loss: 9.8812\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 10.0184 - val_loss: 10.1818\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 10.0485 - val_loss: 9.8737\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 10.0312 - val_loss: 10.0923\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 10.0303 - val_loss: 9.9831\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 10.0103 - val_loss: 9.7820\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.9973 - val_loss: 10.1829\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.9841 - val_loss: 10.3793\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.9636 - val_loss: 9.9282\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 10.0027 - val_loss: 10.2322\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.9757 - val_loss: 10.1377\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.9674 - val_loss: 9.6131\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.9642 - val_loss: 9.9402\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.9158 - val_loss: 9.9822\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.9294 - val_loss: 9.9213\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.8895 - val_loss: 10.5173\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.8838 - val_loss: 9.7092\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.8879 - val_loss: 9.6425\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.8761 - val_loss: 10.0113\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.8519 - val_loss: 10.5284\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.8439 - val_loss: 9.6921\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.8768 - val_loss: 9.8454\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.8083 - val_loss: 10.0193\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.8388 - val_loss: 9.7189\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.7891 - val_loss: 9.6628\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.8071 - val_loss: 11.3045\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.7721 - val_loss: 9.6842\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.7556 - val_loss: 13.0585\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.7719 - val_loss: 9.6428\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.7887 - val_loss: 10.3830\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.7422 - val_loss: 10.3498\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.7131 - val_loss: 9.5980\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.7564 - val_loss: 9.9829\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.7514 - val_loss: 9.7753\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.7248 - val_loss: 10.0612\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.7300 - val_loss: 9.5762\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.7301 - val_loss: 9.7701\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.6852 - val_loss: 10.5393\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.7091 - val_loss: 9.6097\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 35s 142ms/step - loss: 9.7081 - val_loss: 9.4880\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.6708 - val_loss: 9.7647\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.7106 - val_loss: 9.4810\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.7087 - val_loss: 9.6273\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.6389 - val_loss: 9.6380\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.6689 - val_loss: 9.6359\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.6671 - val_loss: 9.4657\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.6467 - val_loss: 9.3944\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.6633 - val_loss: 9.5477\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.6227 - val_loss: 9.8985\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.6224 - val_loss: 9.5318\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.6347 - val_loss: 9.6151\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.6199 - val_loss: 9.4146\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.6029 - val_loss: 9.6423\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.6242 - val_loss: 9.3342\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.6141 - val_loss: 9.4749\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.6140 - val_loss: 9.5085\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 35s 142ms/step - loss: 9.5962 - val_loss: 9.7691\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.6042 - val_loss: 9.5601\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.5876 - val_loss: 9.6851\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.5735 - val_loss: 9.4924\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.5750 - val_loss: 9.2737\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.5402 - val_loss: 9.4904\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.5636 - val_loss: 9.5555\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.5567 - val_loss: 9.6327\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.5881 - val_loss: 9.3521\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.5601 - val_loss: 9.6551\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.5578 - val_loss: 9.6699\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 35s 142ms/step - loss: 9.5300 - val_loss: 9.3446\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 35s 142ms/step - loss: 9.5133 - val_loss: 9.3398\n",
      "Epoch 107/128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 36s 142ms/step - loss: 9.5354 - val_loss: 9.5645\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.5068 - val_loss: 9.3573\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.5261 - val_loss: 9.3893\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.4991 - val_loss: 9.3236\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.4969 - val_loss: 9.3969\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.5237 - val_loss: 9.6172\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 35s 142ms/step - loss: 9.4789 - val_loss: 9.3196\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 35s 142ms/step - loss: 9.5062 - val_loss: 9.3567\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.4918 - val_loss: 9.5382\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.4873 - val_loss: 9.4801\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.4578 - val_loss: 9.3858\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.4670 - val_loss: 9.3420\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.4601 - val_loss: 9.2505\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.4695 - val_loss: 9.3017\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.4663 - val_loss: 9.2653\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.4574 - val_loss: 9.1973\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 35s 142ms/step - loss: 9.4068 - val_loss: 9.3713\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.4310 - val_loss: 10.0300\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.4232 - val_loss: 9.5484\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.4589 - val_loss: 9.3856\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.4282 - val_loss: 9.3635\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 9.4232 - val_loss: 9.3125\n",
      "\n",
      "Loss: 931.25%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 150)       1500      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 94, 94, 150)      600       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 18)        24318     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 92, 92, 18)       72        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 62)        10106     \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 90, 90, 62)       248       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 136)       76024     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 112,872\n",
      "Trainable params: 112,410\n",
      "Non-trainable params: 462\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 12s 43ms/step - loss: 1619.4808 - val_loss: 300.2036\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 25.0233 - val_loss: 11.7687\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.2286 - val_loss: 11.6299\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.1270 - val_loss: 10.1336\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0856 - val_loss: 9.9021\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.1128 - val_loss: 10.1078\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0872 - val_loss: 10.1313\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0609 - val_loss: 9.8493\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0931 - val_loss: 10.1237\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0592 - val_loss: 9.9068\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0467 - val_loss: 10.0305\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0471 - val_loss: 9.9083\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0438 - val_loss: 9.7724\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0836 - val_loss: 9.9616\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0618 - val_loss: 10.2343\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0556 - val_loss: 9.7851\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0388 - val_loss: 9.9688\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0646 - val_loss: 9.7949\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0699 - val_loss: 9.8241\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0633 - val_loss: 10.0278\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0389 - val_loss: 9.8337\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0435 - val_loss: 9.8367\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0545 - val_loss: 9.7780\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0646 - val_loss: 9.8217\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0412 - val_loss: 9.8278\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0549 - val_loss: 9.7653\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0467 - val_loss: 9.7418\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0549 - val_loss: 9.9492\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0641 - val_loss: 9.7928\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0547 - val_loss: 10.0740\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0528 - val_loss: 9.7855\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0482 - val_loss: 9.8221\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0619 - val_loss: 9.8019\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0445 - val_loss: 9.7540\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0556 - val_loss: 9.8332\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0225 - val_loss: 9.7642\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0299 - val_loss: 9.8112\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0400 - val_loss: 9.7849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0223 - val_loss: 9.9099\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0496 - val_loss: 9.9383\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0223 - val_loss: 9.8080\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0366 - val_loss: 9.9717\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0364 - val_loss: 9.7350\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0408 - val_loss: 9.7737\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0597 - val_loss: 9.8244\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0405 - val_loss: 9.8720\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0257 - val_loss: 9.9062\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0546 - val_loss: 9.8272\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0279 - val_loss: 9.7532\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0099 - val_loss: 9.8220\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0442 - val_loss: 9.7814\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0130 - val_loss: 9.7868\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0635 - val_loss: 9.7932\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0305 - val_loss: 9.7370\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0283 - val_loss: 9.8614\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0278 - val_loss: 9.8437\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0503 - val_loss: 9.7616\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0357 - val_loss: 9.7127\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0210 - val_loss: 9.7813\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0226 - val_loss: 9.7802\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0253 - val_loss: 9.9094\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0126 - val_loss: 9.9186\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0381 - val_loss: 9.8557\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0269 - val_loss: 9.7378\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0213 - val_loss: 9.8332\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0096 - val_loss: 9.7192\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0182 - val_loss: 9.7373\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0258 - val_loss: 9.8535\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0140 - val_loss: 9.7238\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0159 - val_loss: 9.7828\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 9.9995 - val_loss: 9.7313\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0101 - val_loss: 9.7055\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0077 - val_loss: 9.8388\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0378 - val_loss: 9.8083\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0107 - val_loss: 9.9783\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0329 - val_loss: 9.8218\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 9.9936 - val_loss: 9.7283\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0000 - val_loss: 9.7738\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0175 - val_loss: 9.7908\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0093 - val_loss: 9.7920\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0125 - val_loss: 9.8216\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0015 - val_loss: 9.7741\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0159 - val_loss: 9.7374\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0048 - val_loss: 9.8386\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0238 - val_loss: 9.7881\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0204 - val_loss: 9.8272\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0195 - val_loss: 9.7247\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 9.9991 - val_loss: 9.7522\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0094 - val_loss: 9.9338\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0094 - val_loss: 9.8154\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0936 - val_loss: 9.8890\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0173 - val_loss: 9.9261\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0393 - val_loss: 9.8298\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0192 - val_loss: 9.7903\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0192 - val_loss: 9.7645\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0321 - val_loss: 9.7222\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0103 - val_loss: 9.7172\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 9.9993 - val_loss: 9.7727\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0287 - val_loss: 9.8628\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0152 - val_loss: 9.7107\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0398 - val_loss: 9.7041\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0296 - val_loss: 9.9703\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0187 - val_loss: 9.9058\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0151 - val_loss: 9.7788\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0097 - val_loss: 9.7800\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0219 - val_loss: 9.7595\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0168 - val_loss: 9.7223\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0231 - val_loss: 9.8294\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0083 - val_loss: 9.8436\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0091 - val_loss: 9.7769\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0144 - val_loss: 9.8228\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0186 - val_loss: 9.7557\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0187 - val_loss: 9.7053\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 9.9974 - val_loss: 9.8717\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0211 - val_loss: 9.7944\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0263 - val_loss: 9.7204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0174 - val_loss: 9.8639\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0123 - val_loss: 9.9697\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0193 - val_loss: 9.7550\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0031 - val_loss: 9.7936\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0174 - val_loss: 9.8004\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0058 - val_loss: 9.7477\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0249 - val_loss: 9.8339\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0135 - val_loss: 9.7980\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0154 - val_loss: 9.7559\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0178 - val_loss: 9.7233\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 10.0176 - val_loss: 9.8412\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 9.9972 - val_loss: 9.7051\n",
      "\n",
      "Loss: 970.51%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 8)         80        \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 8)         584       \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 92, 92, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 92, 92, 8)         0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 256)       18688     \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 88, 88, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 8)         18440     \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 86, 86, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 84, 84, 8)         584       \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 84, 84, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 82, 82, 136)       9928      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 639,508\n",
      "Trainable params: 638,946\n",
      "Non-trainable params: 562\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 19s 72ms/step - loss: 2736.4231 - val_loss: 1657.1832\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 1204.1191 - val_loss: 404.0877\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 219.6017 - val_loss: 58.2551\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 33.9073 - val_loss: 15.5588\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 12.6916 - val_loss: 10.5904\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 10.2711 - val_loss: 9.9943\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.9694 - val_loss: 9.7573\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.9342 - val_loss: 9.8068\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.9274 - val_loss: 9.8118\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.9265 - val_loss: 9.8019\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.9269 - val_loss: 9.8970\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.9237 - val_loss: 10.0170\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.9256 - val_loss: 9.8504\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.9222 - val_loss: 9.7979\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.9300 - val_loss: 9.7036\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.9197 - val_loss: 9.8186\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.9244 - val_loss: 9.9587\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.9282 - val_loss: 9.8525\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.9209 - val_loss: 9.7670\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.9298 - val_loss: 9.8845\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.9228 - val_loss: 9.8433\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.9321 - val_loss: 9.8163\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.9353 - val_loss: 9.8220\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.9240 - val_loss: 9.9046\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.9276 - val_loss: 9.8323\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.9404 - val_loss: 9.7271\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.9306 - val_loss: 9.7092\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.9306 - val_loss: 9.7771\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.9322 - val_loss: 9.7499\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.9235 - val_loss: 10.0266\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.9394 - val_loss: 9.8988\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.9345 - val_loss: 9.6658\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.9340 - val_loss: 9.7343\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.9214 - val_loss: 9.8580\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.9362 - val_loss: 9.7355\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.9353 - val_loss: 9.7178\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.9325 - val_loss: 9.8464\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.9350 - val_loss: 9.7192\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.9292 - val_loss: 9.6992\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.9368 - val_loss: 9.6675\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.9222 - val_loss: 9.7357\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.9187 - val_loss: 9.7652\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.9387 - val_loss: 9.8585\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.9286 - val_loss: 9.6891\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.9291 - val_loss: 9.7857\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.9337 - val_loss: 10.0295\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.9182 - val_loss: 9.8074\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.9321 - val_loss: 9.6820\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.9335 - val_loss: 9.8216\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.9330 - val_loss: 9.7001\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.9261 - val_loss: 9.8369\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.9207 - val_loss: 9.9019\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.9158 - val_loss: 9.9424\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.9155 - val_loss: 9.6558\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.9204 - val_loss: 9.8408\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.9099 - val_loss: 9.7147\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.9202 - val_loss: 9.7380\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.9161 - val_loss: 9.6861\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.9132 - val_loss: 9.7327\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.9138 - val_loss: 9.7200\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.9123 - val_loss: 9.7337\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.9102 - val_loss: 9.7527\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.9106 - val_loss: 9.7937\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.9109 - val_loss: 9.7431\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.9181 - val_loss: 9.6674\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.9054 - val_loss: 9.8451\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.9097 - val_loss: 9.7016\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.9022 - val_loss: 9.7569\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.9212 - val_loss: 9.6549\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.9073 - val_loss: 9.6931\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.8974 - val_loss: 9.8437\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.9016 - val_loss: 9.6845\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.9050 - val_loss: 9.7578\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.8985 - val_loss: 9.6934\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.8936 - val_loss: 9.8408\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.9126 - val_loss: 9.7031\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.8923 - val_loss: 9.6898\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.8867 - val_loss: 9.7235\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.8898 - val_loss: 9.6539\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.8993 - val_loss: 9.6938\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.8963 - val_loss: 9.7654\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.9057 - val_loss: 9.7392\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.9023 - val_loss: 9.7543\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.8876 - val_loss: 9.8750\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.9029 - val_loss: 9.6701\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.8774 - val_loss: 9.6679\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.8890 - val_loss: 9.6750\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.8946 - val_loss: 9.7710\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.8862 - val_loss: 9.6826\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.8822 - val_loss: 9.6491\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.8857 - val_loss: 9.7577\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.8889 - val_loss: 9.8048\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.8810 - val_loss: 9.6878\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.8789 - val_loss: 9.7232\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.8798 - val_loss: 9.7133\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.8821 - val_loss: 9.7023\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.8845 - val_loss: 9.6762\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.8751 - val_loss: 9.7199\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.8776 - val_loss: 9.7314\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.8816 - val_loss: 9.7259\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.8738 - val_loss: 9.6462\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.8771 - val_loss: 9.6596\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.8740 - val_loss: 9.6667\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.8698 - val_loss: 9.6694\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.8683 - val_loss: 9.6089\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.8706 - val_loss: 9.6631\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.8623 - val_loss: 9.6974\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.8574 - val_loss: 9.6744\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.8709 - val_loss: 9.5858\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.8597 - val_loss: 9.7760\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.8704 - val_loss: 9.6325\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.8537 - val_loss: 9.6753\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.8595 - val_loss: 9.6409\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.8500 - val_loss: 9.6160\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.8440 - val_loss: 9.7631\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.8565 - val_loss: 9.6329\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.8504 - val_loss: 9.7244\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.8567 - val_loss: 9.7665\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.8531 - val_loss: 9.7570\n",
      "Epoch 120/128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 18s 72ms/step - loss: 9.8480 - val_loss: 9.6542\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.8490 - val_loss: 9.6425\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.8453 - val_loss: 9.7313\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.8446 - val_loss: 9.7693\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.8356 - val_loss: 9.7200\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.8349 - val_loss: 9.7417\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.8418 - val_loss: 9.7210\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.8227 - val_loss: 9.6526\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 9.8360 - val_loss: 9.5610\n",
      "\n",
      "Loss: 956.10%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 256)       2560      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 92, 92, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 92, 92, 256)       0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 8)         18440     \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 8)         584       \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 88, 88, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 8)         584       \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 86, 86, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 84, 84, 81)        5913      \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 84, 84, 81)       324       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 82, 82, 136)       99280     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 718,857\n",
      "Trainable params: 718,149\n",
      "Non-trainable params: 708\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 26s 98ms/step - loss: 1457.2036 - val_loss: 76.1776\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 15.5971 - val_loss: 12.9954\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.2970 - val_loss: 10.6536\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.1400 - val_loss: 10.7296\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.1260 - val_loss: 9.7875\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.1006 - val_loss: 10.2469\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0951 - val_loss: 9.9592\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0808 - val_loss: 9.9448\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0623 - val_loss: 9.9580\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0523 - val_loss: 10.5300\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.1032 - val_loss: 10.2494\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0488 - val_loss: 10.0005\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0650 - val_loss: 9.9400\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0737 - val_loss: 10.0246\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0446 - val_loss: 9.8710\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0647 - val_loss: 9.9399\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0628 - val_loss: 9.7507\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0394 - val_loss: 9.8777\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0487 - val_loss: 10.1844\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0858 - val_loss: 9.9280\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0805 - val_loss: 9.9832\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0751 - val_loss: 9.8086\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0693 - val_loss: 10.2151\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0890 - val_loss: 9.8015\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0370 - val_loss: 9.8437\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0772 - val_loss: 9.8290\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0724 - val_loss: 9.8453\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0588 - val_loss: 9.9609\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0517 - val_loss: 9.7403\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0474 - val_loss: 9.7426\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0565 - val_loss: 10.0225\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0397 - val_loss: 9.8038\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0606 - val_loss: 9.9894\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0569 - val_loss: 9.7660\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0554 - val_loss: 9.8944\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0793 - val_loss: 9.7509\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0589 - val_loss: 10.0421\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0432 - val_loss: 9.9321\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0587 - val_loss: 9.7393\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0635 - val_loss: 9.8100\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0651 - val_loss: 9.8909\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0523 - val_loss: 9.7451\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0513 - val_loss: 9.8671\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0454 - val_loss: 10.1532\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0534 - val_loss: 9.8049\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0531 - val_loss: 10.0043\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0430 - val_loss: 9.8854\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0515 - val_loss: 9.9081\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0692 - val_loss: 9.7694\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0369 - val_loss: 9.8468\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0454 - val_loss: 9.7197\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0511 - val_loss: 9.8893\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0586 - val_loss: 9.8329\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0624 - val_loss: 9.7631\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0555 - val_loss: 9.7369\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0549 - val_loss: 9.8257\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0468 - val_loss: 9.7931\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0381 - val_loss: 9.7638\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0264 - val_loss: 9.7365\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0238 - val_loss: 9.7754\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0434 - val_loss: 9.7577\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0344 - val_loss: 9.9600\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0467 - val_loss: 9.7676\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0533 - val_loss: 9.7262\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0233 - val_loss: 9.7585\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0249 - val_loss: 9.8972\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0318 - val_loss: 9.7497\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0377 - val_loss: 9.7183\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0465 - val_loss: 9.9517\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0270 - val_loss: 9.8195\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0487 - val_loss: 9.7699\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0550 - val_loss: 9.7475\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0369 - val_loss: 9.8190\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0465 - val_loss: 9.7665\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0381 - val_loss: 10.1928\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0275 - val_loss: 9.7997\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0198 - val_loss: 9.7625\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0436 - val_loss: 9.7933\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0486 - val_loss: 9.8342\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0177 - val_loss: 9.7843\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0185 - val_loss: 9.7952\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0217 - val_loss: 9.8685\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0221 - val_loss: 9.7871\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0097 - val_loss: 9.8534\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0123 - val_loss: 9.7614\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0411 - val_loss: 9.7208\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0193 - val_loss: 9.7745\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0295 - val_loss: 9.8350\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 9.9995 - val_loss: 9.8839\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0362 - val_loss: 9.7644\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0182 - val_loss: 9.8903\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0320 - val_loss: 9.7692\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0071 - val_loss: 9.9683\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0350 - val_loss: 9.8291\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0136 - val_loss: 9.7938\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0259 - val_loss: 9.8181\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0137 - val_loss: 9.7991\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0182 - val_loss: 9.8258\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 9.9993 - val_loss: 9.9885\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0194 - val_loss: 9.8901\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0168 - val_loss: 9.7769\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 9.9952 - val_loss: 9.8356\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0034 - val_loss: 10.0714\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0009 - val_loss: 9.7855\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0181 - val_loss: 9.7692\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0143 - val_loss: 9.7859\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0164 - val_loss: 9.9433\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0036 - val_loss: 9.7781\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0299 - val_loss: 9.9296\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0049 - val_loss: 9.9325\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0153 - val_loss: 9.8788\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0100 - val_loss: 9.9677\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0228 - val_loss: 9.8467\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0102 - val_loss: 9.8054\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0008 - val_loss: 9.8292\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0060 - val_loss: 9.9077\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 9.9730 - val_loss: 9.9998\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0193 - val_loss: 9.8392\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 9.9910 - val_loss: 9.9108\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 9.9870 - val_loss: 9.7538\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 9.9959 - val_loss: 9.7778\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 9.9879 - val_loss: 9.8160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 123/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 9.9687 - val_loss: 9.9487\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 9.9751 - val_loss: 9.9979\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 10.0021 - val_loss: 9.8397\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 9.9795 - val_loss: 9.8014\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 9.9808 - val_loss: 10.0472\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 9.9557 - val_loss: 9.8877\n",
      "\n",
      "Loss: 988.77%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 256)       2560      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 92, 92, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 92, 92, 256)       0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 90, 90, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 90, 90, 256)       0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 256)       590080    \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 86, 86, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 84, 84, 136)       313480    \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 84, 84, 136)      544       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 82, 82, 136)       166600    \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 82, 82, 136)      544       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 80, 80, 136)       166600    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,013,724\n",
      "Trainable params: 3,011,642\n",
      "Non-trainable params: 2,082\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 67s 262ms/step - loss: 1196.1526 - val_loss: 79.3964\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 65s 258ms/step - loss: 11.0748 - val_loss: 12.3236\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 65s 258ms/step - loss: 10.5229 - val_loss: 11.8543\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 65s 258ms/step - loss: 10.4492 - val_loss: 17.1653\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 10.3580 - val_loss: 10.0973\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 65s 258ms/step - loss: 10.3409 - val_loss: 9.9866\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 65s 258ms/step - loss: 10.2343 - val_loss: 10.8684\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 65s 258ms/step - loss: 10.2231 - val_loss: 9.9816\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 65s 258ms/step - loss: 10.2025 - val_loss: 9.9507\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 65s 258ms/step - loss: 10.1770 - val_loss: 10.0264\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 65s 258ms/step - loss: 10.1535 - val_loss: 10.2524\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 65s 258ms/step - loss: 10.2013 - val_loss: 9.8739\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 65s 258ms/step - loss: 10.1575 - val_loss: 10.0280\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 65s 258ms/step - loss: 10.1620 - val_loss: 10.1013\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 65s 258ms/step - loss: 10.1443 - val_loss: 9.9162\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 65s 258ms/step - loss: 10.1689 - val_loss: 10.4837\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 10.1531 - val_loss: 10.6266\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 65s 258ms/step - loss: 10.1442 - val_loss: 10.3521\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 65s 258ms/step - loss: 10.1651 - val_loss: 9.9703\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 65s 258ms/step - loss: 10.1128 - val_loss: 10.3176\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 10.1308 - val_loss: 10.2015\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 65s 258ms/step - loss: 10.1460 - val_loss: 10.0595\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 65s 258ms/step - loss: 10.1136 - val_loss: 10.2944\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 65s 258ms/step - loss: 10.1396 - val_loss: 10.0047\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 65s 258ms/step - loss: 10.1295 - val_loss: 10.0642\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 65s 258ms/step - loss: 10.1402 - val_loss: 9.9027\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 65s 258ms/step - loss: 10.1322 - val_loss: 9.8975\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 65s 258ms/step - loss: 10.0972 - val_loss: 10.0925\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 65s 258ms/step - loss: 10.1076 - val_loss: 9.9129\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 65s 258ms/step - loss: 10.1183 - val_loss: 10.0678\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 10.1143 - val_loss: 9.8280\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 65s 258ms/step - loss: 10.1176 - val_loss: 10.0430\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 65s 258ms/step - loss: 10.0692 - val_loss: 10.1177\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 10.0897 - val_loss: 10.0746\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 65s 258ms/step - loss: 10.0821 - val_loss: 9.8256\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 65s 258ms/step - loss: 10.0457 - val_loss: 9.9627\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 65s 258ms/step - loss: 10.0553 - val_loss: 9.9545\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 65s 258ms/step - loss: 10.0258 - val_loss: 10.1554\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 65s 258ms/step - loss: 10.0280 - val_loss: 10.0271\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 10.0304 - val_loss: 10.9681\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 65s 258ms/step - loss: 10.0035 - val_loss: 9.8286\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 10.0282 - val_loss: 9.8417\n",
      "Epoch 43/128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 64s 258ms/step - loss: 10.0076 - val_loss: 9.8792\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 9.9991 - val_loss: 9.8844\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 9.9808 - val_loss: 9.7723\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 9.9825 - val_loss: 9.7464\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 9.9987 - val_loss: 10.6296\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 9.9794 - val_loss: 10.0117\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 9.9380 - val_loss: 9.7310\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 9.9394 - val_loss: 9.7404\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 9.9001 - val_loss: 9.6005\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 9.8794 - val_loss: 10.1365\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 9.9070 - val_loss: 9.7093\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 9.8625 - val_loss: 9.7948\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 9.8740 - val_loss: 9.5600\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 9.8437 - val_loss: 9.7007\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 9.7836 - val_loss: 9.5783\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 9.8155 - val_loss: 9.4609\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 9.8015 - val_loss: 9.7632\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 9.7913 - val_loss: 9.7280\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 9.7404 - val_loss: 9.4716\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 9.7246 - val_loss: 9.7344\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 9.7373 - val_loss: 9.6082\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 9.7313 - val_loss: 9.4876\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 9.6918 - val_loss: 10.1901\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 9.6725 - val_loss: 10.1203\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 9.6524 - val_loss: 9.5858\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 9.6683 - val_loss: 9.2871\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 9.6429 - val_loss: 9.4375\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 9.6333 - val_loss: 9.5199\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 9.6061 - val_loss: 9.2794\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 9.5799 - val_loss: 9.5452\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 9.5978 - val_loss: 9.8569\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 9.5615 - val_loss: 9.2738\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 9.5523 - val_loss: 9.2655\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 9.5202 - val_loss: 9.5577\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 9.5324 - val_loss: 9.4005\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 9.4977 - val_loss: 9.9010\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 9.5005 - val_loss: 9.3437\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 9.4748 - val_loss: 9.5015\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 9.4513 - val_loss: 9.1445\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 9.4447 - val_loss: 9.1282\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 9.4307 - val_loss: 9.6394\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 9.4194 - val_loss: 9.2427\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 9.3771 - val_loss: 9.1752\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 9.3994 - val_loss: 9.0152\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 9.3973 - val_loss: 9.0585\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 9.3667 - val_loss: 9.1267\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 9.3729 - val_loss: 9.3470\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 9.3157 - val_loss: 9.0352\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 9.3463 - val_loss: 9.2292\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 9.3134 - val_loss: 9.0848\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 9.2894 - val_loss: 8.9972\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 9.2731 - val_loss: 8.9267\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 9.2749 - val_loss: 9.0350\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 9.2350 - val_loss: 8.9542\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 9.2302 - val_loss: 9.0327\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 9.2304 - val_loss: 9.0979\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 9.2166 - val_loss: 9.0181\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 9.2041 - val_loss: 9.3742\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 9.1917 - val_loss: 8.9348\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 9.1930 - val_loss: 9.0008\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 9.1831 - val_loss: 8.8460\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 9.1628 - val_loss: 9.1116\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 9.1066 - val_loss: 9.5209\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 9.1229 - val_loss: 9.0452\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 9.0803 - val_loss: 9.0170\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 9.1292 - val_loss: 9.0779\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 9.1058 - val_loss: 8.8569\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 9.0984 - val_loss: 9.2847\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 9.0902 - val_loss: 8.7961\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 9.0719 - val_loss: 9.1478\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 9.0502 - val_loss: 9.2110\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 9.0484 - val_loss: 8.9280\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 8.9992 - val_loss: 9.0407\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 9.0260 - val_loss: 9.0279\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 9.0066 - val_loss: 8.9158\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 9.0025 - val_loss: 9.0592\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 8.9876 - val_loss: 9.2331\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 8.9465 - val_loss: 8.9017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 121/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 9.0026 - val_loss: 9.1620\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 8.9812 - val_loss: 9.2012\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 8.9721 - val_loss: 9.3509\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 8.9348 - val_loss: 8.8741\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 8.9547 - val_loss: 8.8562\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 8.9085 - val_loss: 9.0569\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 65s 258ms/step - loss: 8.9094 - val_loss: 8.7959\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 8.9432 - val_loss: 8.8227\n",
      "\n",
      "Loss: 882.27%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 256)       2560      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 92, 92, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 92, 92, 256)       0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 8)         18440     \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 8)         584       \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 88, 88, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 88, 88, 8)         0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 8)         584       \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 84, 84, 8)         584       \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 84, 84, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 82, 82, 8)         584       \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 82, 82, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 80, 80, 8)         584       \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 80, 80, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 78, 78, 136)       9928      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 625,084\n",
      "Trainable params: 624,506\n",
      "Non-trainable params: 578\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 23s 87ms/step - loss: 2734.0691 - val_loss: 2108.6963\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 21s 86ms/step - loss: 1201.7933 - val_loss: 568.6879\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 219.9248 - val_loss: 82.6477\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 34.2373 - val_loss: 18.7816\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 12.8888 - val_loss: 11.0572\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.4179 - val_loss: 9.9815\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.1044 - val_loss: 9.9185\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0379 - val_loss: 9.9432\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0214 - val_loss: 9.7605\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0082 - val_loss: 9.9157\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0115 - val_loss: 10.0177\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0060 - val_loss: 9.8924\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.9894 - val_loss: 9.7937\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.9965 - val_loss: 9.9322\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.9902 - val_loss: 9.9342\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.9875 - val_loss: 9.8134\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.9695 - val_loss: 9.8024\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.9751 - val_loss: 9.7472\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.9576 - val_loss: 9.7278\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.9614 - val_loss: 9.7622\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.9726 - val_loss: 9.7734\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.9606 - val_loss: 9.7098\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.9618 - val_loss: 9.7636\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.9779 - val_loss: 9.8408\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.9613 - val_loss: 9.8206\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.9628 - val_loss: 9.7581\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.9735 - val_loss: 9.7914\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.9623 - val_loss: 9.7838\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.9611 - val_loss: 9.7457\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.9579 - val_loss: 9.7113\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.9619 - val_loss: 9.7382\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.9524 - val_loss: 9.7029\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.9499 - val_loss: 9.7826\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.9501 - val_loss: 9.8606\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.9700 - val_loss: 9.7181\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.9521 - val_loss: 10.0099\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.9616 - val_loss: 9.7246\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.9428 - val_loss: 9.7856\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.9557 - val_loss: 9.6765\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.9534 - val_loss: 9.7525\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.9429 - val_loss: 9.7105\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.9474 - val_loss: 9.7957\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.9532 - val_loss: 9.6919\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.9478 - val_loss: 9.7844\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.9548 - val_loss: 9.7381\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.9519 - val_loss: 9.7144\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.9466 - val_loss: 9.6867\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.9261 - val_loss: 9.7800\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 21s 86ms/step - loss: 9.9457 - val_loss: 9.6770\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.9306 - val_loss: 9.7138\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.9355 - val_loss: 9.7316\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.9287 - val_loss: 9.7058\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.9236 - val_loss: 9.6626\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.9268 - val_loss: 9.6754\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.9249 - val_loss: 9.6616\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.9113 - val_loss: 9.6944\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.9294 - val_loss: 9.6476\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.9134 - val_loss: 9.6638\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.9089 - val_loss: 9.7457\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.8916 - val_loss: 9.7130\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.8985 - val_loss: 9.6328\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.9103 - val_loss: 9.6298\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.9071 - val_loss: 9.6955\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.8895 - val_loss: 9.6648\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.8645 - val_loss: 9.6811\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.8793 - val_loss: 9.6094\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.8662 - val_loss: 9.7180\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.8665 - val_loss: 9.6875\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.8540 - val_loss: 9.7148\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.8426 - val_loss: 9.5995\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.8369 - val_loss: 9.6726\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.8476 - val_loss: 9.7664\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.8296 - val_loss: 9.5774\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.8305 - val_loss: 9.6345\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 21s 86ms/step - loss: 9.8317 - val_loss: 9.5912\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.8375 - val_loss: 9.6182\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.8189 - val_loss: 9.8662\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.8067 - val_loss: 9.5780\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.8183 - val_loss: 9.6399\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.8005 - val_loss: 9.6484\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.8080 - val_loss: 9.6028\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.8027 - val_loss: 9.6109\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.8123 - val_loss: 9.6367\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.8053 - val_loss: 9.6706\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.7859 - val_loss: 9.5451\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.7853 - val_loss: 9.5958\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.7833 - val_loss: 9.6818\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.7777 - val_loss: 9.6142\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.7832 - val_loss: 9.5213\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.7805 - val_loss: 9.5745\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.7770 - val_loss: 9.5619\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.7832 - val_loss: 9.8213\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.7870 - val_loss: 9.6442\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.7652 - val_loss: 9.6099\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.7563 - val_loss: 9.5897\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.7623 - val_loss: 9.5401\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.7549 - val_loss: 9.5404\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.7626 - val_loss: 9.6738\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.7497 - val_loss: 9.5571\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.7386 - val_loss: 9.6276\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.7338 - val_loss: 9.5444\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.7440 - val_loss: 9.5667\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.7523 - val_loss: 9.5852\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.7536 - val_loss: 9.5879\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.7410 - val_loss: 9.5200\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.7302 - val_loss: 9.5414\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.7321 - val_loss: 9.5716\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.7280 - val_loss: 9.5332\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.7318 - val_loss: 9.5238\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.7146 - val_loss: 9.6242\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.7168 - val_loss: 9.4861\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.7219 - val_loss: 9.4934\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.7268 - val_loss: 9.5763\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.7148 - val_loss: 9.5848\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.7024 - val_loss: 9.4913\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.7172 - val_loss: 9.6292\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.7186 - val_loss: 9.5094\n",
      "Epoch 118/128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 21s 85ms/step - loss: 9.7167 - val_loss: 9.6044\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.6923 - val_loss: 9.5941\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.7171 - val_loss: 9.5974\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.6960 - val_loss: 9.4958\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.6958 - val_loss: 9.5495\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.6875 - val_loss: 9.5353\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.6895 - val_loss: 9.6168\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.6789 - val_loss: 9.5847\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.6904 - val_loss: 9.5261\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.6987 - val_loss: 9.5348\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 9.6794 - val_loss: 9.5447\n",
      "\n",
      "Loss: 954.47%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 217)       2170      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 217)       424018    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 92, 92, 217)      868       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 92, 92, 217)       0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 186)       363444    \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 186)       311550    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 88, 88, 186)      744       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 88, 88, 186)       0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 238)       398650    \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 84, 84, 238)       510034    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 84, 84, 238)      952       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 84, 84, 238)       0         \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 82, 82, 117)       250731    \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 80, 80, 117)       123318    \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 80, 80, 117)      468       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 78, 78, 77)        81158     \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 78, 78, 77)       308       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 76, 76, 136)       94384     \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 76, 76, 136)      544       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 74, 74, 136)       166600    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,729,945\n",
      "Trainable params: 2,728,001\n",
      "Non-trainable params: 1,944\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 76s 286ms/step - loss: 1195.4686 - val_loss: 38.1450\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 10.6093 - val_loss: 11.0055\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 10.0875 - val_loss: 10.1471\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 10.0680 - val_loss: 10.3310\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 10.0640 - val_loss: 9.7650\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 10.0518 - val_loss: 9.8578\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 10.0677 - val_loss: 9.9190\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 10.0684 - val_loss: 10.8355\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 10.0799 - val_loss: 10.0267\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 10.1255 - val_loss: 10.2832\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 10.1135 - val_loss: 9.8960\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 10.0923 - val_loss: 10.4596\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 10.0792 - val_loss: 10.1285\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 10.1081 - val_loss: 10.8972\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 10.1277 - val_loss: 10.7927\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 10.0929 - val_loss: 9.9968\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 10.0719 - val_loss: 10.3469\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 10.1202 - val_loss: 10.5131\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 10.0935 - val_loss: 9.9769\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 10.0763 - val_loss: 10.2676\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 10.0955 - val_loss: 10.5906\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 10.1231 - val_loss: 10.1416\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 10.1194 - val_loss: 10.8263\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 10.0893 - val_loss: 9.8779\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 10.0835 - val_loss: 9.9844\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 10.0498 - val_loss: 9.8107\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 10.0100 - val_loss: 10.2708\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 9.9680 - val_loss: 9.7476\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 9.9424 - val_loss: 9.6671\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 9.9151 - val_loss: 10.3063\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 9.8751 - val_loss: 18.4815\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 9.8166 - val_loss: 9.7806\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 9.8204 - val_loss: 9.8031\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 9.8122 - val_loss: 9.9616\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 9.7413 - val_loss: 10.0568\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 9.7132 - val_loss: 9.8020\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 9.6602 - val_loss: 9.7158\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 9.5920 - val_loss: 9.7204\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 9.5428 - val_loss: 9.2403\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 9.4442 - val_loss: 11.0534\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 9.2946 - val_loss: 9.7641\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 9.2489 - val_loss: 10.2361\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 9.1297 - val_loss: 9.3921\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 9.0520 - val_loss: 9.7916\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 8.9770 - val_loss: 9.0800\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 8.8738 - val_loss: 10.4684\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 8.8650 - val_loss: 20.5592\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 8.8021 - val_loss: 9.6335\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 8.7351 - val_loss: 10.4434\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 8.6855 - val_loss: 8.8487\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 8.6316 - val_loss: 8.7240\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 8.6291 - val_loss: 11.1329\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 8.5438 - val_loss: 21.8929\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 8.5361 - val_loss: 9.2311\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 8.4643 - val_loss: 12.9240\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 8.5050 - val_loss: 9.3764\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 8.3905 - val_loss: 8.1935\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 8.3991 - val_loss: 13.1422\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 8.3523 - val_loss: 8.1672\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 8.3293 - val_loss: 8.2247\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 8.2873 - val_loss: 15.6831\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 8.2532 - val_loss: 12.6631\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 8.2378 - val_loss: 205.6349\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 8.2166 - val_loss: 9.7212\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 8.1654 - val_loss: 9.8879\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 8.1296 - val_loss: 9.8915\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 8.1280 - val_loss: 15.1549\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 8.0770 - val_loss: 10.8986\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 8.0796 - val_loss: 9.0328\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 8.0670 - val_loss: 9.1337\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 7.9936 - val_loss: 9.7198\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 8.0052 - val_loss: 9.2587\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 7.9818 - val_loss: 9.5220\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 7.9514 - val_loss: 10.0803\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 7.8997 - val_loss: 9.0222\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 7.8788 - val_loss: 8.2033\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 7.9367 - val_loss: 9.0482\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 7.8564 - val_loss: 8.1394\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 7.8347 - val_loss: 8.3087\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 7.8328 - val_loss: 8.3004\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 7.7819 - val_loss: 8.3848\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 7.7652 - val_loss: 8.7304\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 7.7688 - val_loss: 8.5059\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 7.7398 - val_loss: 7.8901\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 7.7154 - val_loss: 8.4666\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 7.6973 - val_loss: 8.3062\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 7.6952 - val_loss: 9.3665\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 7.6461 - val_loss: 7.8539\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 7.6273 - val_loss: 8.2935\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 7.6100 - val_loss: 10.3519\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 7.6057 - val_loss: 8.7344\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 7.5990 - val_loss: 7.9037\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 7.5654 - val_loss: 8.0849\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 7.5901 - val_loss: 8.6978\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 7.5383 - val_loss: 8.2261\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 7.5460 - val_loss: 9.0129\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 7.5166 - val_loss: 8.1282\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 7.5286 - val_loss: 8.8140\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 7.4890 - val_loss: 8.8842\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 7.4604 - val_loss: 8.3259\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 7.5218 - val_loss: 8.3028\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 7.4042 - val_loss: 7.9261\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 70s 278ms/step - loss: 7.4207 - val_loss: 8.1983\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 69s 277ms/step - loss: 7.4223 - val_loss: 8.2412\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 7.4463 - val_loss: 8.0815\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 7.4080 - val_loss: 8.0946\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 7.3798 - val_loss: 8.6398\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 7.3904 - val_loss: 15.9812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 109/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 7.3283 - val_loss: 8.6879\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 7.2979 - val_loss: 8.6769\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 7.3606 - val_loss: 8.1140\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 7.3109 - val_loss: 8.5185\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 7.2632 - val_loss: 8.2258\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 7.2828 - val_loss: 8.2627\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 7.2399 - val_loss: 8.5286\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 7.2211 - val_loss: 8.0576\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 7.2686 - val_loss: 8.6352\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 7.2323 - val_loss: 8.0076\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 7.1954 - val_loss: 7.8457\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 7.2033 - val_loss: 7.7950\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 7.1912 - val_loss: 7.6925\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 7.1852 - val_loss: 7.8685\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 7.1824 - val_loss: 8.2311\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 7.1446 - val_loss: 8.5824\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 7.1076 - val_loss: 8.5458\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 7.1192 - val_loss: 8.0677\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 7.1803 - val_loss: 7.8375\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 7.1118 - val_loss: 8.1315\n",
      "\n",
      "Loss: 813.15%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 256)       2560      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 92, 92, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 92, 92, 256)       0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 256)       590080    \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 88, 88, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 88, 88, 256)       0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 8)         18440     \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 84, 84, 8)         584       \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 84, 84, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 82, 82, 136)       9928      \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 82, 82, 136)      544       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 80, 80, 136)       166600    \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 80, 80, 136)      544       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 78, 78, 136)       166600    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,138,124\n",
      "Trainable params: 2,136,538\n",
      "Non-trainable params: 1,586\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 52s 205ms/step - loss: 1191.2078 - val_loss: 34.2690\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 11.8266 - val_loss: 16.2409\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.9215 - val_loss: 28.2462\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.8476 - val_loss: 12.9573\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.3834 - val_loss: 15.0219\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.3535 - val_loss: 15.9866\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.2515 - val_loss: 10.6684\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.2820 - val_loss: 12.2980\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.1723 - val_loss: 10.2286\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.1499 - val_loss: 10.2611\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.1260 - val_loss: 10.1859\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.1541 - val_loss: 10.7302\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.1738 - val_loss: 10.9663\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.1795 - val_loss: 10.6687\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.1567 - val_loss: 10.1987\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.1717 - val_loss: 10.3527\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.1586 - val_loss: 14.1311\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.1871 - val_loss: 12.6606\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.1630 - val_loss: 10.8372\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.1302 - val_loss: 10.4130\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.1092 - val_loss: 10.0885\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.1775 - val_loss: 10.4880\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.1543 - val_loss: 11.8921\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.1367 - val_loss: 10.5322\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.1295 - val_loss: 10.8093\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.1446 - val_loss: 10.7394\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.0955 - val_loss: 10.5227\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.1566 - val_loss: 10.0246\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.1327 - val_loss: 12.6358\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.1293 - val_loss: 10.1503\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.1314 - val_loss: 9.8495\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.1072 - val_loss: 10.2340\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.1353 - val_loss: 10.5307\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.1178 - val_loss: 11.9261\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.1192 - val_loss: 9.9775\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.1141 - val_loss: 9.9035\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.0978 - val_loss: 10.0725\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.1598 - val_loss: 9.8232\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.0760 - val_loss: 9.9073\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.0966 - val_loss: 10.2331\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.1282 - val_loss: 10.3343\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.0990 - val_loss: 10.0704\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.0943 - val_loss: 10.0007\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.1176 - val_loss: 10.2175\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.1019 - val_loss: 10.0116\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.0991 - val_loss: 10.0888\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.1093 - val_loss: 9.8587\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.1108 - val_loss: 10.3897\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.1165 - val_loss: 9.9793\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.1028 - val_loss: 10.2875\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.0722 - val_loss: 10.3339\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.0759 - val_loss: 10.0287\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.0620 - val_loss: 10.0390\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.0881 - val_loss: 9.9122\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.0919 - val_loss: 9.8634\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.0893 - val_loss: 10.0480\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.0816 - val_loss: 9.8779\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.1152 - val_loss: 10.3147\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.0776 - val_loss: 9.9020\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.0605 - val_loss: 9.8688\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.0624 - val_loss: 10.0112\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.0426 - val_loss: 9.8335\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.0752 - val_loss: 9.9219\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.0594 - val_loss: 9.9792\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.0484 - val_loss: 10.3178\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.0572 - val_loss: 10.1109\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.0657 - val_loss: 9.9529\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.0518 - val_loss: 10.0162\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.0313 - val_loss: 9.9150\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.0282 - val_loss: 10.2284\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.0539 - val_loss: 10.4181\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.0400 - val_loss: 9.8022\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.0124 - val_loss: 10.5153\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.0612 - val_loss: 9.9282\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.0412 - val_loss: 9.7332\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.0160 - val_loss: 9.9202\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.0309 - val_loss: 9.8701\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.0287 - val_loss: 9.9432\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.0202 - val_loss: 9.8987\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.0493 - val_loss: 9.9367\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.0041 - val_loss: 10.0231\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.9743 - val_loss: 10.0967\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.0058 - val_loss: 10.3026\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.9810 - val_loss: 10.1098\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.0063 - val_loss: 9.9485\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.9879 - val_loss: 10.0595\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.0050 - val_loss: 10.0768\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.9852 - val_loss: 9.9988\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.9458 - val_loss: 10.1212\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.9896 - val_loss: 9.9492\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.9477 - val_loss: 10.0379\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.9758 - val_loss: 9.8772\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.9486 - val_loss: 9.9908\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.9647 - val_loss: 10.0477\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.9317 - val_loss: 10.2710\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.9131 - val_loss: 9.9931\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.9000 - val_loss: 10.0504\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.9106 - val_loss: 10.1077\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.9036 - val_loss: 10.1646\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.9194 - val_loss: 9.9699\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.9054 - val_loss: 10.0711\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.8712 - val_loss: 12.2607\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.8650 - val_loss: 9.9157\n",
      "Epoch 104/128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 51s 203ms/step - loss: 9.8568 - val_loss: 9.8276\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.8537 - val_loss: 10.1522\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.8443 - val_loss: 10.0316\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.8198 - val_loss: 9.9788\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.8312 - val_loss: 9.8782\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.8365 - val_loss: 9.8716\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.8226 - val_loss: 10.7733\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.8151 - val_loss: 10.3344\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.7848 - val_loss: 10.0925\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.7979 - val_loss: 10.4086\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.7847 - val_loss: 10.3559\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.7803 - val_loss: 10.0418\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.7643 - val_loss: 9.8814\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.7662 - val_loss: 9.9240\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.7884 - val_loss: 9.9825\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.7408 - val_loss: 10.3015\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.7477 - val_loss: 10.0417\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.7318 - val_loss: 10.0174\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 51s 202ms/step - loss: 9.7236 - val_loss: 9.8538\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.7166 - val_loss: 10.0385\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.7373 - val_loss: 9.8445\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.7294 - val_loss: 9.9173\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.6659 - val_loss: 9.9438\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.6701 - val_loss: 10.1949\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 51s 202ms/step - loss: 9.6789 - val_loss: 9.9940\n",
      "\n",
      "Loss: 999.40%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 176)       1760      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 94, 94, 176)      704       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 94, 94, 176)       0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 8)         12680     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 92, 92, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 92, 92, 8)         0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 205)       14965     \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 205)       378430    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 88, 88, 205)      820       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 88, 88, 205)       0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 256)       472576    \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 84, 84, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 84, 84, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 82, 82, 58)        133690    \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 82, 82, 58)       232       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 80, 80, 86)        44978     \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 80, 80, 86)       344       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 78, 78, 136)       105400    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,757,719\n",
      "Trainable params: 1,756,139\n",
      "Non-trainable params: 1,580\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 51s 192ms/step - loss: 1437.5874 - val_loss: 59.8776\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 14.2090 - val_loss: 10.7655\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.1820 - val_loss: 10.1747\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.1403 - val_loss: 9.8846\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.0926 - val_loss: 9.9250\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.0697 - val_loss: 9.9110\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.1120 - val_loss: 10.0182\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.0983 - val_loss: 9.8527\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.0982 - val_loss: 10.2127\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.0769 - val_loss: 9.9383\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.0254 - val_loss: 9.9037\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.0543 - val_loss: 9.8842\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.0672 - val_loss: 10.0595\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.0791 - val_loss: 10.0386\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.0565 - val_loss: 9.8224\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.0492 - val_loss: 9.8585\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.0507 - val_loss: 10.0795\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.0544 - val_loss: 9.8480\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.0619 - val_loss: 10.0614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.0670 - val_loss: 10.1200\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.0673 - val_loss: 9.8595\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.0692 - val_loss: 10.0154\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.0662 - val_loss: 10.1912\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.0594 - val_loss: 9.9350\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.0614 - val_loss: 9.8090\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.0578 - val_loss: 9.9605\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.0632 - val_loss: 9.8088\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.0569 - val_loss: 9.8759\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.0556 - val_loss: 9.8518\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.0809 - val_loss: 9.9119\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.0866 - val_loss: 9.8372\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.0526 - val_loss: 9.8606\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.0876 - val_loss: 9.9618\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.0638 - val_loss: 10.0312\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.0511 - val_loss: 9.8203\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.0650 - val_loss: 9.8830\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.0297 - val_loss: 9.9498\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.0419 - val_loss: 10.0534\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.0316 - val_loss: 10.0935\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.0453 - val_loss: 9.8568\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.0614 - val_loss: 9.9064\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.0441 - val_loss: 9.8214\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.0242 - val_loss: 10.2118\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.0640 - val_loss: 13.3918\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.0490 - val_loss: 9.8540\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.0284 - val_loss: 9.7797\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.0143 - val_loss: 9.7965\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.9998 - val_loss: 9.8653\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.0287 - val_loss: 9.8461\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.0233 - val_loss: 9.7154\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.0269 - val_loss: 9.9635\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.9987 - val_loss: 9.7600\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.0108 - val_loss: 9.8669\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.0254 - val_loss: 9.8291\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.9978 - val_loss: 9.9346\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.9729 - val_loss: 9.9779\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.9681 - val_loss: 10.0778\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 10.0179 - val_loss: 9.8359\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.9690 - val_loss: 9.7755\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.9776 - val_loss: 9.9984\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.9815 - val_loss: 9.6500\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.9331 - val_loss: 9.7751\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.9268 - val_loss: 9.7368\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.9219 - val_loss: 9.9585\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.9127 - val_loss: 9.7501\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.9017 - val_loss: 9.6327\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.8945 - val_loss: 9.9562\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.8675 - val_loss: 10.0080\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.8630 - val_loss: 9.9746\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.8771 - val_loss: 9.5636\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.8644 - val_loss: 9.6384\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.7999 - val_loss: 9.6183\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.7811 - val_loss: 9.8935\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.7532 - val_loss: 9.5897\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.7553 - val_loss: 9.6633\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.7329 - val_loss: 9.5839\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.7224 - val_loss: 10.0863\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.6815 - val_loss: 9.7958\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.6965 - val_loss: 9.4659\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.6819 - val_loss: 9.5173\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.6218 - val_loss: 10.4512\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.6079 - val_loss: 9.6667\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.6155 - val_loss: 9.9497\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.6094 - val_loss: 9.3713\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.5625 - val_loss: 9.7073\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.5353 - val_loss: 9.9272\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.5480 - val_loss: 9.5483\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.4905 - val_loss: 9.9228\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.4965 - val_loss: 9.7686\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.4425 - val_loss: 9.5582\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.4459 - val_loss: 9.6570\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.4550 - val_loss: 9.4893\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.4140 - val_loss: 9.6137\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.3924 - val_loss: 9.5855\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.3767 - val_loss: 9.4642\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.3598 - val_loss: 9.3765\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.3672 - val_loss: 9.3572\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.3565 - val_loss: 9.6827\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.3498 - val_loss: 9.3671\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.3048 - val_loss: 9.6315\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.2998 - val_loss: 9.5989\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.2897 - val_loss: 9.9396\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.2612 - val_loss: 9.3992\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.2470 - val_loss: 9.5256\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.2314 - val_loss: 9.2935\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.2479 - val_loss: 9.4579\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.2332 - val_loss: 9.4673\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.2024 - val_loss: 9.6438\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.2044 - val_loss: 9.9772\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.2162 - val_loss: 9.3006\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.1999 - val_loss: 9.2329\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.1777 - val_loss: 9.3384\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.1777 - val_loss: 9.2809\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.1475 - val_loss: 9.1604\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.1279 - val_loss: 9.3613\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.1318 - val_loss: 9.4406\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.1141 - val_loss: 9.3300\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.1030 - val_loss: 9.2742\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.0971 - val_loss: 9.7281\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.1135 - val_loss: 9.4036\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.0849 - val_loss: 9.6970\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.0625 - val_loss: 9.5070\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.0575 - val_loss: 9.4413\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.0494 - val_loss: 9.7521\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.0669 - val_loss: 9.5017\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.0422 - val_loss: 9.7654\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.0626 - val_loss: 9.6091\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 8.9969 - val_loss: 9.5496\n",
      "\n",
      "Loss: 954.96%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 166)       1660      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 166)       248170    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 92, 92, 166)      664       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 92, 92, 166)       0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 256)       382720    \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 88, 88, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 88, 88, 256)       0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 100)       230500    \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 84, 84, 100)       90100     \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 84, 84, 100)      400       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 82, 82, 46)        41446     \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 82, 82, 46)       184       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 80, 80, 136)       56440     \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 80, 80, 136)      544       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 78, 78, 136)       166600    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,810,536\n",
      "Trainable params: 1,809,126\n",
      "Non-trainable params: 1,410\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 52s 198ms/step - loss: 1198.2936 - val_loss: 312.9501\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 48s 194ms/step - loss: 11.6163 - val_loss: 47.1506\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 48s 194ms/step - loss: 10.7690 - val_loss: 31.7508\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 10.4749 - val_loss: 18.3079\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 48s 194ms/step - loss: 10.4713 - val_loss: 22.5192\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 48s 194ms/step - loss: 10.4463 - val_loss: 14.7220\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 10.2819 - val_loss: 11.9870\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 48s 194ms/step - loss: 10.2277 - val_loss: 13.3057\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 10.2025 - val_loss: 11.7503\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 10.2433 - val_loss: 11.3604\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 10.2023 - val_loss: 11.7974\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 10.2394 - val_loss: 13.3454\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 10.1858 - val_loss: 10.0931\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 10.1656 - val_loss: 12.4552\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 48s 194ms/step - loss: 10.2004 - val_loss: 10.8032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 10.1535 - val_loss: 11.1333\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 10.1889 - val_loss: 10.7529\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 10.1749 - val_loss: 12.8324\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 10.1793 - val_loss: 10.8570\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 48s 194ms/step - loss: 10.1763 - val_loss: 11.6323\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 48s 194ms/step - loss: 10.1733 - val_loss: 11.6592\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 10.1818 - val_loss: 10.6961\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 10.1690 - val_loss: 11.5731\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 10.1618 - val_loss: 11.2187\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 48s 194ms/step - loss: 10.1723 - val_loss: 11.5353\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 48s 194ms/step - loss: 10.1909 - val_loss: 10.2961\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 48s 194ms/step - loss: 10.1612 - val_loss: 14.0811\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 48s 194ms/step - loss: 10.1637 - val_loss: 10.9303\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 10.1747 - val_loss: 11.8246\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 10.1878 - val_loss: 10.9497\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 10.2127 - val_loss: 11.9998\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 10.1813 - val_loss: 14.3342\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 10.1530 - val_loss: 9.8586\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 10.1040 - val_loss: 10.1495\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 48s 194ms/step - loss: 10.1330 - val_loss: 10.7192\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 10.1191 - val_loss: 10.6546\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 10.1367 - val_loss: 12.1852\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 10.1112 - val_loss: 11.2646\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 48s 194ms/step - loss: 10.0881 - val_loss: 10.2897\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 48s 194ms/step - loss: 10.1415 - val_loss: 11.2946\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 10.1191 - val_loss: 11.0658\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 10.1376 - val_loss: 10.8303\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 10.1005 - val_loss: 9.9488\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 10.0895 - val_loss: 10.8993\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 10.0965 - val_loss: 10.9496\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 10.0747 - val_loss: 11.3646\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 10.1021 - val_loss: 10.0336\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 10.0837 - val_loss: 10.1649\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 10.0770 - val_loss: 10.2146\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 10.0529 - val_loss: 10.3719\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 10.1096 - val_loss: 9.7495\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 10.0813 - val_loss: 10.2683\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 48s 194ms/step - loss: 10.0715 - val_loss: 11.4637\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 10.0579 - val_loss: 10.4488\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 10.0497 - val_loss: 10.5532\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 10.0406 - val_loss: 10.9704\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 10.0245 - val_loss: 10.2721\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 10.0274 - val_loss: 10.2283\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 48s 194ms/step - loss: 10.0158 - val_loss: 10.6651\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 9.9911 - val_loss: 9.9919\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 9.9929 - val_loss: 10.4985\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 9.9720 - val_loss: 10.1235\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 9.9909 - val_loss: 10.4461\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 9.9672 - val_loss: 9.8957\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 9.9468 - val_loss: 10.2321\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 9.9510 - val_loss: 9.7424\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 9.9215 - val_loss: 9.9905\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 9.9343 - val_loss: 10.5279\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 9.8913 - val_loss: 10.1911\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 9.8860 - val_loss: 10.5472\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 9.8483 - val_loss: 9.8468\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 9.8392 - val_loss: 9.7372\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 9.8147 - val_loss: 10.6939\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 9.8371 - val_loss: 9.8682\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 9.8228 - val_loss: 10.1560\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 9.7730 - val_loss: 9.9628\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 9.7600 - val_loss: 9.6776\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 9.7083 - val_loss: 10.0321\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 9.6797 - val_loss: 9.4278\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 48s 194ms/step - loss: 9.6878 - val_loss: 9.6754\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 9.6403 - val_loss: 10.0504\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 9.6164 - val_loss: 9.7950\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 9.5847 - val_loss: 10.1543\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 9.6004 - val_loss: 9.9816\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 9.5252 - val_loss: 9.6150\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 9.4580 - val_loss: 9.2812\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 9.4286 - val_loss: 10.4701\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 9.4378 - val_loss: 9.8305\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 9.3933 - val_loss: 9.5268\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 9.3750 - val_loss: 9.4885\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 9.3460 - val_loss: 9.8340\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 48s 194ms/step - loss: 9.3176 - val_loss: 9.6626\n",
      "Epoch 93/128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 48s 193ms/step - loss: 9.3362 - val_loss: 9.2471\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 9.2978 - val_loss: 9.3726\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 48s 194ms/step - loss: 9.2759 - val_loss: 9.6505\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 9.2679 - val_loss: 9.4141\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 10.3312 - val_loss: 9.6916\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 9.6726 - val_loss: 9.4747\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 9.5487 - val_loss: 9.4130\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 9.4376 - val_loss: 9.7163\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 9.3798 - val_loss: 9.3868\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 9.3404 - val_loss: 9.2673\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 9.3010 - val_loss: 9.0742\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 9.2703 - val_loss: 9.2275\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 9.2422 - val_loss: 9.1737\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 9.2686 - val_loss: 9.6363\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 9.2444 - val_loss: 9.0498\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 9.1925 - val_loss: 9.1112\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 9.1809 - val_loss: 9.0274\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 9.2150 - val_loss: 8.9204\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 9.2009 - val_loss: 8.9660\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 9.1952 - val_loss: 9.1173\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 9.1386 - val_loss: 9.4910\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 9.1265 - val_loss: 9.0764\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 9.0854 - val_loss: 9.1855\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 9.0951 - val_loss: 9.3841\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 9.0560 - val_loss: 9.0348\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 9.0694 - val_loss: 9.2382\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 9.2657 - val_loss: 9.0986\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 9.1508 - val_loss: 9.0551\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 9.0750 - val_loss: 8.9123\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 9.0332 - val_loss: 9.2644\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 9.0503 - val_loss: 9.2314\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 9.0660 - val_loss: 9.1485\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 9.0303 - val_loss: 8.9771\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 48s 194ms/step - loss: 8.9960 - val_loss: 8.9840\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 48s 194ms/step - loss: 9.0073 - val_loss: 9.1643\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 9.0024 - val_loss: 8.8084\n",
      "\n",
      "Loss: 880.84%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 179)       1790      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 94, 94, 179)      716       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 94, 94, 179)       0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 160)       257920    \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 160)       230560    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 90, 90, 160)      640       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 90, 90, 160)       0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 256)       368896    \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 86, 86, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 86, 86, 256)       0         \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 84, 84, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 84, 84, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 82, 82, 69)        159045    \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 82, 82, 69)       276       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 80, 80, 136)       84592     \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 80, 80, 136)      544       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 78, 78, 136)       166600    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,453,791\n",
      "Trainable params: 2,451,677\n",
      "Non-trainable params: 2,114\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 66s 250ms/step - loss: 1192.7933 - val_loss: 39.8242\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 61s 243ms/step - loss: 11.2033 - val_loss: 12.7487\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 61s 243ms/step - loss: 10.6570 - val_loss: 17.5333\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 61s 243ms/step - loss: 10.3823 - val_loss: 10.2803\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 61s 243ms/step - loss: 10.3101 - val_loss: 12.3285\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 61s 243ms/step - loss: 10.2466 - val_loss: 10.0276\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 61s 243ms/step - loss: 10.2113 - val_loss: 10.2276\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 61s 243ms/step - loss: 10.2871 - val_loss: 9.8682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/128\n",
      "250/250 [==============================] - 61s 243ms/step - loss: 10.1887 - val_loss: 9.8702\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 61s 243ms/step - loss: 10.0893 - val_loss: 10.1988\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 61s 243ms/step - loss: 10.1306 - val_loss: 10.2234\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 61s 243ms/step - loss: 10.1636 - val_loss: 10.0904\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 61s 243ms/step - loss: 10.1387 - val_loss: 10.2312\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 61s 243ms/step - loss: 10.1501 - val_loss: 10.3809\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 61s 243ms/step - loss: 10.1607 - val_loss: 9.9779\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 61s 243ms/step - loss: 10.1144 - val_loss: 9.9050\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 61s 243ms/step - loss: 10.1237 - val_loss: 10.1449\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 61s 243ms/step - loss: 10.1390 - val_loss: 10.3367\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 61s 243ms/step - loss: 10.1410 - val_loss: 10.5441\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 61s 243ms/step - loss: 10.1603 - val_loss: 9.9149\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 61s 243ms/step - loss: 10.1396 - val_loss: 9.9422\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 61s 243ms/step - loss: 10.1528 - val_loss: 9.8966\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 61s 243ms/step - loss: 10.0920 - val_loss: 10.2705\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 61s 243ms/step - loss: 10.1201 - val_loss: 10.7430\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 61s 243ms/step - loss: 10.0974 - val_loss: 10.1846\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 61s 243ms/step - loss: 10.1097 - val_loss: 9.9754\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 61s 243ms/step - loss: 10.1013 - val_loss: 10.6483\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 61s 243ms/step - loss: 10.0596 - val_loss: 9.9782\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 61s 243ms/step - loss: 10.1492 - val_loss: 12.3630\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 61s 243ms/step - loss: 10.1408 - val_loss: 10.0944\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 61s 243ms/step - loss: 10.0891 - val_loss: 9.8519\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 61s 243ms/step - loss: 10.0780 - val_loss: 9.9852\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 61s 243ms/step - loss: 10.0623 - val_loss: 9.7651\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 61s 243ms/step - loss: 10.0203 - val_loss: 9.8511\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 61s 243ms/step - loss: 10.0193 - val_loss: 9.7896\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 61s 243ms/step - loss: 10.0225 - val_loss: 9.9181\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 61s 243ms/step - loss: 10.0012 - val_loss: 9.7537\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 61s 243ms/step - loss: 9.9704 - val_loss: 9.9692\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 61s 243ms/step - loss: 9.9645 - val_loss: 10.1190\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 61s 243ms/step - loss: 9.9429 - val_loss: 9.8203\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 61s 243ms/step - loss: 9.9475 - val_loss: 10.5877\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 61s 243ms/step - loss: 9.8944 - val_loss: 9.8329\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 61s 243ms/step - loss: 9.9020 - val_loss: 10.1580\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 61s 243ms/step - loss: 9.8909 - val_loss: 9.5761\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 61s 243ms/step - loss: 9.8247 - val_loss: 9.6471\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 61s 243ms/step - loss: 9.8503 - val_loss: 9.8290\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 61s 243ms/step - loss: 9.8041 - val_loss: 9.5935\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 61s 243ms/step - loss: 9.7734 - val_loss: 9.6258\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 61s 243ms/step - loss: 9.7681 - val_loss: 9.8887\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 61s 243ms/step - loss: 9.7336 - val_loss: 9.4679\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 61s 243ms/step - loss: 9.6852 - val_loss: 9.3998\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 61s 243ms/step - loss: 9.6565 - val_loss: 9.4469\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 61s 243ms/step - loss: 9.5870 - val_loss: 9.4467\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 61s 243ms/step - loss: 9.5643 - val_loss: 9.2689\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 61s 243ms/step - loss: 9.5427 - val_loss: 9.4173\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 61s 243ms/step - loss: 9.4982 - val_loss: 9.5131\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 61s 243ms/step - loss: 9.4532 - val_loss: 9.8215\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 61s 243ms/step - loss: 9.4449 - val_loss: 9.1232\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 61s 243ms/step - loss: 9.3949 - val_loss: 9.2209\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 61s 243ms/step - loss: 9.3828 - val_loss: 9.1760\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 61s 243ms/step - loss: 9.3118 - val_loss: 9.3096\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 61s 242ms/step - loss: 9.2984 - val_loss: 9.0061\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 61s 243ms/step - loss: 9.2825 - val_loss: 9.2647\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 61s 243ms/step - loss: 9.2317 - val_loss: 9.0788\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 61s 243ms/step - loss: 9.1701 - val_loss: 9.1640\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 61s 242ms/step - loss: 9.1564 - val_loss: 9.1289\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 61s 242ms/step - loss: 9.1611 - val_loss: 9.0460\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 61s 243ms/step - loss: 9.1123 - val_loss: 8.8701\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 61s 242ms/step - loss: 9.0867 - val_loss: 8.9479\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 61s 243ms/step - loss: 9.0704 - val_loss: 8.7765\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 61s 242ms/step - loss: 9.0430 - val_loss: 8.9123\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 61s 243ms/step - loss: 9.0393 - val_loss: 8.7701\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 61s 242ms/step - loss: 9.0132 - val_loss: 8.8327\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 61s 242ms/step - loss: 8.9755 - val_loss: 8.7915\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 61s 242ms/step - loss: 8.9567 - val_loss: 8.7104\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 61s 242ms/step - loss: 8.9584 - val_loss: 8.9073\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 61s 242ms/step - loss: 8.9104 - val_loss: 8.9410\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 61s 242ms/step - loss: 8.8999 - val_loss: 8.7423\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 61s 242ms/step - loss: 8.8913 - val_loss: 8.8795\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 61s 243ms/step - loss: 8.8532 - val_loss: 8.6356\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 61s 242ms/step - loss: 8.8297 - val_loss: 8.8253\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 61s 243ms/step - loss: 8.8109 - val_loss: 8.8316\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 61s 243ms/step - loss: 8.8157 - val_loss: 8.5747\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 61s 242ms/step - loss: 8.7756 - val_loss: 8.7932\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 61s 243ms/step - loss: 8.7924 - val_loss: 8.7164\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 61s 242ms/step - loss: 8.7299 - val_loss: 8.6967\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 61s 242ms/step - loss: 8.7332 - val_loss: 8.5792\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 61s 242ms/step - loss: 8.7553 - val_loss: 8.5546\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 61s 243ms/step - loss: 8.7009 - val_loss: 8.6594\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 61s 242ms/step - loss: 8.7187 - val_loss: 8.8461\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 61s 242ms/step - loss: 8.6836 - val_loss: 8.5662\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 61s 242ms/step - loss: 8.6617 - val_loss: 8.7488\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 61s 242ms/step - loss: 8.6597 - val_loss: 8.6767\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 61s 242ms/step - loss: 8.6317 - val_loss: 9.1376\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 61s 242ms/step - loss: 8.6556 - val_loss: 8.5412\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 61s 242ms/step - loss: 8.6001 - val_loss: 8.6610\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 61s 242ms/step - loss: 8.6205 - val_loss: 8.5557\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 61s 242ms/step - loss: 8.5668 - val_loss: 8.7638\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 61s 242ms/step - loss: 8.5699 - val_loss: 8.6136\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 61s 242ms/step - loss: 8.5212 - val_loss: 8.5276\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 61s 242ms/step - loss: 8.5503 - val_loss: 8.5427\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 61s 242ms/step - loss: 8.5125 - val_loss: 8.5946\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 61s 242ms/step - loss: 8.5351 - val_loss: 8.8547\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 61s 242ms/step - loss: 8.5067 - val_loss: 8.7202\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 61s 242ms/step - loss: 8.4684 - val_loss: 8.5813\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 61s 242ms/step - loss: 8.4583 - val_loss: 8.5444\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 61s 242ms/step - loss: 8.4328 - val_loss: 8.5130\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 61s 242ms/step - loss: 8.4196 - val_loss: 8.5505\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 61s 242ms/step - loss: 8.4645 - val_loss: 8.6424\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 61s 242ms/step - loss: 8.4346 - val_loss: 8.5653\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 61s 242ms/step - loss: 8.3942 - val_loss: 8.4716\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 61s 242ms/step - loss: 8.4437 - val_loss: 8.4620\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 61s 242ms/step - loss: 8.3681 - val_loss: 8.6962\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 61s 242ms/step - loss: 8.4200 - val_loss: 8.3749\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 61s 242ms/step - loss: 8.3840 - val_loss: 8.5309\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 61s 242ms/step - loss: 8.3537 - val_loss: 8.5387\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 61s 242ms/step - loss: 8.3214 - val_loss: 8.9162\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 61s 242ms/step - loss: 8.3564 - val_loss: 8.3816\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 61s 242ms/step - loss: 8.5911 - val_loss: 8.6021\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 61s 242ms/step - loss: 8.4240 - val_loss: 8.6659\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 61s 242ms/step - loss: 8.3494 - val_loss: 8.6248\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 61s 242ms/step - loss: 8.2995 - val_loss: 8.4715\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 61s 242ms/step - loss: 8.3303 - val_loss: 8.4145\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 61s 242ms/step - loss: 8.3036 - val_loss: 8.6491\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 61s 242ms/step - loss: 8.3012 - val_loss: 8.7373\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 61s 242ms/step - loss: 8.3232 - val_loss: 8.3696\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 61s 242ms/step - loss: 8.2702 - val_loss: 8.9207\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 61s 242ms/step - loss: 8.3078 - val_loss: 8.4606\n",
      "\n",
      "Loss: 846.06%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 112)       1120      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 94, 94, 112)      448       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 94, 94, 112)       0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 166)       167494    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 92, 92, 166)      664       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 73)        109135    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 90, 90, 73)       292       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 132)       86856     \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 88, 88, 132)      528       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 136)       161704    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 528,245\n",
      "Trainable params: 527,277\n",
      "Non-trainable params: 968\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 28s 101ms/step - loss: 1199.0128 - val_loss: 29.5175\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.8792 - val_loss: 11.3509\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.2237 - val_loss: 11.2472\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.1732 - val_loss: 10.2671\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.1580 - val_loss: 10.2915\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.1370 - val_loss: 10.1158\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.1240 - val_loss: 9.9325\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.1093 - val_loss: 9.9500\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.1274 - val_loss: 10.2835\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.1065 - val_loss: 10.3757\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.1055 - val_loss: 9.9789\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.1078 - val_loss: 9.9311\n",
      "Epoch 13/128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 24s 95ms/step - loss: 10.1303 - val_loss: 10.5060\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.1051 - val_loss: 10.3094\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.1192 - val_loss: 9.9784\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.1199 - val_loss: 10.0334\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.1045 - val_loss: 9.9718\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.1220 - val_loss: 10.0112\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.1224 - val_loss: 10.0296\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.1205 - val_loss: 9.8366\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.1190 - val_loss: 10.2035\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.1131 - val_loss: 10.0775\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.0862 - val_loss: 9.9901\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.1023 - val_loss: 9.8202\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.1249 - val_loss: 9.9082\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.1176 - val_loss: 9.9195\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.1089 - val_loss: 9.9106\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.1289 - val_loss: 9.8215\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.1290 - val_loss: 9.8749\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.1078 - val_loss: 10.1886\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.0978 - val_loss: 9.9708\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.1201 - val_loss: 10.1308\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.1199 - val_loss: 10.1116\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.1101 - val_loss: 9.9458\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.1192 - val_loss: 10.1315\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.1005 - val_loss: 9.9793\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.1249 - val_loss: 9.9309\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.1026 - val_loss: 10.3559\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.1287 - val_loss: 10.4984\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.1354 - val_loss: 9.9533\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.0943 - val_loss: 9.8392\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.0777 - val_loss: 10.1428\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.0965 - val_loss: 10.2764\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.0560 - val_loss: 9.9791\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.1000 - val_loss: 9.8611\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.0759 - val_loss: 9.9338\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.0943 - val_loss: 9.8459\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.0627 - val_loss: 10.3514\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.0903 - val_loss: 9.8878\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.0661 - val_loss: 9.9884\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.0631 - val_loss: 9.9241\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.0922 - val_loss: 9.8767\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.0532 - val_loss: 9.8401\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.0411 - val_loss: 9.9742\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.0654 - val_loss: 9.9132\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.0502 - val_loss: 9.8950\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.0973 - val_loss: 9.9094\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.0650 - val_loss: 10.1603\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.0558 - val_loss: 10.0108\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.0119 - val_loss: 9.7614\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.0467 - val_loss: 9.7788\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.0434 - val_loss: 9.7932\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.0618 - val_loss: 9.8834\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.0410 - val_loss: 9.9980\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.0282 - val_loss: 9.8172\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.0322 - val_loss: 9.9609\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.0392 - val_loss: 9.8087\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.0600 - val_loss: 9.8143\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.0268 - val_loss: 9.8492\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.0184 - val_loss: 11.1218\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.0255 - val_loss: 10.0171\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.0195 - val_loss: 9.7854\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.9850 - val_loss: 9.7620\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.0480 - val_loss: 10.1749\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.0419 - val_loss: 9.8394\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.0198 - val_loss: 9.7434\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.0153 - val_loss: 9.8210\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.0349 - val_loss: 10.4209\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.0304 - val_loss: 9.8562\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.9811 - val_loss: 9.8336\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.9895 - val_loss: 10.1498\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.0375 - val_loss: 10.2514\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.0136 - val_loss: 9.8904\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.9989 - val_loss: 10.0118\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.0179 - val_loss: 9.7817\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.9925 - val_loss: 9.9446\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.9987 - val_loss: 9.8879\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.0135 - val_loss: 9.8616\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.9957 - val_loss: 10.0108\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.0239 - val_loss: 10.7283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.9829 - val_loss: 9.7708\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.9762 - val_loss: 9.7884\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.9717 - val_loss: 10.0366\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.9814 - val_loss: 9.9211\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.9965 - val_loss: 9.9297\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.9844 - val_loss: 10.2402\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.9878 - val_loss: 10.0343\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.9946 - val_loss: 9.7577\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.9497 - val_loss: 10.3158\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.9454 - val_loss: 9.9466\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.9674 - val_loss: 9.9490\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.9844 - val_loss: 9.8519\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.9579 - val_loss: 10.3441\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.9755 - val_loss: 9.7223\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.9507 - val_loss: 10.0338\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.9821 - val_loss: 9.7711\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.9608 - val_loss: 11.4200\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 10.0143 - val_loss: 9.6837\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.9557 - val_loss: 9.9487\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.9891 - val_loss: 9.7399\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.9456 - val_loss: 10.0610\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.9640 - val_loss: 9.8694\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.9662 - val_loss: 13.4529\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.9885 - val_loss: 10.3336\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.9572 - val_loss: 9.9126\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.9667 - val_loss: 9.9488\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.9665 - val_loss: 10.2296\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.9460 - val_loss: 10.2011\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.9852 - val_loss: 9.7093\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.9400 - val_loss: 9.8954\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.9329 - val_loss: 10.1033\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.9288 - val_loss: 10.6668\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.9895 - val_loss: 9.6284\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.9411 - val_loss: 10.4612\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.9207 - val_loss: 9.7988\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.9102 - val_loss: 9.6934\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.9483 - val_loss: 9.9133\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 9.8947 - val_loss: 9.6476\n",
      "\n",
      "Loss: 964.76%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 43)        430       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 43)        16684     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 92, 92, 43)       172       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 92, 92, 43)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 145)       56260     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 90, 90, 145)      580       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 72)        94032     \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 88, 88, 72)       288       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 68)        44132     \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 86, 86, 68)       272       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 84, 84, 136)       83368     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 296,222\n",
      "Trainable params: 295,564\n",
      "Non-trainable params: 658\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 21s 76ms/step - loss: 1565.1521 - val_loss: 105.9826\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 20.2347 - val_loss: 12.6104\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 10.0923 - val_loss: 10.3025\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 10.0432 - val_loss: 10.0256\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 10.0329 - val_loss: 9.9468\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 10.0258 - val_loss: 9.9331\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 10.0071 - val_loss: 9.9678\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 10.0085 - val_loss: 10.0338\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 10.0223 - val_loss: 9.7952\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 10.0069 - val_loss: 9.7621\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 10.0075 - val_loss: 9.9249\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 10.0060 - val_loss: 9.8047\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 10.0152 - val_loss: 9.8781\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 10.0146 - val_loss: 10.0222\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 10.0161 - val_loss: 9.7922\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 10.0286 - val_loss: 9.8699\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 10.0254 - val_loss: 9.7360\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 10.0005 - val_loss: 9.8592\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 10.0148 - val_loss: 9.8412\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 10.0209 - val_loss: 10.0476\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 10.0209 - val_loss: 9.8022\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 10.0356 - val_loss: 9.8533\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 10.0365 - val_loss: 9.8398\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 10.0097 - val_loss: 9.8916\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 10.0001 - val_loss: 9.8107\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 10.0176 - val_loss: 9.8518\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 10.0358 - val_loss: 9.9737\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 10.0312 - val_loss: 9.9819\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 10.0206 - val_loss: 9.7948\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 10.0109 - val_loss: 9.7710\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 10.0439 - val_loss: 9.8983\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 10.0179 - val_loss: 9.8779\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 10.0268 - val_loss: 9.7700\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 10.0218 - val_loss: 9.9669\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 10.0125 - val_loss: 9.8481\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 10.0026 - val_loss: 9.7208\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 10.0052 - val_loss: 9.7613\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 10.0086 - val_loss: 9.7834\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 10.0154 - val_loss: 9.8364\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 10.0037 - val_loss: 9.7880\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 10.0171 - val_loss: 10.0524\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 10.0222 - val_loss: 9.8831\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.9912 - val_loss: 9.7645\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.9946 - val_loss: 9.8259\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 10.0052 - val_loss: 9.7733\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 10.0039 - val_loss: 9.7982\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 10.0099 - val_loss: 9.7807\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 10.0043 - val_loss: 9.8317\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.9966 - val_loss: 9.9185\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.9969 - val_loss: 9.7752\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.9909 - val_loss: 9.7901\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.9709 - val_loss: 9.7554\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.9726 - val_loss: 9.6926\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.9972 - val_loss: 9.7242\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.9773 - val_loss: 9.8130\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.9801 - val_loss: 9.9005\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.9821 - val_loss: 9.9572\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 10.0112 - val_loss: 10.0121\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.9772 - val_loss: 9.9209\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.9746 - val_loss: 9.8162\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.9469 - val_loss: 9.7462\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.9673 - val_loss: 9.7325\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.9629 - val_loss: 9.6831\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.9759 - val_loss: 9.7611\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.9580 - val_loss: 9.9590\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.9931 - val_loss: 9.8348\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.9543 - val_loss: 9.7140\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.9841 - val_loss: 9.7627\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.9690 - val_loss: 9.7787\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.9467 - val_loss: 9.8705\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.9521 - val_loss: 9.6965\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.9605 - val_loss: 9.7072\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.9541 - val_loss: 9.8906\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.9611 - val_loss: 9.6686\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.9354 - val_loss: 9.8287\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.9330 - val_loss: 9.7920\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.9247 - val_loss: 9.7386\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.9107 - val_loss: 9.6442\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.9356 - val_loss: 9.7612\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.9053 - val_loss: 9.7793\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.9069 - val_loss: 9.7775\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.9042 - val_loss: 9.6686\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.9228 - val_loss: 9.7649\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.8961 - val_loss: 9.8449\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.9042 - val_loss: 9.7210\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.9200 - val_loss: 9.8457\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.8967 - val_loss: 9.7005\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.9199 - val_loss: 9.8422\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.8916 - val_loss: 9.8082\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.9022 - val_loss: 9.6861\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.8646 - val_loss: 9.8413\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.8887 - val_loss: 9.7940\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.8672 - val_loss: 9.8272\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.8836 - val_loss: 9.9949\n",
      "Epoch 95/128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 18s 71ms/step - loss: 9.8718 - val_loss: 9.9037\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.8582 - val_loss: 10.0749\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.8761 - val_loss: 9.8954\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.8700 - val_loss: 9.9389\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.8708 - val_loss: 9.8893\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.8705 - val_loss: 9.7393\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.8590 - val_loss: 9.7621\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.8647 - val_loss: 9.6575\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.8660 - val_loss: 9.6613\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.8772 - val_loss: 9.6610\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.8534 - val_loss: 9.9907\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.8678 - val_loss: 9.7009\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.8658 - val_loss: 9.6636\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.8408 - val_loss: 9.6887\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.8379 - val_loss: 9.6246\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.8317 - val_loss: 10.1831\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.8612 - val_loss: 9.6384\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.8320 - val_loss: 9.6367\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.8426 - val_loss: 9.8062\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.8492 - val_loss: 9.8221\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.8252 - val_loss: 9.6868\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.8234 - val_loss: 9.6656\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.8219 - val_loss: 9.6609\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.8264 - val_loss: 9.6386\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.8202 - val_loss: 9.6293\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.8562 - val_loss: 9.7858\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.8126 - val_loss: 9.7820\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.8119 - val_loss: 9.6373\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.8166 - val_loss: 9.8753\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.8055 - val_loss: 9.7472\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.8193 - val_loss: 9.5932\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.8267 - val_loss: 9.6056\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.8089 - val_loss: 9.7525\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 9.8246 - val_loss: 9.6176\n",
      "\n",
      "Loss: 961.76%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 230)       2300      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 230)       476330    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 92, 92, 230)      920       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 92, 92, 230)       0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 184)       381064    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 90, 90, 184)      736       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 23)        38111     \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 88, 88, 23)       92        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 103)       21424     \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 86, 86, 103)      412       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 84, 84, 136)       126208    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,047,601\n",
      "Trainable params: 1,046,519\n",
      "Non-trainable params: 1,082\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 37s 137ms/step - loss: 1329.5726 - val_loss: 36.7713\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 11.9160 - val_loss: 10.7755\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 10.1126 - val_loss: 10.7842\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 10.0851 - val_loss: 9.8656\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 10.0818 - val_loss: 9.9492\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 10.0526 - val_loss: 9.9967\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 10.0563 - val_loss: 9.9254\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 10.0416 - val_loss: 9.9074\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 10.0690 - val_loss: 9.9128\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 10.0692 - val_loss: 9.8550\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 10.0888 - val_loss: 9.8636\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 10.0470 - val_loss: 9.8611\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 10.0550 - val_loss: 10.0620\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 10.0500 - val_loss: 10.0819\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 10.0818 - val_loss: 10.0778\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 10.0751 - val_loss: 10.0753\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 10.0637 - val_loss: 9.8114\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 10.0663 - val_loss: 10.0004\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 10.0656 - val_loss: 9.9568\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 10.0614 - val_loss: 9.7514\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 10.0633 - val_loss: 9.9821\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 10.0639 - val_loss: 10.4309\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 10.0880 - val_loss: 9.9233\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 10.0672 - val_loss: 9.9513\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 10.0653 - val_loss: 10.1229\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 10.0655 - val_loss: 9.9623\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 10.0621 - val_loss: 9.7998\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 10.0650 - val_loss: 9.8190\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 10.0439 - val_loss: 10.0105\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 10.0619 - val_loss: 9.8671\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 10.0618 - val_loss: 10.0247\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 10.0528 - val_loss: 10.6865\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 10.0593 - val_loss: 10.0661\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 10.0641 - val_loss: 10.0632\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 10.0619 - val_loss: 9.8715\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 10.0322 - val_loss: 9.9075\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 10.0643 - val_loss: 10.0025\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 10.0549 - val_loss: 10.7136\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 10.0752 - val_loss: 9.9827\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 10.0333 - val_loss: 9.7162\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 10.0533 - val_loss: 10.3012\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 10.0480 - val_loss: 9.7942\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 10.0465 - val_loss: 10.2112\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 10.0328 - val_loss: 9.9915\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 10.0287 - val_loss: 9.9580\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 10.0170 - val_loss: 10.0914\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 10.0120 - val_loss: 9.9154\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 10.0361 - val_loss: 10.0738\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 10.0272 - val_loss: 10.1764\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 10.0399 - val_loss: 10.0775\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 10.0296 - val_loss: 9.7449\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 10.0140 - val_loss: 10.1869\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 10.0068 - val_loss: 10.1622\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 10.0245 - val_loss: 9.9979\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 10.0090 - val_loss: 10.0112\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 10.0086 - val_loss: 9.7935\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 9.9861 - val_loss: 9.9004\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 9.9987 - val_loss: 9.8128\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 10.0009 - val_loss: 9.9283\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 10.0095 - val_loss: 9.8666\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 9.9980 - val_loss: 9.9068\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 10.0133 - val_loss: 9.8976\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 10.0020 - val_loss: 9.8268\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 10.0059 - val_loss: 10.5781\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 9.9838 - val_loss: 10.3774\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 9.9827 - val_loss: 9.9499\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 9.9877 - val_loss: 9.9921\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 9.9860 - val_loss: 9.9095\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 9.9747 - val_loss: 9.8587\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 9.9752 - val_loss: 10.0132\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 9.9714 - val_loss: 10.0295\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 9.9733 - val_loss: 9.7770\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 9.9507 - val_loss: 10.1243\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 9.9693 - val_loss: 9.8019\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 9.9956 - val_loss: 9.8614\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 9.9398 - val_loss: 10.0649\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 9.9821 - val_loss: 9.7808\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 9.9539 - val_loss: 9.8337\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 9.9315 - val_loss: 9.9295\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 9.9776 - val_loss: 9.8648\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 9.9576 - val_loss: 9.7620\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 9.9514 - val_loss: 9.9805\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 9.9407 - val_loss: 9.9986\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 9.9526 - val_loss: 9.9240\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 9.9738 - val_loss: 9.8543\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 9.9408 - val_loss: 9.9574\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 9.9425 - val_loss: 10.1698\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 9.9275 - val_loss: 9.8113\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 9.9238 - val_loss: 9.7394\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 9.9474 - val_loss: 9.7433\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 9.9258 - val_loss: 9.9466\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 9.9340 - val_loss: 9.7540\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 9.9135 - val_loss: 10.0097\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 9.9277 - val_loss: 11.3678\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 9.9198 - val_loss: 9.7847\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 9.9412 - val_loss: 9.8180\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 9.9244 - val_loss: 9.7600\n",
      "Epoch 98/128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 33s 132ms/step - loss: 9.9143 - val_loss: 9.7504\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 9.9374 - val_loss: 9.7923\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 9.9157 - val_loss: 9.9394\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 9.9209 - val_loss: 9.8696\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 9.9129 - val_loss: 10.0988\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 9.9150 - val_loss: 9.7458\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 9.9010 - val_loss: 9.8016\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 9.9079 - val_loss: 9.9077\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 9.8878 - val_loss: 9.7252\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 9.9013 - val_loss: 10.2095\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 9.8952 - val_loss: 9.7097\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 9.8983 - val_loss: 9.7405\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 9.9147 - val_loss: 10.0238\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 9.9182 - val_loss: 9.9245\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 9.8978 - val_loss: 9.7454\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 9.9155 - val_loss: 9.7855\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 9.8835 - val_loss: 9.8611\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 9.9223 - val_loss: 9.8617\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 9.8926 - val_loss: 9.7383\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 9.8956 - val_loss: 10.0792\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 9.8933 - val_loss: 10.0384\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 9.8928 - val_loss: 9.7288\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 9.9225 - val_loss: 10.0865\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 9.8969 - val_loss: 9.9021\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 9.8978 - val_loss: 9.8509\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 9.8842 - val_loss: 9.7768\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 9.8825 - val_loss: 9.6954\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 9.8723 - val_loss: 9.7602\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 33s 132ms/step - loss: 9.9011 - val_loss: 9.8458\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 9.8976 - val_loss: 9.7391\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 33s 131ms/step - loss: 9.8802 - val_loss: 9.7333\n",
      "\n",
      "Loss: 973.33%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 191)       1910      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 94, 94, 191)      764       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 80)        137600    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 92, 92, 80)       320       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 44)        31724     \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 90, 90, 44)       176       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 136)       53992     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 226,490\n",
      "Trainable params: 225,858\n",
      "Non-trainable params: 632\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 17s 60ms/step - loss: 1861.3036 - val_loss: 531.3199\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 58.4021 - val_loss: 18.8653\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.2816 - val_loss: 9.8166\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0037 - val_loss: 10.0888\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0047 - val_loss: 10.2992\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9976 - val_loss: 9.8103\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0012 - val_loss: 9.7108\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9943 - val_loss: 9.8645\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0114 - val_loss: 9.9255\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0035 - val_loss: 9.9385\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9933 - val_loss: 9.8628\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9870 - val_loss: 9.8971\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9895 - val_loss: 9.8502\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0068 - val_loss: 9.7141\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0263 - val_loss: 9.7446\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9959 - val_loss: 9.9150\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0076 - val_loss: 9.9285\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0037 - val_loss: 9.7698\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0176 - val_loss: 9.7549\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0084 - val_loss: 9.7320\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9936 - val_loss: 9.8174\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0043 - val_loss: 9.8153\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0021 - val_loss: 9.8273\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0007 - val_loss: 9.9174\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0159 - val_loss: 9.7642\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0211 - val_loss: 9.9146\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0232 - val_loss: 9.8578\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0239 - val_loss: 9.7825\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0214 - val_loss: 9.7295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0047 - val_loss: 9.8262\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0279 - val_loss: 9.7780\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9983 - val_loss: 9.7121\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0214 - val_loss: 9.8837\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0193 - val_loss: 9.7632\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0014 - val_loss: 9.7459\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0123 - val_loss: 9.7536\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0085 - val_loss: 9.9344\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9905 - val_loss: 9.8801\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0121 - val_loss: 9.8076\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0041 - val_loss: 9.7790\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0019 - val_loss: 9.9157\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0153 - val_loss: 9.8381\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0072 - val_loss: 9.8343\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0256 - val_loss: 9.8120\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0137 - val_loss: 9.7623\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0053 - val_loss: 9.8403\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9971 - val_loss: 9.8114\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0003 - val_loss: 9.8177\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0000 - val_loss: 9.7552\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0085 - val_loss: 9.8410\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0084 - val_loss: 9.8002\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9916 - val_loss: 9.6917\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0093 - val_loss: 9.8442\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0046 - val_loss: 9.8466\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9863 - val_loss: 9.8686\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0034 - val_loss: 9.8037\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0166 - val_loss: 9.7347\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0085 - val_loss: 9.7888\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0028 - val_loss: 9.7115\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0070 - val_loss: 9.7971\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9935 - val_loss: 9.7240\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0173 - val_loss: 9.7783\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0015 - val_loss: 9.7527\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9985 - val_loss: 9.8519\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0020 - val_loss: 9.8552\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0148 - val_loss: 9.7646\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9948 - val_loss: 9.7980\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0048 - val_loss: 9.7658\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9959 - val_loss: 9.7738\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0083 - val_loss: 9.8342\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9889 - val_loss: 9.7649\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0041 - val_loss: 9.8507\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0015 - val_loss: 9.7782\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9958 - val_loss: 9.8089\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9842 - val_loss: 9.8163\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9807 - val_loss: 9.7442\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9946 - val_loss: 9.7178\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9753 - val_loss: 10.0379\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9926 - val_loss: 9.7639\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9890 - val_loss: 9.8270\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0013 - val_loss: 9.7793\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0051 - val_loss: 9.7921\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9888 - val_loss: 9.7454\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9779 - val_loss: 9.6984\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9805 - val_loss: 9.8074\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0036 - val_loss: 9.7754\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9787 - val_loss: 9.7817\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9904 - val_loss: 9.7149\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9715 - val_loss: 9.7187\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9709 - val_loss: 9.8022\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9912 - val_loss: 9.7661\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9956 - val_loss: 9.8736\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9875 - val_loss: 9.7519\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9816 - val_loss: 9.7194\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9737 - val_loss: 9.7407\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9909 - val_loss: 9.8190\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9945 - val_loss: 9.7092\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9786 - val_loss: 9.7969\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0056 - val_loss: 9.7948\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9819 - val_loss: 9.7547\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9836 - val_loss: 9.7919\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9897 - val_loss: 9.7605\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9773 - val_loss: 9.7791\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9875 - val_loss: 9.7959\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9676 - val_loss: 9.7360\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9868 - val_loss: 9.7325\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9757 - val_loss: 9.8108\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9788 - val_loss: 9.8111\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9889 - val_loss: 9.7444\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 10.0024 - val_loss: 9.6938\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9556 - val_loss: 9.6785\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9679 - val_loss: 9.6930\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9929 - val_loss: 9.7100\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9634 - val_loss: 9.7114\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9693 - val_loss: 9.8894\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9705 - val_loss: 9.7980\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9677 - val_loss: 9.7965\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9775 - val_loss: 9.8391\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9680 - val_loss: 9.7067\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9627 - val_loss: 9.7173\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9663 - val_loss: 9.7637\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9853 - val_loss: 9.7371\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9711 - val_loss: 9.9049\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9664 - val_loss: 9.8913\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9649 - val_loss: 9.7501\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9726 - val_loss: 9.8546\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9658 - val_loss: 9.7928\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 14s 57ms/step - loss: 9.9701 - val_loss: 9.8136\n",
      "\n",
      "Loss: 981.36%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 148)       1480      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 148)       197284    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 92, 92, 148)      592       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 92, 92, 148)       0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 256)       341248    \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 88, 88, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 88, 88, 256)       0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 154)       354970    \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 84, 84, 154)       213598    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 84, 84, 154)      616       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 82, 82, 126)       174762    \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 82, 82, 126)      504       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 80, 80, 119)       135065    \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 80, 80, 119)      476       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 78, 78, 136)       145792    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,157,495\n",
      "Trainable params: 2,155,887\n",
      "Non-trainable params: 1,608\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 61s 231ms/step - loss: 1257.8553 - val_loss: 81.5448\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 56s 223ms/step - loss: 12.0303 - val_loss: 14.3533\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 56s 223ms/step - loss: 10.5574 - val_loss: 13.2472\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 56s 223ms/step - loss: 10.4322 - val_loss: 11.4683\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 56s 223ms/step - loss: 10.4375 - val_loss: 11.8203\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 56s 224ms/step - loss: 10.3398 - val_loss: 11.2130\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 56s 223ms/step - loss: 10.2423 - val_loss: 10.6418\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 56s 223ms/step - loss: 10.2360 - val_loss: 10.2923\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 56s 224ms/step - loss: 10.1793 - val_loss: 9.9335\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 56s 224ms/step - loss: 10.1849 - val_loss: 10.3799\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 56s 224ms/step - loss: 10.1553 - val_loss: 10.2690\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 56s 224ms/step - loss: 10.2338 - val_loss: 12.1898\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 56s 223ms/step - loss: 10.1342 - val_loss: 10.4849\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 56s 223ms/step - loss: 10.1852 - val_loss: 10.1181\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 56s 223ms/step - loss: 10.1742 - val_loss: 10.5770\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 56s 223ms/step - loss: 10.1171 - val_loss: 10.5031\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 56s 224ms/step - loss: 10.1263 - val_loss: 9.9871\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 56s 223ms/step - loss: 10.0961 - val_loss: 10.0259\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 56s 224ms/step - loss: 10.1279 - val_loss: 10.2626\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 56s 224ms/step - loss: 10.1559 - val_loss: 10.3911\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 56s 223ms/step - loss: 10.1759 - val_loss: 10.4166\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 56s 223ms/step - loss: 10.0984 - val_loss: 9.9538\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 56s 223ms/step - loss: 10.1453 - val_loss: 10.2684\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 56s 224ms/step - loss: 10.1177 - val_loss: 10.5093\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 56s 224ms/step - loss: 10.1117 - val_loss: 10.0653\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 56s 224ms/step - loss: 10.1175 - val_loss: 9.9492\n",
      "Epoch 27/128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 56s 223ms/step - loss: 10.1192 - val_loss: 11.8444\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 56s 224ms/step - loss: 10.1322 - val_loss: 10.0363\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 56s 224ms/step - loss: 10.0769 - val_loss: 10.2348\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 56s 224ms/step - loss: 10.0503 - val_loss: 10.9243\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 56s 223ms/step - loss: 10.0972 - val_loss: 9.8225\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 56s 224ms/step - loss: 10.0435 - val_loss: 9.7244\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 56s 224ms/step - loss: 10.0577 - val_loss: 11.0489\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 56s 224ms/step - loss: 10.0548 - val_loss: 10.5073\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 56s 224ms/step - loss: 10.0136 - val_loss: 10.4592\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 56s 224ms/step - loss: 9.9997 - val_loss: 10.0357\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 56s 223ms/step - loss: 9.9828 - val_loss: 9.6901\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 56s 224ms/step - loss: 9.9844 - val_loss: 10.3724\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 56s 224ms/step - loss: 9.9515 - val_loss: 9.7268\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 56s 224ms/step - loss: 9.9373 - val_loss: 10.4990\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 56s 224ms/step - loss: 9.9384 - val_loss: 9.5725\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 56s 224ms/step - loss: 9.9523 - val_loss: 10.0085\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 56s 224ms/step - loss: 9.8961 - val_loss: 9.8389\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 56s 223ms/step - loss: 9.8857 - val_loss: 10.5034\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 56s 224ms/step - loss: 9.8891 - val_loss: 9.6806\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 56s 224ms/step - loss: 9.8693 - val_loss: 10.1521\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 56s 224ms/step - loss: 9.8601 - val_loss: 10.1835\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 56s 224ms/step - loss: 9.8329 - val_loss: 9.5918\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 56s 224ms/step - loss: 9.8699 - val_loss: 9.7849\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 56s 223ms/step - loss: 9.8244 - val_loss: 9.5489\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 56s 224ms/step - loss: 9.8123 - val_loss: 10.0298\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 56s 224ms/step - loss: 9.7890 - val_loss: 9.8011\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 56s 224ms/step - loss: 9.7353 - val_loss: 10.6339\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 56s 223ms/step - loss: 9.7308 - val_loss: 9.4672\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 56s 223ms/step - loss: 9.7270 - val_loss: 9.9808\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 56s 224ms/step - loss: 9.6633 - val_loss: 9.5542\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 56s 224ms/step - loss: 9.6294 - val_loss: 9.3904\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 56s 224ms/step - loss: 9.5762 - val_loss: 9.3651\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 56s 224ms/step - loss: 9.5539 - val_loss: 9.4607\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 56s 223ms/step - loss: 9.5701 - val_loss: 9.4864\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 56s 223ms/step - loss: 9.5174 - val_loss: 9.5439\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 56s 223ms/step - loss: 9.4402 - val_loss: 9.3221\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 56s 224ms/step - loss: 9.4436 - val_loss: 9.8240\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 56s 223ms/step - loss: 9.4305 - val_loss: 9.4851\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 56s 223ms/step - loss: 9.3611 - val_loss: 9.3618\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 56s 224ms/step - loss: 9.3488 - val_loss: 9.4790\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 56s 224ms/step - loss: 9.3244 - val_loss: 9.2701\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 56s 223ms/step - loss: 9.2941 - val_loss: 9.5591\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 56s 223ms/step - loss: 9.2612 - val_loss: 9.2557\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 56s 224ms/step - loss: 9.2613 - val_loss: 9.1047\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 56s 224ms/step - loss: 9.2378 - val_loss: 9.0049\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 56s 224ms/step - loss: 9.2266 - val_loss: 9.5166\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 56s 224ms/step - loss: 9.2243 - val_loss: 9.0397\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 56s 223ms/step - loss: 9.1726 - val_loss: 9.4316\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 56s 223ms/step - loss: 9.1578 - val_loss: 8.9589\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 56s 223ms/step - loss: 9.1224 - val_loss: 8.9406\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 56s 223ms/step - loss: 9.1463 - val_loss: 8.9911\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 56s 223ms/step - loss: 9.1131 - val_loss: 8.9930\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 56s 223ms/step - loss: 9.0919 - val_loss: 10.0595\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 56s 223ms/step - loss: 9.0652 - val_loss: 9.1906\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 56s 223ms/step - loss: 9.0641 - val_loss: 8.8894\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 56s 223ms/step - loss: 9.0554 - val_loss: 9.1381\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 56s 223ms/step - loss: 9.0292 - val_loss: 9.0699\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 56s 224ms/step - loss: 9.0139 - val_loss: 8.8154\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 56s 223ms/step - loss: 9.0275 - val_loss: 8.9813\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 56s 223ms/step - loss: 9.0002 - val_loss: 9.0073\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 56s 224ms/step - loss: 9.0108 - val_loss: 9.1013\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 56s 223ms/step - loss: 8.9514 - val_loss: 9.1130\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 56s 223ms/step - loss: 8.9868 - val_loss: 8.7740\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 56s 224ms/step - loss: 8.9300 - val_loss: 8.9934\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 56s 224ms/step - loss: 8.9077 - val_loss: 8.9311\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 56s 224ms/step - loss: 8.8921 - val_loss: 9.3245\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 56s 224ms/step - loss: 8.8707 - val_loss: 8.6902\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 56s 224ms/step - loss: 8.8530 - val_loss: 8.8470\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 56s 224ms/step - loss: 8.8628 - val_loss: 9.7985\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 56s 223ms/step - loss: 8.8882 - val_loss: 8.7924\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 56s 224ms/step - loss: 8.8325 - val_loss: 9.7082\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 56s 224ms/step - loss: 8.7820 - val_loss: 9.2196\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 56s 223ms/step - loss: 8.8450 - val_loss: 9.7472\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 56s 223ms/step - loss: 8.8174 - val_loss: 8.7566\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 56s 223ms/step - loss: 8.7771 - val_loss: 8.8013\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 56s 224ms/step - loss: 8.7830 - val_loss: 8.7910\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 56s 224ms/step - loss: 8.7464 - val_loss: 9.0649\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 56s 224ms/step - loss: 8.7535 - val_loss: 9.0577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 105/128\n",
      "250/250 [==============================] - 56s 224ms/step - loss: 8.7496 - val_loss: 9.0512\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 56s 224ms/step - loss: 8.7403 - val_loss: 8.7726\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 56s 224ms/step - loss: 8.7085 - val_loss: 8.6805\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 56s 224ms/step - loss: 8.7116 - val_loss: 8.9025\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 56s 223ms/step - loss: 8.7393 - val_loss: 9.1958\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 56s 223ms/step - loss: 8.7068 - val_loss: 8.7522\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 56s 224ms/step - loss: 8.7039 - val_loss: 12.4566\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 56s 224ms/step - loss: 8.6238 - val_loss: 9.2216\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 56s 223ms/step - loss: 8.6214 - val_loss: 8.9643\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 56s 223ms/step - loss: 8.6558 - val_loss: 8.7712\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 56s 223ms/step - loss: 8.6324 - val_loss: 8.9133\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 56s 224ms/step - loss: 8.6189 - val_loss: 8.8971\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 56s 223ms/step - loss: 8.6511 - val_loss: 8.5927\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 56s 223ms/step - loss: 8.6250 - val_loss: 11.4808\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 56s 223ms/step - loss: 8.6215 - val_loss: 9.8377\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 56s 223ms/step - loss: 8.6064 - val_loss: 8.6178\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 56s 224ms/step - loss: 8.5954 - val_loss: 8.8442\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 56s 223ms/step - loss: 8.5922 - val_loss: 8.6545\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 56s 223ms/step - loss: 8.5558 - val_loss: 8.7971\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 56s 223ms/step - loss: 8.5921 - val_loss: 9.1082\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 56s 223ms/step - loss: 8.5466 - val_loss: 8.6494\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 56s 223ms/step - loss: 8.5400 - val_loss: 9.2594\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 56s 223ms/step - loss: 8.5342 - val_loss: 8.7383\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 56s 223ms/step - loss: 8.5417 - val_loss: 9.0676\n",
      "\n",
      "Loss: 906.76%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 256)       2560      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 92, 92, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 92, 92, 256)       0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 256)       590080    \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 88, 88, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 88, 88, 256)       0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 8)         18440     \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 84, 84, 8)         584       \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 84, 84, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 82, 82, 136)       9928      \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 82, 82, 136)      544       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 80, 80, 136)       166600    \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 80, 80, 136)      544       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 78, 78, 136)       166600    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,138,124\n",
      "Trainable params: 2,136,538\n",
      "Non-trainable params: 1,586\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 52s 204ms/step - loss: 1203.6837 - val_loss: 23.9007\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 11.0623 - val_loss: 14.2235\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.3490 - val_loss: 10.7581\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.2027 - val_loss: 11.6167\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.2007 - val_loss: 10.1090\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.1975 - val_loss: 9.9658\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.1598 - val_loss: 11.9355\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.2194 - val_loss: 10.1849\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.1679 - val_loss: 10.5925\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.1285 - val_loss: 10.4916\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.1251 - val_loss: 10.5340\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.1859 - val_loss: 10.7132\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.1424 - val_loss: 9.9989\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.1737 - val_loss: 11.0048\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.1282 - val_loss: 10.2591\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.1486 - val_loss: 10.2046\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.1143 - val_loss: 10.0514\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.1119 - val_loss: 10.2635\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.1313 - val_loss: 10.4924\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.1950 - val_loss: 10.2532\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.1248 - val_loss: 10.1294\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.1261 - val_loss: 10.5794\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.1531 - val_loss: 10.9795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.1223 - val_loss: 10.1890\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.1060 - val_loss: 9.9451\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.1102 - val_loss: 10.2429\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.0999 - val_loss: 10.5141\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.0978 - val_loss: 9.9827\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.0819 - val_loss: 10.0029\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.0687 - val_loss: 10.1112\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.0606 - val_loss: 10.7320\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.0628 - val_loss: 10.0895\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.0842 - val_loss: 10.8547\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.0755 - val_loss: 10.2947\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.0401 - val_loss: 9.8143\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.0206 - val_loss: 10.8293\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.0023 - val_loss: 9.8372\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.0134 - val_loss: 10.1497\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.0014 - val_loss: 9.8663\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.9873 - val_loss: 9.7723\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 10.0022 - val_loss: 10.1307\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.9631 - val_loss: 10.2751\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.9651 - val_loss: 10.3134\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.9427 - val_loss: 9.8214\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.9261 - val_loss: 9.6803\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.8992 - val_loss: 9.8715\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.8786 - val_loss: 9.7592\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.8803 - val_loss: 9.5246\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.8488 - val_loss: 9.9423\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.8222 - val_loss: 9.5003\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.8356 - val_loss: 9.7172\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.7770 - val_loss: 9.6257\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.7970 - val_loss: 9.9442\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.7311 - val_loss: 9.6537\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.7593 - val_loss: 9.6044\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.7085 - val_loss: 9.4611\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.7346 - val_loss: 9.7735\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.7257 - val_loss: 9.6453\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.6798 - val_loss: 9.9986\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.6802 - val_loss: 9.5623\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.6320 - val_loss: 9.5828\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.6744 - val_loss: 9.5012\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.6102 - val_loss: 9.4976\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.5897 - val_loss: 9.4431\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.5531 - val_loss: 9.3125\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.5633 - val_loss: 9.5501\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.5433 - val_loss: 9.7058\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.5489 - val_loss: 9.5312\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.5061 - val_loss: 9.6753\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.5226 - val_loss: 9.3443\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.4702 - val_loss: 9.3859\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.4721 - val_loss: 9.3593\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.4853 - val_loss: 9.4640\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.4376 - val_loss: 9.5439\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.4324 - val_loss: 9.2537\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.4113 - val_loss: 9.0712\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.4163 - val_loss: 9.6537\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.3755 - val_loss: 9.4679\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.3726 - val_loss: 9.9928\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.3643 - val_loss: 9.0865\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.3371 - val_loss: 9.2470\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.3114 - val_loss: 9.1937\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.3450 - val_loss: 9.2676\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.3385 - val_loss: 9.0719\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.3262 - val_loss: 8.9691\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.3226 - val_loss: 9.4256\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.3013 - val_loss: 9.0815\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.2715 - val_loss: 10.5844\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.2503 - val_loss: 9.0700\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.2639 - val_loss: 9.2710\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.2112 - val_loss: 9.6768\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.2407 - val_loss: 9.5165\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.2548 - val_loss: 9.4232\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.2573 - val_loss: 9.1343\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.1786 - val_loss: 9.0053\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.2031 - val_loss: 9.3158\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.2124 - val_loss: 9.1036\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.1967 - val_loss: 9.0779\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.1751 - val_loss: 8.9203\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.1632 - val_loss: 9.1740\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.1174 - val_loss: 9.5523\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.1413 - val_loss: 9.2030\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.1174 - val_loss: 9.5260\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.1177 - val_loss: 9.4819\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.1386 - val_loss: 9.0161\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.0671 - val_loss: 9.1113\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.1020 - val_loss: 9.3181\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.0704 - val_loss: 9.1163\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.0693 - val_loss: 9.1055\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.0173 - val_loss: 9.4363\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.0627 - val_loss: 9.1361\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.0586 - val_loss: 8.9889\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.0336 - val_loss: 9.3645\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 8.9922 - val_loss: 9.2475\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.0329 - val_loss: 8.9489\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.0190 - val_loss: 9.0061\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 8.9892 - val_loss: 9.0872\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 9.0042 - val_loss: 9.4784\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 8.9516 - val_loss: 9.2190\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 8.9717 - val_loss: 9.3092\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 8.9488 - val_loss: 9.1805\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 8.9413 - val_loss: 9.6497\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 8.9660 - val_loss: 8.8935\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 8.9093 - val_loss: 9.1901\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 8.9297 - val_loss: 9.2328\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 8.9332 - val_loss: 9.9368\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 8.8941 - val_loss: 8.9021\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 8.8723 - val_loss: 9.1289\n",
      "\n",
      "Loss: 912.89%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 256)       2560      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 92, 92, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 56)        129080    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 90, 90, 56)       224       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 8)         4040      \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 88, 88, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 136)       9928      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 736,972\n",
      "Trainable params: 736,330\n",
      "Non-trainable params: 642\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 23s 87ms/step - loss: 2734.7341 - val_loss: 2224.8645\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 1202.6526 - val_loss: 547.5131\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 218.7685 - val_loss: 73.5665\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 33.7275 - val_loss: 17.7361\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 12.7675 - val_loss: 10.8427\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 10.3349 - val_loss: 9.9384\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 10.0535 - val_loss: 9.9247\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 10.0076 - val_loss: 9.8025\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9849 - val_loss: 9.8371\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9798 - val_loss: 9.7465\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9791 - val_loss: 9.7499\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9717 - val_loss: 9.7440\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9765 - val_loss: 9.7853\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9720 - val_loss: 9.7139\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9592 - val_loss: 9.7332\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9557 - val_loss: 9.7521\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9593 - val_loss: 9.7536\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9540 - val_loss: 9.6919\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9554 - val_loss: 9.7249\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9631 - val_loss: 9.7613\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9682 - val_loss: 9.7237\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9621 - val_loss: 9.8177\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9566 - val_loss: 9.7524\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9561 - val_loss: 9.6998\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9620 - val_loss: 9.7105\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9573 - val_loss: 9.7106\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9482 - val_loss: 9.8175\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9588 - val_loss: 9.7104\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9544 - val_loss: 9.7104\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9661 - val_loss: 9.7830\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9579 - val_loss: 9.7819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9551 - val_loss: 9.7507\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9535 - val_loss: 9.6964\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9595 - val_loss: 9.7161\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9551 - val_loss: 9.7131\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9530 - val_loss: 9.7378\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9613 - val_loss: 9.7143\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9523 - val_loss: 9.6978\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9402 - val_loss: 9.7913\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9470 - val_loss: 9.7128\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9470 - val_loss: 9.7335\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9428 - val_loss: 9.7097\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9484 - val_loss: 9.7142\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9424 - val_loss: 9.7745\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9592 - val_loss: 9.7905\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9460 - val_loss: 9.7415\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9475 - val_loss: 9.7793\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9489 - val_loss: 9.6906\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9514 - val_loss: 9.7040\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9569 - val_loss: 9.7655\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9551 - val_loss: 9.7299\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9515 - val_loss: 9.7362\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9445 - val_loss: 9.7028\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9466 - val_loss: 9.7117\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9344 - val_loss: 9.6813\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9486 - val_loss: 9.7540\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9400 - val_loss: 9.7170\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9411 - val_loss: 9.6949\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9429 - val_loss: 9.7167\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9430 - val_loss: 9.7037\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9404 - val_loss: 9.7022\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9360 - val_loss: 9.6848\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9342 - val_loss: 9.7247\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9314 - val_loss: 9.7050\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9418 - val_loss: 9.7007\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9320 - val_loss: 9.6980\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9345 - val_loss: 9.7100\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9329 - val_loss: 9.7692\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9309 - val_loss: 9.7272\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9388 - val_loss: 9.7157\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9303 - val_loss: 9.6756\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9279 - val_loss: 9.7085\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9277 - val_loss: 9.7412\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9420 - val_loss: 9.7313\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9363 - val_loss: 9.6792\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9339 - val_loss: 9.6759\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9419 - val_loss: 9.7096\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9228 - val_loss: 9.7611\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9449 - val_loss: 9.7087\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9289 - val_loss: 9.7658\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9494 - val_loss: 9.7839\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9383 - val_loss: 9.7159\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9294 - val_loss: 9.7673\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9367 - val_loss: 9.6557\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9284 - val_loss: 9.7341\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9184 - val_loss: 9.8025\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9281 - val_loss: 9.7210\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9255 - val_loss: 9.6796\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9223 - val_loss: 9.7933\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9318 - val_loss: 9.7435\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9223 - val_loss: 9.6710\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9179 - val_loss: 9.6612\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9196 - val_loss: 9.6587\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9149 - val_loss: 9.7025\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9249 - val_loss: 9.7082\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9164 - val_loss: 9.7394\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9067 - val_loss: 9.6971\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9142 - val_loss: 9.7547\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9108 - val_loss: 9.6647\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9140 - val_loss: 9.7038\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9080 - val_loss: 9.7015\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9061 - val_loss: 9.7175\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9088 - val_loss: 9.7926\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9113 - val_loss: 9.6951\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9027 - val_loss: 9.7014\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9166 - val_loss: 9.7281\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9163 - val_loss: 9.7055\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9119 - val_loss: 9.6806\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9046 - val_loss: 9.7308\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9113 - val_loss: 9.6887\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.8994 - val_loss: 9.7871\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9134 - val_loss: 9.6900\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9029 - val_loss: 9.6798\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9078 - val_loss: 9.6992\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.8987 - val_loss: 9.7041\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9016 - val_loss: 9.6748\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.8979 - val_loss: 9.7016\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.8943 - val_loss: 9.7188\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.8899 - val_loss: 9.6781\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9178 - val_loss: 9.6776\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.9023 - val_loss: 9.7104\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.8961 - val_loss: 9.6653\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.8909 - val_loss: 9.7236\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.8892 - val_loss: 9.6736\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.8953 - val_loss: 9.6805\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.8908 - val_loss: 9.6729\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.8927 - val_loss: 9.6575\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 9.8812 - val_loss: 9.7143\n",
      "\n",
      "Loss: 971.43%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 150)       1500      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 150)       202650    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 92, 92, 150)      600       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 92, 92, 150)       0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 8)         10808     \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 8)         584       \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 88, 88, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 88, 88, 8)         0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 231)       16863     \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 86, 86, 231)      924       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 84, 84, 88)        183040    \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 84, 84, 88)       352       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 82, 82, 136)       107848    \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 82, 82, 136)      544       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 80, 80, 136)       166600    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 692,349\n",
      "Trainable params: 691,121\n",
      "Non-trainable params: 1,228\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 34s 124ms/step - loss: 1188.9860 - val_loss: 34.2137\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 11.2000 - val_loss: 12.6920\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 10.5754 - val_loss: 10.5501\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 10.4431 - val_loss: 11.6420\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 10.3646 - val_loss: 10.0043\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 10.2046 - val_loss: 10.1887\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 10.2231 - val_loss: 10.3592\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 10.2094 - val_loss: 9.8399\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 10.1991 - val_loss: 10.0311\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 10.1438 - val_loss: 13.1480\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 10.1644 - val_loss: 9.9057\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 10.1399 - val_loss: 10.1161\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 10.2024 - val_loss: 11.3906\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 10.1537 - val_loss: 10.8352\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 10.1630 - val_loss: 9.7502\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 10.1591 - val_loss: 10.0523\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 10.1759 - val_loss: 10.5818\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 10.1473 - val_loss: 10.1241\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 10.1588 - val_loss: 10.0923\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 10.1202 - val_loss: 9.8111\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 10.0900 - val_loss: 10.0370\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 10.0671 - val_loss: 10.3149\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 10.1013 - val_loss: 9.9007\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 10.1184 - val_loss: 10.0208\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 10.1193 - val_loss: 9.8144\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 10.0895 - val_loss: 9.9912\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 10.0854 - val_loss: 10.1428\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 10.0823 - val_loss: 9.8631\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 10.0728 - val_loss: 9.9119\n",
      "Epoch 30/128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 30s 118ms/step - loss: 10.0940 - val_loss: 9.9115\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 10.0838 - val_loss: 9.8303\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 10.0863 - val_loss: 10.7786\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 10.1028 - val_loss: 9.7716\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 10.0716 - val_loss: 10.1740\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 10.0588 - val_loss: 9.8584\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 10.0621 - val_loss: 9.8743\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 10.0217 - val_loss: 10.2220\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 10.0581 - val_loss: 9.9014\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 10.0304 - val_loss: 9.7753\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 10.0185 - val_loss: 10.0341\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 10.0391 - val_loss: 10.0568\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 10.0421 - val_loss: 10.1581\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 10.0280 - val_loss: 9.8598\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 10.0275 - val_loss: 10.8536\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 10.0232 - val_loss: 9.9651\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 10.0343 - val_loss: 9.7963\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 10.0010 - val_loss: 10.2698\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.9962 - val_loss: 10.4364\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.9859 - val_loss: 9.7695\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.9814 - val_loss: 10.2935\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.9783 - val_loss: 10.1631\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.9666 - val_loss: 9.7611\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.9803 - val_loss: 9.8182\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.9812 - val_loss: 9.9155\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.9679 - val_loss: 10.1934\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.9334 - val_loss: 9.8077\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.9488 - val_loss: 9.7176\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.9618 - val_loss: 9.6585\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.9260 - val_loss: 10.0039\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.9546 - val_loss: 10.2478\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.9297 - val_loss: 9.7251\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.9142 - val_loss: 10.0039\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.9257 - val_loss: 9.6870\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.9213 - val_loss: 10.3508\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.9371 - val_loss: 9.8889\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.9115 - val_loss: 9.9984\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.8974 - val_loss: 9.7636\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.9227 - val_loss: 9.7989\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.8679 - val_loss: 9.9855\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.8712 - val_loss: 9.7247\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.8819 - val_loss: 9.7872\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.8749 - val_loss: 9.6364\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.8707 - val_loss: 9.6606\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.8523 - val_loss: 9.9777\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.8391 - val_loss: 9.7738\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.8564 - val_loss: 9.7714\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.8402 - val_loss: 9.7979\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.8453 - val_loss: 9.7157\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.8286 - val_loss: 9.9343\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.8211 - val_loss: 9.6128\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.8452 - val_loss: 9.6693\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.8567 - val_loss: 10.7474\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.8246 - val_loss: 9.8808\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.8241 - val_loss: 9.5795\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.8160 - val_loss: 9.7426\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.8091 - val_loss: 9.8613\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.8204 - val_loss: 9.7295\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.7816 - val_loss: 9.6387\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.7810 - val_loss: 10.3856\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.7967 - val_loss: 9.6667\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.7975 - val_loss: 9.6228\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.7967 - val_loss: 9.5653\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.8101 - val_loss: 9.5938\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.8019 - val_loss: 9.5190\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.7688 - val_loss: 9.5420\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.7573 - val_loss: 9.6227\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.7856 - val_loss: 9.6411\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.7634 - val_loss: 10.1202\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.7455 - val_loss: 9.5180\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.7498 - val_loss: 9.6867\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.7434 - val_loss: 9.7557\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.7130 - val_loss: 9.7901\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.7177 - val_loss: 10.2910\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.7464 - val_loss: 9.7536\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.7016 - val_loss: 9.8554\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.7524 - val_loss: 9.5635\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.6875 - val_loss: 9.8883\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.7329 - val_loss: 9.5858\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.6939 - val_loss: 9.6576\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.6944 - val_loss: 9.5273\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.6783 - val_loss: 9.6968\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.6816 - val_loss: 9.6742\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.6581 - val_loss: 9.4761\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.6556 - val_loss: 9.5943\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.6951 - val_loss: 9.5438\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.6763 - val_loss: 9.9530\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.7021 - val_loss: 9.7560\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.6794 - val_loss: 9.5430\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.6716 - val_loss: 9.8813\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.6796 - val_loss: 9.6454\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.6705 - val_loss: 9.6954\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.6613 - val_loss: 9.3988\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.6553 - val_loss: 9.5022\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.6699 - val_loss: 9.4514\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.6561 - val_loss: 9.5436\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.6578 - val_loss: 9.5813\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.6606 - val_loss: 9.5972\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 30s 118ms/step - loss: 9.6297 - val_loss: 9.6235\n",
      "\n",
      "Loss: 962.35%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 8)         80        \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 8)         584       \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 92, 92, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 92, 92, 8)         0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 256)       18688     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 90, 90, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 136)       313480    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 88, 88, 136)      544       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 136)       166600    \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 86, 86, 136)      544       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 84, 84, 136)       166600    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 668,180\n",
      "Trainable params: 667,106\n",
      "Non-trainable params: 1,074\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 25s 97ms/step - loss: 1179.8101 - val_loss: 14.5486\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 10.5457 - val_loss: 10.4654\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 10.1135 - val_loss: 10.2533\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 10.0711 - val_loss: 10.0401\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 10.0505 - val_loss: 10.0160\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 10.0822 - val_loss: 9.9066\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 10.0645 - val_loss: 9.8300\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 10.0913 - val_loss: 9.7928\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 10.0596 - val_loss: 9.8316\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 10.0899 - val_loss: 9.9708\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 10.0647 - val_loss: 9.8615\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 10.0838 - val_loss: 9.9595\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 10.0703 - val_loss: 9.9009\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 10.0846 - val_loss: 9.9336\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 10.0742 - val_loss: 9.9410\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 10.1236 - val_loss: 9.8155\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 10.0999 - val_loss: 9.9270\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 10.0899 - val_loss: 9.7910\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 10.0645 - val_loss: 9.8831\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 10.0881 - val_loss: 10.2833\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 10.1070 - val_loss: 9.9942\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 10.0897 - val_loss: 9.9129\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 10.1001 - val_loss: 10.1227\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 10.0941 - val_loss: 9.9109\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 10.0916 - val_loss: 9.8848\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 10.0878 - val_loss: 9.9912\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 10.0935 - val_loss: 9.7955\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 10.0728 - val_loss: 9.9904\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 10.0873 - val_loss: 10.0685\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 10.0676 - val_loss: 9.9938\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 10.0563 - val_loss: 9.7757\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 10.0356 - val_loss: 10.0087\n",
      "Epoch 33/128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 24s 94ms/step - loss: 10.0333 - val_loss: 9.9850\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 10.0410 - val_loss: 9.7239\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 10.0198 - val_loss: 9.7287\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 10.0324 - val_loss: 9.8978\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.9987 - val_loss: 9.9544\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 10.0196 - val_loss: 9.7864\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 10.0052 - val_loss: 9.8850\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.9989 - val_loss: 9.7096\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 10.0076 - val_loss: 9.7956\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.9644 - val_loss: 9.8021\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.9667 - val_loss: 9.7730\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.9631 - val_loss: 9.7589\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.9738 - val_loss: 9.6366\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.9457 - val_loss: 9.7123\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.9275 - val_loss: 9.7180\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.9403 - val_loss: 9.8266\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.9386 - val_loss: 9.6935\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.9335 - val_loss: 9.6803\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.9736 - val_loss: 9.7045\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.9022 - val_loss: 9.7327\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.9147 - val_loss: 10.0329\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.9370 - val_loss: 9.7838\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.9084 - val_loss: 9.5831\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.8891 - val_loss: 9.7104\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.9226 - val_loss: 9.8163\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.9086 - val_loss: 9.6309\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.8971 - val_loss: 9.7504\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.8675 - val_loss: 9.6742\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 9.8950 - val_loss: 9.7187\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.8868 - val_loss: 9.9594\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.8744 - val_loss: 9.5911\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.8898 - val_loss: 9.9578\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.8717 - val_loss: 9.6079\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.8578 - val_loss: 9.7072\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.8457 - val_loss: 10.0539\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.8444 - val_loss: 9.6776\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.8520 - val_loss: 9.6406\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.8415 - val_loss: 9.7242\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.8512 - val_loss: 9.7303\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.8724 - val_loss: 9.7996\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.8517 - val_loss: 9.7103\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.8627 - val_loss: 9.6462\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.8424 - val_loss: 9.5690\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.8611 - val_loss: 9.7106\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.8426 - val_loss: 9.6116\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.8466 - val_loss: 9.8548\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.8224 - val_loss: 9.6031\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.7997 - val_loss: 9.8235\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.8318 - val_loss: 9.6044\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.7972 - val_loss: 9.6932\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.8265 - val_loss: 9.7482\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.7935 - val_loss: 9.6395\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.7914 - val_loss: 9.7022\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.8071 - val_loss: 9.8574\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.8019 - val_loss: 9.5920\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.8013 - val_loss: 9.7203\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.8159 - val_loss: 9.5369\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.7944 - val_loss: 9.9589\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.7877 - val_loss: 9.6857\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.7976 - val_loss: 9.9741\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.7911 - val_loss: 9.6021\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.7621 - val_loss: 9.5821\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.7563 - val_loss: 9.7293\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.7604 - val_loss: 9.8616\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.7828 - val_loss: 9.6834\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.7745 - val_loss: 9.7970\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.7552 - val_loss: 9.5461\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.7823 - val_loss: 9.8284\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.7735 - val_loss: 9.9343\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.7559 - val_loss: 9.6670\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.7408 - val_loss: 9.6559\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.7159 - val_loss: 9.7538\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.7372 - val_loss: 9.5915\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.7560 - val_loss: 9.7913\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.7263 - val_loss: 9.7613\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.7528 - val_loss: 9.6316\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.7412 - val_loss: 10.0382\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.7400 - val_loss: 9.5293\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.7190 - val_loss: 9.7492\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.7088 - val_loss: 9.5013\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.7032 - val_loss: 10.2715\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.7501 - val_loss: 9.6210\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.7245 - val_loss: 9.5229\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.7250 - val_loss: 9.4940\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.7096 - val_loss: 9.9031\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.7121 - val_loss: 9.6859\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.7200 - val_loss: 9.8090\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.7039 - val_loss: 9.5331\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.6892 - val_loss: 9.7749\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.7374 - val_loss: 9.6614\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.7157 - val_loss: 9.7107\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.6908 - val_loss: 9.6220\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.6991 - val_loss: 9.8277\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.7072 - val_loss: 9.6366\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.6847 - val_loss: 9.7791\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 9.7221 - val_loss: 9.8173\n",
      "\n",
      "Loss: 981.73%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 8)         80        \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 94, 94, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 94, 94, 8)         0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 239)       17447     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 92, 92, 239)      956       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 87)        187224    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 90, 90, 87)       348       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 136)       106624    \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 88, 88, 136)      544       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 136)       166600    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 479,859\n",
      "Trainable params: 478,917\n",
      "Non-trainable params: 942\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 24s 89ms/step - loss: 1190.1958 - val_loss: 82.3124\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.8355 - val_loss: 13.0744\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.2487 - val_loss: 11.6197\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.2371 - val_loss: 10.5157\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.2029 - val_loss: 11.7453\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.1599 - val_loss: 10.1227\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.1103 - val_loss: 10.8456\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.1235 - val_loss: 9.9774\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0834 - val_loss: 9.7758\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.1260 - val_loss: 10.5842\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.1233 - val_loss: 9.9594\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.1563 - val_loss: 10.2713\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0850 - val_loss: 9.7406\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.1234 - val_loss: 10.1747\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0948 - val_loss: 10.0267\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.1512 - val_loss: 10.4407\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.1104 - val_loss: 9.8833\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0879 - val_loss: 10.0997\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.1142 - val_loss: 9.9915\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.1001 - val_loss: 10.4070\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.1382 - val_loss: 10.0527\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0992 - val_loss: 10.5821\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.1078 - val_loss: 10.0463\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0971 - val_loss: 9.7993\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.1418 - val_loss: 10.2649\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0994 - val_loss: 9.8085\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0794 - val_loss: 9.9121\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0962 - val_loss: 10.0810\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.1234 - val_loss: 10.2535\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0857 - val_loss: 9.9377\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.1022 - val_loss: 10.4614\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.1438 - val_loss: 9.9667\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.1283 - val_loss: 9.8554\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.1003 - val_loss: 9.8973\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.1335 - val_loss: 10.2961\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.1073 - val_loss: 10.0673\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0907 - val_loss: 10.2627\n",
      "Epoch 38/128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 21s 85ms/step - loss: 10.1121 - val_loss: 10.1045\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0968 - val_loss: 10.0993\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.1118 - val_loss: 9.8370\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.1089 - val_loss: 10.4427\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.1054 - val_loss: 10.0672\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.1043 - val_loss: 9.7860\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.1129 - val_loss: 10.0911\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0997 - val_loss: 10.1001\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0945 - val_loss: 9.8638\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0865 - val_loss: 9.8970\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0830 - val_loss: 9.9432\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.1215 - val_loss: 9.7826\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0983 - val_loss: 10.0965\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.1273 - val_loss: 9.8354\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.1091 - val_loss: 9.8934\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0864 - val_loss: 10.2526\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0795 - val_loss: 9.7964\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0891 - val_loss: 9.9987\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0912 - val_loss: 10.0351\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0996 - val_loss: 10.1929\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0791 - val_loss: 10.1973\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0853 - val_loss: 10.1796\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0747 - val_loss: 10.0035\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.1144 - val_loss: 9.9774\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0851 - val_loss: 9.7397\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0884 - val_loss: 10.0128\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0990 - val_loss: 9.8632\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0782 - val_loss: 9.9847\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0905 - val_loss: 9.8356\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0532 - val_loss: 10.0134\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0660 - val_loss: 9.9765\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0933 - val_loss: 9.8245\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0887 - val_loss: 9.9340\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0883 - val_loss: 9.8950\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0552 - val_loss: 10.1080\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0777 - val_loss: 9.9295\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0482 - val_loss: 10.0538\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.1045 - val_loss: 9.8401\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0964 - val_loss: 9.9566\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.1221 - val_loss: 9.7219\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0509 - val_loss: 9.7895\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0461 - val_loss: 9.9024\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.1003 - val_loss: 9.8295\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0885 - val_loss: 9.8436\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0765 - val_loss: 10.1079\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0477 - val_loss: 9.7761\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0830 - val_loss: 9.7570\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0799 - val_loss: 9.8389\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0623 - val_loss: 9.7739\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0612 - val_loss: 10.7681\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0837 - val_loss: 9.7889\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0469 - val_loss: 9.9355\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0579 - val_loss: 9.8130\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0713 - val_loss: 9.7878\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0542 - val_loss: 9.7806\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0733 - val_loss: 9.8095\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.1100 - val_loss: 9.8042\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0599 - val_loss: 9.8109\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0684 - val_loss: 9.8414\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0765 - val_loss: 9.9480\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0738 - val_loss: 9.7330\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0787 - val_loss: 9.8614\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0476 - val_loss: 9.7331\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0493 - val_loss: 9.9839\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0746 - val_loss: 10.1527\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0587 - val_loss: 9.8799\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0764 - val_loss: 9.7161\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0541 - val_loss: 9.9636\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0783 - val_loss: 9.9747\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0608 - val_loss: 10.1501\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0562 - val_loss: 10.0067\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0597 - val_loss: 9.9686\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0574 - val_loss: 9.8024\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0975 - val_loss: 9.9034\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0275 - val_loss: 9.9842\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0254 - val_loss: 10.0083\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0582 - val_loss: 9.8804\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0536 - val_loss: 9.7875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 116/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0372 - val_loss: 9.7715\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0583 - val_loss: 9.7146\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0373 - val_loss: 9.8927\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0747 - val_loss: 9.9518\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0292 - val_loss: 9.9269\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0507 - val_loss: 9.7633\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0767 - val_loss: 10.2850\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0634 - val_loss: 9.8198\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0726 - val_loss: 10.1480\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0491 - val_loss: 9.8477\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0557 - val_loss: 9.7249\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0263 - val_loss: 9.8117\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 10.0547 - val_loss: 10.0046\n",
      "\n",
      "Loss: 1000.46%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 8)         80        \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 94, 94, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 94, 94, 8)         0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 8)         584       \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 92, 92, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 64)        4672      \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 90, 90, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 136)       78472     \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 88, 88, 136)      544       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 136)       166600    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 251,276\n",
      "Trainable params: 250,842\n",
      "Non-trainable params: 434\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 14s 50ms/step - loss: 1177.8556 - val_loss: 15.9625\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 11.7745 - val_loss: 18.3661\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.9844 - val_loss: 19.8566\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.8850 - val_loss: 15.3420\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.5622 - val_loss: 11.0647\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.5087 - val_loss: 11.6666\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.4501 - val_loss: 11.4935\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.4602 - val_loss: 12.0709\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.3150 - val_loss: 10.3780\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.2987 - val_loss: 10.4517\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.2790 - val_loss: 10.6642\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.3208 - val_loss: 10.1061\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.2319 - val_loss: 10.8329\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.2659 - val_loss: 9.9610\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.2153 - val_loss: 10.1251\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.2709 - val_loss: 10.1245\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.2205 - val_loss: 10.5551\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.2275 - val_loss: 10.1097\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.2016 - val_loss: 10.1134\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.1986 - val_loss: 10.1138\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.2121 - val_loss: 10.1051\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.1700 - val_loss: 10.0487\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.1643 - val_loss: 9.9636\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.1382 - val_loss: 10.6080\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.2015 - val_loss: 10.2113\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.1480 - val_loss: 9.8988\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.1708 - val_loss: 9.8488\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.1822 - val_loss: 10.0285\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.1533 - val_loss: 9.8221\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.1049 - val_loss: 9.8569\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.1296 - val_loss: 9.9336\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.1419 - val_loss: 9.8633\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.1735 - val_loss: 10.2380\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.1877 - val_loss: 9.9436\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.1159 - val_loss: 9.8972\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.1195 - val_loss: 10.0977\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.1488 - val_loss: 10.3647\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.1462 - val_loss: 10.0241\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.1203 - val_loss: 9.8544\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.1352 - val_loss: 10.0907\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.1227 - val_loss: 9.8197\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.1174 - val_loss: 9.7528\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.0930 - val_loss: 9.8854\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.1364 - val_loss: 10.5900\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.1265 - val_loss: 9.8094\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.1387 - val_loss: 9.8037\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.1072 - val_loss: 9.8645\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.1101 - val_loss: 9.7290\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.1039 - val_loss: 9.7573\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.1194 - val_loss: 9.8914\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.0901 - val_loss: 9.8652\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.1550 - val_loss: 9.8257\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.1273 - val_loss: 9.9166\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.0949 - val_loss: 9.8477\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.0936 - val_loss: 9.9366\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.1014 - val_loss: 9.9736\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.0877 - val_loss: 9.8171\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.0930 - val_loss: 9.7207\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.1041 - val_loss: 9.8223\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.0864 - val_loss: 9.8939\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.0771 - val_loss: 9.7344\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.0784 - val_loss: 9.7930\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.0879 - val_loss: 9.8096\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 12s 47ms/step - loss: 10.0708 - val_loss: 9.9134\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.0694 - val_loss: 9.7312\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.0502 - val_loss: 9.7942\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.0640 - val_loss: 9.9328\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.0853 - val_loss: 9.8485\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.0754 - val_loss: 9.8689\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 12s 47ms/step - loss: 10.0474 - val_loss: 9.7981\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.0578 - val_loss: 9.8353\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.0714 - val_loss: 9.7882\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 12s 47ms/step - loss: 10.0585 - val_loss: 9.8564\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.0407 - val_loss: 9.7358\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.0694 - val_loss: 9.8071\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.0652 - val_loss: 9.8055\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.0565 - val_loss: 9.7222\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.0746 - val_loss: 9.8438\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.0466 - val_loss: 9.7571\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.0379 - val_loss: 10.2396\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 12s 47ms/step - loss: 10.0484 - val_loss: 9.8797\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 12s 47ms/step - loss: 10.0326 - val_loss: 9.7023\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 12s 47ms/step - loss: 10.0613 - val_loss: 9.7701\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 12s 47ms/step - loss: 10.0583 - val_loss: 9.8696\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.0654 - val_loss: 9.7789\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 12s 47ms/step - loss: 10.0521 - val_loss: 9.8815\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.0483 - val_loss: 9.8296\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.0318 - val_loss: 9.9045\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 12s 47ms/step - loss: 10.0523 - val_loss: 9.9724\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.0530 - val_loss: 9.7390\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 12s 47ms/step - loss: 10.0287 - val_loss: 9.8701\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 12s 47ms/step - loss: 10.0512 - val_loss: 9.8772\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.0679 - val_loss: 9.9793\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 12s 47ms/step - loss: 10.0947 - val_loss: 9.9116\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.0533 - val_loss: 9.8631\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.0414 - val_loss: 9.8575\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.0517 - val_loss: 9.7811\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.0294 - val_loss: 9.8332\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.0695 - val_loss: 10.1241\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.0693 - val_loss: 10.0285\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.0903 - val_loss: 9.7533\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.0326 - val_loss: 10.0150\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 12s 47ms/step - loss: 10.0463 - val_loss: 9.8175\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.0371 - val_loss: 9.9770\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.0398 - val_loss: 9.7622\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.0505 - val_loss: 10.0175\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.0374 - val_loss: 9.9017\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.0303 - val_loss: 9.7111\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.0327 - val_loss: 9.8516\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 12s 47ms/step - loss: 10.0414 - val_loss: 9.7600\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.0535 - val_loss: 9.9305\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.0442 - val_loss: 9.8043\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.0113 - val_loss: 9.9278\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.0325 - val_loss: 9.7799\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.0515 - val_loss: 9.7793\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.0375 - val_loss: 9.7585\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.0285 - val_loss: 9.7649\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.0379 - val_loss: 10.0289\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.0644 - val_loss: 9.8036\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.0383 - val_loss: 9.6941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 121/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.0529 - val_loss: 9.8297\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.0462 - val_loss: 9.9070\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.0374 - val_loss: 9.9167\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.0648 - val_loss: 9.7695\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.0297 - val_loss: 9.7472\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.0185 - val_loss: 9.7507\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.0367 - val_loss: 10.0152\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 10.0210 - val_loss: 9.8349\n",
      "\n",
      "Loss: 983.49%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 250)       2500      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 94, 94, 250)      1000      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 94, 94, 250)       0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 243)       546993    \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 243)       531684    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 90, 90, 243)      972       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 90, 90, 243)       0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 22)        48136     \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 22)        4378      \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 86, 86, 22)       88        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 84, 84, 31)        6169      \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 84, 84, 31)       124       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 82, 82, 25)        7000      \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 82, 82, 25)       100       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 80, 80, 136)       30736     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,179,884\n",
      "Trainable params: 1,178,740\n",
      "Non-trainable params: 1,144\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 42s 155ms/step - loss: 2221.6802 - val_loss: 787.7327\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 228.8292 - val_loss: 26.1859\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 37s 149ms/step - loss: 14.3415 - val_loss: 10.1766\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 37s 149ms/step - loss: 10.0769 - val_loss: 9.9010\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 37s 149ms/step - loss: 9.9531 - val_loss: 9.7092\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 37s 149ms/step - loss: 9.9424 - val_loss: 9.7152\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 37s 149ms/step - loss: 9.9489 - val_loss: 9.6868\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 37s 149ms/step - loss: 9.9343 - val_loss: 9.7382\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.9308 - val_loss: 9.7513\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.9305 - val_loss: 9.7197\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 37s 149ms/step - loss: 9.9301 - val_loss: 9.8361\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 37s 149ms/step - loss: 9.9222 - val_loss: 9.6969\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 37s 149ms/step - loss: 9.9187 - val_loss: 9.6444\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 37s 149ms/step - loss: 9.9025 - val_loss: 9.7025\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 37s 149ms/step - loss: 9.8986 - val_loss: 9.8388\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.9018 - val_loss: 9.6591\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 37s 149ms/step - loss: 9.9014 - val_loss: 9.6401\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 37s 149ms/step - loss: 9.8895 - val_loss: 9.6449\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.8889 - val_loss: 9.8171\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.8803 - val_loss: 9.7391\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.8750 - val_loss: 9.7288\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.8576 - val_loss: 9.9062\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.8550 - val_loss: 9.7113\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.8378 - val_loss: 9.6683\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.8270 - val_loss: 10.3151\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.8177 - val_loss: 9.7044\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.8155 - val_loss: 9.5181\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.7947 - val_loss: 9.6725\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.7459 - val_loss: 9.6130\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.7406 - val_loss: 9.5753\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.7278 - val_loss: 9.4354\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.7092 - val_loss: 9.5752\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.6778 - val_loss: 9.4673\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.6416 - val_loss: 9.3699\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.6185 - val_loss: 9.6501\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.6176 - val_loss: 9.8697\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.5760 - val_loss: 9.5441\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.5851 - val_loss: 9.4171\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.5572 - val_loss: 9.3020\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.5496 - val_loss: 9.4028\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.5038 - val_loss: 9.5330\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.4905 - val_loss: 9.9923\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.5110 - val_loss: 9.7170\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.4503 - val_loss: 9.2721\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.4308 - val_loss: 9.3118\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.4297 - val_loss: 9.2852\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.4039 - val_loss: 9.4363\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.4183 - val_loss: 9.1931\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.3774 - val_loss: 9.5662\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.3710 - val_loss: 9.1919\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.3358 - val_loss: 9.2053\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.3247 - val_loss: 9.2230\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.3082 - val_loss: 9.3247\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.3016 - val_loss: 9.7409\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.3117 - val_loss: 9.2760\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.3064 - val_loss: 9.0150\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.3058 - val_loss: 12.4005\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.3075 - val_loss: 9.3795\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.2805 - val_loss: 9.2259\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.2528 - val_loss: 9.1618\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.2617 - val_loss: 9.5381\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.2266 - val_loss: 9.1905\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.2000 - val_loss: 9.1234\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.2046 - val_loss: 9.1005\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.2010 - val_loss: 9.0329\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.1859 - val_loss: 9.2445\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.1695 - val_loss: 9.0103\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.1982 - val_loss: 9.1458\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.1509 - val_loss: 9.0609\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.1317 - val_loss: 9.4221\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.1337 - val_loss: 9.4149\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.1760 - val_loss: 9.0372\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.1440 - val_loss: 8.9639\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.1268 - val_loss: 9.2461\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.1214 - val_loss: 9.2386\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.1132 - val_loss: 8.9824\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.1156 - val_loss: 8.9678\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.1147 - val_loss: 9.6396\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.1047 - val_loss: 8.9961\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.0889 - val_loss: 9.1250\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.0549 - val_loss: 9.3546\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.0675 - val_loss: 9.1975\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.0689 - val_loss: 8.9676\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.0417 - val_loss: 9.6366\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.0493 - val_loss: 9.1879\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.0572 - val_loss: 9.0476\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.0289 - val_loss: 9.0437\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.0382 - val_loss: 8.9041\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.0106 - val_loss: 9.0383\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.0294 - val_loss: 9.0251\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.0150 - val_loss: 8.9748\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.0163 - val_loss: 8.8872\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 8.9831 - val_loss: 8.9166\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 8.9988 - val_loss: 9.0571\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 9.0220 - val_loss: 9.5488\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 8.9904 - val_loss: 8.9880\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 8.9802 - val_loss: 9.0051\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 8.9834 - val_loss: 9.2704\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 8.9544 - val_loss: 8.9089\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 8.9728 - val_loss: 9.0013\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 8.9522 - val_loss: 8.9969\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 8.9521 - val_loss: 9.3089\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 8.9745 - val_loss: 8.9592\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 8.9377 - val_loss: 8.9881\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 37s 149ms/step - loss: 8.9376 - val_loss: 9.0040\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 8.9248 - val_loss: 9.0541\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 8.9525 - val_loss: 9.0716\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 8.9237 - val_loss: 9.2491\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 8.9612 - val_loss: 9.3484\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 8.9219 - val_loss: 8.8830\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 8.8970 - val_loss: 9.2682\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 8.9098 - val_loss: 9.0465\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 8.9219 - val_loss: 8.9734\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 8.9042 - val_loss: 9.4611\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 8.8923 - val_loss: 8.9389\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 8.9104 - val_loss: 9.0284\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 8.8728 - val_loss: 9.3453\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 8.8796 - val_loss: 9.1248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 119/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 8.8832 - val_loss: 9.2580\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 8.8922 - val_loss: 8.8984\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 8.8781 - val_loss: 8.9639\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 8.8626 - val_loss: 9.0828\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 8.8796 - val_loss: 8.9795\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 8.8749 - val_loss: 8.9350\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 8.8609 - val_loss: 9.5392\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 8.8583 - val_loss: 8.8848\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 8.8358 - val_loss: 8.8917\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 8.8467 - val_loss: 9.1133\n",
      "\n",
      "Loss: 911.33%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 8)         80        \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 8)         584       \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 92, 92, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 92, 92, 8)         0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 156)       11388     \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 156)       219180    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 88, 88, 156)      624       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 88, 88, 156)       0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 256)       359680    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 86, 86, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 86, 86, 256)       0         \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 84, 84, 8)         18440     \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 84, 84, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 82, 82, 136)       9928      \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 82, 82, 136)      544       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 80, 80, 136)       166600    \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 80, 80, 136)      544       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 78, 78, 136)       166600    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 955,284\n",
      "Trainable params: 953,882\n",
      "Non-trainable params: 1,402\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 36s 138ms/step - loss: 1199.3059 - val_loss: 18.6347\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.8731 - val_loss: 10.3079\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.2242 - val_loss: 11.0923\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.1547 - val_loss: 13.4770\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.1187 - val_loss: 11.4764\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.1375 - val_loss: 11.0809\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.1455 - val_loss: 16.6030\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.1011 - val_loss: 225.1040\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.0994 - val_loss: 39.9211\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.1046 - val_loss: 15.7553\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 34s 135ms/step - loss: 10.0744 - val_loss: 36.1425\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.0923 - val_loss: 91.7900\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.0894 - val_loss: 13.3700\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.0813 - val_loss: 57.8818\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.1290 - val_loss: 26.5455\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.1226 - val_loss: 50.8715\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.0948 - val_loss: 35.2492\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.1129 - val_loss: 14.7634\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.1105 - val_loss: 12.4283\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.0981 - val_loss: 13.9094\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.0789 - val_loss: 11.8578\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.1201 - val_loss: 14.2946\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.0770 - val_loss: 17.1617\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.1281 - val_loss: 12.3761\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.0971 - val_loss: 10.5088\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.0792 - val_loss: 12.0537\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.1119 - val_loss: 11.2782\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.0879 - val_loss: 13.1523\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.0829 - val_loss: 19.2633\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.0984 - val_loss: 11.0319\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.0974 - val_loss: 11.9015\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.0793 - val_loss: 11.3908\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.0709 - val_loss: 11.4508\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.1032 - val_loss: 10.0357\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.0860 - val_loss: 11.8587\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.0899 - val_loss: 10.6316\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.0724 - val_loss: 10.1035\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.0799 - val_loss: 10.2010\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.1194 - val_loss: 10.1961\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.0696 - val_loss: 9.9487\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.0901 - val_loss: 10.2032\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.0958 - val_loss: 9.8571\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.0890 - val_loss: 10.8682\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.0716 - val_loss: 10.0061\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.0826 - val_loss: 9.9041\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.0664 - val_loss: 10.0840\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.0657 - val_loss: 9.9091\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.0529 - val_loss: 10.0327\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.0596 - val_loss: 9.9023\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.0481 - val_loss: 9.9855\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.0824 - val_loss: 9.7880\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.0505 - val_loss: 9.9157\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.0725 - val_loss: 9.9886\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.0616 - val_loss: 10.0060\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.0521 - val_loss: 9.9465\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.0539 - val_loss: 9.7554\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.0472 - val_loss: 9.9244\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.0699 - val_loss: 9.8832\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.0554 - val_loss: 10.0265\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.0365 - val_loss: 9.8957\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.0750 - val_loss: 9.7614\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.0681 - val_loss: 9.8870\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.0318 - val_loss: 9.7787\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.0150 - val_loss: 9.9079\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.0390 - val_loss: 9.9836\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.0485 - val_loss: 9.8312\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.0561 - val_loss: 9.8416\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.0498 - val_loss: 10.0087\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.0499 - val_loss: 10.1584\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.0422 - val_loss: 9.8790\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.0769 - val_loss: 9.9106\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.0484 - val_loss: 9.8013\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.0386 - val_loss: 10.0542\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.0285 - val_loss: 9.8240\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.0409 - val_loss: 10.0320\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.0100 - val_loss: 9.8554\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.0220 - val_loss: 9.8995\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.0282 - val_loss: 9.8660\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.0309 - val_loss: 9.8654\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.0126 - val_loss: 9.8176\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.0264 - val_loss: 9.8399\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.0099 - val_loss: 10.3114\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.0421 - val_loss: 9.8465\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.0566 - val_loss: 9.9664\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.0123 - val_loss: 9.8738\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.0458 - val_loss: 9.7973\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 9.9910 - val_loss: 9.9969\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 9.9900 - val_loss: 9.8669\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 9.9947 - val_loss: 9.8260\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.0096 - val_loss: 9.9125\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 9.9585 - val_loss: 9.8562\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 9.9868 - val_loss: 9.7948\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 10.0026 - val_loss: 9.9002\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 9.9932 - val_loss: 9.9685\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 9.9918 - val_loss: 9.8996\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 9.9752 - val_loss: 9.9112\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 9.9778 - val_loss: 10.0798\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 9.9951 - val_loss: 10.2527\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 9.9552 - val_loss: 9.8843\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 9.9796 - val_loss: 10.0643\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 9.9414 - val_loss: 10.1381\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 9.9608 - val_loss: 9.9723\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 9.9442 - val_loss: 10.1868\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 9.9542 - val_loss: 10.5562\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 9.9667 - val_loss: 10.1509\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 9.9842 - val_loss: 10.4982\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 9.9280 - val_loss: 9.9980\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 9.9471 - val_loss: 9.9416\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 9.9178 - val_loss: 10.0008\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 9.9434 - val_loss: 10.0906\n",
      "Epoch 111/128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 34s 136ms/step - loss: 9.9162 - val_loss: 9.9435\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 9.9327 - val_loss: 10.0467\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 9.9171 - val_loss: 10.3650\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 9.9395 - val_loss: 10.1639\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 9.9253 - val_loss: 9.9779\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 9.9231 - val_loss: 10.0554\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 9.9078 - val_loss: 10.5589\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 9.9162 - val_loss: 10.1708\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 9.9284 - val_loss: 10.0750\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 9.8830 - val_loss: 10.3053\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 9.9062 - val_loss: 10.1741\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 9.8679 - val_loss: 10.7918\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 9.8874 - val_loss: 10.3293\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 9.8891 - val_loss: 10.4131\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 9.8853 - val_loss: 10.6282\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 9.8706 - val_loss: 10.9531\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 9.8750 - val_loss: 10.4168\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 34s 136ms/step - loss: 9.8584 - val_loss: 10.5495\n",
      "\n",
      "Loss: 1054.95%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 256)       2560      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 92, 92, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 92, 92, 256)       0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 256)       590080    \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 88, 88, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 88, 88, 256)       0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 8)         18440     \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 84, 84, 8)         584       \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 84, 84, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 82, 82, 42)        3066      \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 82, 82, 42)       168       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 80, 80, 136)       51544     \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 80, 80, 136)      544       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 78, 78, 136)       166600    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,015,830\n",
      "Trainable params: 2,014,432\n",
      "Non-trainable params: 1,398\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 50s 193ms/step - loss: 1192.4800 - val_loss: 17.9230\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 11.0385 - val_loss: 11.1768\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 10.3078 - val_loss: 10.2359\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 48s 192ms/step - loss: 10.3609 - val_loss: 11.8365\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 48s 192ms/step - loss: 10.1822 - val_loss: 10.5501\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 48s 192ms/step - loss: 10.2205 - val_loss: 10.6321\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 48s 192ms/step - loss: 10.2006 - val_loss: 10.0795\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 48s 192ms/step - loss: 10.1151 - val_loss: 10.1532\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 48s 192ms/step - loss: 10.1441 - val_loss: 10.3587\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 48s 192ms/step - loss: 10.1409 - val_loss: 11.8152\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 10.1232 - val_loss: 10.3143\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 48s 192ms/step - loss: 10.1556 - val_loss: 9.8600\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 48s 192ms/step - loss: 10.1018 - val_loss: 10.6435\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 48s 192ms/step - loss: 10.1660 - val_loss: 9.9082\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 48s 192ms/step - loss: 10.1453 - val_loss: 10.8049\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 48s 192ms/step - loss: 10.1235 - val_loss: 9.9440\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 10.1228 - val_loss: 9.9805\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 48s 192ms/step - loss: 10.1001 - val_loss: 9.8951\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 48s 192ms/step - loss: 10.0922 - val_loss: 9.9289\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 48s 192ms/step - loss: 10.1198 - val_loss: 10.0132\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 48s 192ms/step - loss: 10.1152 - val_loss: 9.9763\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 48s 192ms/step - loss: 10.1267 - val_loss: 10.2301\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 48s 192ms/step - loss: 10.1410 - val_loss: 10.2051\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 48s 192ms/step - loss: 10.1287 - val_loss: 10.3655\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 48s 192ms/step - loss: 10.1426 - val_loss: 9.7807\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 48s 192ms/step - loss: 10.1355 - val_loss: 10.1668\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 48s 192ms/step - loss: 10.1175 - val_loss: 10.0732\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 48s 192ms/step - loss: 10.1266 - val_loss: 10.0540\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 48s 192ms/step - loss: 10.1213 - val_loss: 10.0137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/128\n",
      "250/250 [==============================] - 48s 192ms/step - loss: 10.1108 - val_loss: 9.9810\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 48s 192ms/step - loss: 10.0917 - val_loss: 9.8827\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 48s 192ms/step - loss: 10.1277 - val_loss: 10.5673\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 48s 192ms/step - loss: 10.1055 - val_loss: 10.0674\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 48s 192ms/step - loss: 10.0927 - val_loss: 9.9152\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 48s 192ms/step - loss: 10.1124 - val_loss: 10.0857\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 48s 192ms/step - loss: 10.1469 - val_loss: 10.0005\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 48s 192ms/step - loss: 10.0971 - val_loss: 9.8099\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 48s 192ms/step - loss: 10.1053 - val_loss: 11.3827\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 48s 192ms/step - loss: 10.0896 - val_loss: 10.2730\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 48s 192ms/step - loss: 10.1438 - val_loss: 9.9839\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 48s 192ms/step - loss: 10.1299 - val_loss: 9.8663\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 48s 192ms/step - loss: 10.0965 - val_loss: 9.9994\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 48s 192ms/step - loss: 10.0993 - val_loss: 9.9564\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 48s 192ms/step - loss: 10.0840 - val_loss: 9.8192\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 48s 192ms/step - loss: 10.0877 - val_loss: 10.3565\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 48s 192ms/step - loss: 10.0916 - val_loss: 10.5011\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 48s 192ms/step - loss: 10.1186 - val_loss: 10.1829\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 48s 192ms/step - loss: 10.1068 - val_loss: 9.9555\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 48s 192ms/step - loss: 10.0829 - val_loss: 9.8100\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 48s 192ms/step - loss: 10.0577 - val_loss: 9.7924\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 48s 192ms/step - loss: 10.0641 - val_loss: 9.9836\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 48s 192ms/step - loss: 10.0827 - val_loss: 10.0749\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 10.0594 - val_loss: 9.9654\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 48s 192ms/step - loss: 10.0388 - val_loss: 9.7814\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 48s 192ms/step - loss: 10.0668 - val_loss: 9.9032\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 48s 192ms/step - loss: 10.0515 - val_loss: 9.8665\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 48s 192ms/step - loss: 10.0079 - val_loss: 9.8226\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 48s 192ms/step - loss: 10.0131 - val_loss: 10.1989\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 48s 192ms/step - loss: 10.0261 - val_loss: 9.9739\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 48s 192ms/step - loss: 9.9868 - val_loss: 9.8882\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 10.0450 - val_loss: 9.9759\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 9.9806 - val_loss: 10.0273\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 9.9900 - val_loss: 9.8153\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 9.9939 - val_loss: 9.8920\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 9.9730 - val_loss: 10.1588\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 9.9624 - val_loss: 9.9706\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 9.9754 - val_loss: 10.0930\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 9.9458 - val_loss: 9.8702\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 9.9259 - val_loss: 9.9065\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 9.9109 - val_loss: 9.8222\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 9.8793 - val_loss: 9.9407\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 9.8490 - val_loss: 9.9293\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 9.8651 - val_loss: 9.7393\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 9.8308 - val_loss: 9.6529\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 9.8294 - val_loss: 9.7090\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 9.7805 - val_loss: 9.6079\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 9.7657 - val_loss: 9.5780\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 9.7907 - val_loss: 9.6100\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 9.7357 - val_loss: 9.6608\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 9.6996 - val_loss: 9.5788\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 9.7391 - val_loss: 9.5174\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 9.6866 - val_loss: 9.5320\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 9.6849 - val_loss: 9.7188\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 9.6782 - val_loss: 9.5786\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 9.6238 - val_loss: 9.5633\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 9.6265 - val_loss: 10.0662\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 9.6296 - val_loss: 9.5677\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 9.6027 - val_loss: 9.5259\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 9.5517 - val_loss: 9.7301\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 9.5746 - val_loss: 9.6730\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 9.5864 - val_loss: 9.8345\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 9.5547 - val_loss: 9.8339\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 9.5053 - val_loss: 9.7765\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 9.5087 - val_loss: 9.4231\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 9.5014 - val_loss: 9.5266\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 9.4782 - val_loss: 9.6103\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 9.4556 - val_loss: 9.3701\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 9.4648 - val_loss: 10.1280\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 9.4267 - val_loss: 9.7529\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 9.4129 - val_loss: 9.5948\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 9.3843 - val_loss: 9.7239\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 9.4124 - val_loss: 9.5871\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 9.3998 - val_loss: 9.7819\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 9.4037 - val_loss: 9.5098\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 9.3840 - val_loss: 9.5857\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 9.3441 - val_loss: 9.4499\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 9.3540 - val_loss: 9.6789\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 9.3251 - val_loss: 9.3805\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 9.3250 - val_loss: 9.4667\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 9.3117 - val_loss: 9.4154\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 9.3041 - val_loss: 9.8795\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 9.3333 - val_loss: 9.5503\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 9.2586 - val_loss: 9.3935\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 9.3002 - val_loss: 9.4554\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 9.2712 - val_loss: 9.6238\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 9.2273 - val_loss: 9.5035\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 9.2860 - val_loss: 9.3294\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 9.2551 - val_loss: 9.6072\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 9.2368 - val_loss: 9.3710\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 9.2104 - val_loss: 9.3405\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 9.1963 - val_loss: 9.6516\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 9.2223 - val_loss: 9.2631\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 9.2260 - val_loss: 9.7797\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 9.1826 - val_loss: 9.5964\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 9.1963 - val_loss: 9.3957\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 9.1716 - val_loss: 9.5740\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 9.1556 - val_loss: 9.3846\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 9.1548 - val_loss: 9.3928\n",
      "\n",
      "Loss: 939.28%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 256)       2560      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 94, 94, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 94, 94, 256)       0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 92, 92, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 92, 92, 256)       0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 46)        106030    \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 46)        19090     \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 88, 88, 46)       184       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 88, 88, 46)        0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 8)         3320      \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 84, 84, 8)         584       \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 84, 84, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 84, 84, 8)         0         \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 82, 82, 8)         584       \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 82, 82, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 80, 80, 62)        4526      \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 80, 80, 62)       248       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 78, 78, 8)         4472      \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 78, 78, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 76, 76, 136)       9928      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 743,754\n",
      "Trainable params: 742,464\n",
      "Non-trainable params: 1,290\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 33s 124ms/step - loss: 2732.3264 - val_loss: 2604.8303\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 1200.2417 - val_loss: 553.5302\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 219.1480 - val_loss: 67.6534\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 34.0104 - val_loss: 16.7521\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 12.7352 - val_loss: 10.5233\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 10.2713 - val_loss: 9.8953\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.9908 - val_loss: 9.7610\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.9330 - val_loss: 9.7295\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.9369 - val_loss: 9.6843\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.9263 - val_loss: 9.6836\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.9396 - val_loss: 9.6842\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.9323 - val_loss: 9.7591\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.9283 - val_loss: 9.7208\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.9345 - val_loss: 9.7702\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.9293 - val_loss: 9.7240\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.9233 - val_loss: 9.6747\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.9275 - val_loss: 9.6709\n",
      "Epoch 18/128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 30s 120ms/step - loss: 9.9254 - val_loss: 9.6760\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.9199 - val_loss: 9.7692\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.9325 - val_loss: 9.7464\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.9238 - val_loss: 9.6980\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.9281 - val_loss: 9.6938\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.9231 - val_loss: 9.6946\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.9245 - val_loss: 9.6808\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.9242 - val_loss: 9.6739\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.9360 - val_loss: 9.7318\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.9337 - val_loss: 9.7176\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.9221 - val_loss: 9.7283\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.9155 - val_loss: 9.7470\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.9306 - val_loss: 9.6868\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.9267 - val_loss: 9.8575\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.9281 - val_loss: 9.7349\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.9263 - val_loss: 9.7587\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.9166 - val_loss: 9.7082\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.9137 - val_loss: 9.7055\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.9139 - val_loss: 9.7229\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.9195 - val_loss: 9.8409\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.9172 - val_loss: 9.7810\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.9060 - val_loss: 9.7502\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.9119 - val_loss: 9.7979\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.8953 - val_loss: 9.8849\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.9114 - val_loss: 9.8074\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.9053 - val_loss: 9.8894\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.8954 - val_loss: 9.7938\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.8945 - val_loss: 9.8081\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.8721 - val_loss: 9.9929\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.8733 - val_loss: 9.7963\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.8608 - val_loss: 9.9298\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.8595 - val_loss: 9.8982\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.8614 - val_loss: 10.0308\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.8596 - val_loss: 9.9067\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.8422 - val_loss: 10.0096\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.8563 - val_loss: 9.6603\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.8234 - val_loss: 9.8568\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.8305 - val_loss: 9.8560\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.8253 - val_loss: 9.8733\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.8159 - val_loss: 10.0063\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.8052 - val_loss: 9.8228\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.8097 - val_loss: 9.8485\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.7971 - val_loss: 9.6922\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.7942 - val_loss: 9.9932\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.7907 - val_loss: 9.7043\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.7876 - val_loss: 9.7876\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.7803 - val_loss: 9.7193\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.7715 - val_loss: 10.3047\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.7549 - val_loss: 10.0951\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.7626 - val_loss: 9.6817\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.7573 - val_loss: 9.6986\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.7528 - val_loss: 9.6236\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.7384 - val_loss: 9.6825\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.7348 - val_loss: 9.5978\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.7292 - val_loss: 10.3879\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.7209 - val_loss: 9.7406\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.7170 - val_loss: 9.7539\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.7103 - val_loss: 10.0312\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.7072 - val_loss: 9.6377\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.6867 - val_loss: 9.6939\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.6855 - val_loss: 9.6869\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.6841 - val_loss: 9.5925\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.6749 - val_loss: 9.8662\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.6579 - val_loss: 9.6681\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.6667 - val_loss: 9.5863\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.6513 - val_loss: 9.5847\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.6269 - val_loss: 9.7136\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.6474 - val_loss: 9.5510\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.6301 - val_loss: 9.7423\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.6169 - val_loss: 9.5830\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.6191 - val_loss: 9.5101\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.6181 - val_loss: 9.6462\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.6062 - val_loss: 9.5412\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.6013 - val_loss: 9.5589\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.6078 - val_loss: 9.5598\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.6026 - val_loss: 9.5685\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.5881 - val_loss: 9.6332\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.5687 - val_loss: 9.6665\n",
      "Epoch 96/128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 30s 120ms/step - loss: 9.5672 - val_loss: 9.5443\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.5539 - val_loss: 9.5831\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.5616 - val_loss: 9.7042\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.5548 - val_loss: 10.2671\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.5480 - val_loss: 9.6207\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.5275 - val_loss: 9.5810\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.5417 - val_loss: 9.5435\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.5534 - val_loss: 9.5276\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.5203 - val_loss: 9.5346\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.5171 - val_loss: 9.5961\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.5251 - val_loss: 9.5226\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.5122 - val_loss: 9.6945\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.5071 - val_loss: 9.6114\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.4908 - val_loss: 9.6661\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.5080 - val_loss: 9.6448\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.4971 - val_loss: 9.5025\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.4825 - val_loss: 9.5926\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.4861 - val_loss: 9.6333\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.4832 - val_loss: 9.6685\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.4672 - val_loss: 9.6351\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.4753 - val_loss: 9.6399\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.4575 - val_loss: 9.6137\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.4634 - val_loss: 9.5026\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.4699 - val_loss: 9.7496\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.4602 - val_loss: 9.8935\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.4593 - val_loss: 9.7217\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.4439 - val_loss: 9.6084\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.4410 - val_loss: 9.5500\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.4405 - val_loss: 10.0222\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.4481 - val_loss: 9.6387\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.4468 - val_loss: 9.6353\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.4261 - val_loss: 9.5451\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 30s 120ms/step - loss: 9.4288 - val_loss: 9.6173\n",
      "\n",
      "Loss: 961.73%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 256)       2560      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 92, 92, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 92, 92, 256)       0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 243)       560115    \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 243)       531684    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 88, 88, 243)      972       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 88, 88, 243)       0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 256)       560128    \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 84, 84, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 84, 84, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 82, 82, 112)       258160    \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 82, 82, 112)      448       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 80, 80, 8)         8072      \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 80, 80, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 78, 78, 136)       9928      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,114,311\n",
      "Trainable params: 3,112,559\n",
      "Non-trainable params: 1,752\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 70s 265ms/step - loss: 2733.9519 - val_loss: 2191.3284\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 1202.6483 - val_loss: 559.9404\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 219.3969 - val_loss: 77.7299\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 34.0462 - val_loss: 18.0993\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 12.8033 - val_loss: 10.6320\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 10.3385 - val_loss: 9.8665\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 10.0443 - val_loss: 9.8117\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 10.0138 - val_loss: 9.8489\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 9.9644 - val_loss: 9.7930\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 9.9566 - val_loss: 9.8109\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 9.9579 - val_loss: 9.7668\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 9.9676 - val_loss: 9.6838\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 9.9536 - val_loss: 9.8120\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 9.9440 - val_loss: 9.8113\n",
      "Epoch 15/128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 64s 257ms/step - loss: 9.9402 - val_loss: 9.7231\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 9.9430 - val_loss: 9.6833\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 9.9307 - val_loss: 10.0503\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 9.9351 - val_loss: 9.9393\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 9.9373 - val_loss: 9.9377\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 9.9241 - val_loss: 9.7783\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 9.9338 - val_loss: 9.7842\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 9.9358 - val_loss: 9.9061\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 9.9255 - val_loss: 9.6983\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 9.9237 - val_loss: 9.7181\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 9.9397 - val_loss: 9.7783\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 9.9337 - val_loss: 9.6598\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 9.9189 - val_loss: 9.7572\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 9.9313 - val_loss: 10.0337\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 9.9194 - val_loss: 9.6604\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 9.9242 - val_loss: 9.6591\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 9.9000 - val_loss: 9.6752\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 9.9004 - val_loss: 9.8528\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 9.9203 - val_loss: 9.6624\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 9.8843 - val_loss: 9.7169\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 9.8917 - val_loss: 9.6674\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 9.8899 - val_loss: 9.9273\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 9.8761 - val_loss: 9.8387\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 9.8632 - val_loss: 10.1247\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 9.8570 - val_loss: 9.6705\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 9.8537 - val_loss: 9.7041\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 9.8537 - val_loss: 9.8287\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 9.8673 - val_loss: 9.7091\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 9.8456 - val_loss: 9.6556\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 9.8338 - val_loss: 9.5885\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 9.8339 - val_loss: 9.8115\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 9.8377 - val_loss: 9.8098\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 9.8135 - val_loss: 9.6329\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 9.8043 - val_loss: 9.6040\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 9.7950 - val_loss: 9.5560\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 9.7824 - val_loss: 9.6128\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 9.7584 - val_loss: 9.5573\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 9.7459 - val_loss: 9.6490\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 9.7251 - val_loss: 9.4961\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 9.7054 - val_loss: 9.4529\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 9.6939 - val_loss: 9.4548\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 9.6783 - val_loss: 9.4582\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 9.6506 - val_loss: 9.4328\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 9.6223 - val_loss: 9.3144\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 9.6205 - val_loss: 9.3850\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 9.5820 - val_loss: 9.3897\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 9.5545 - val_loss: 9.3245\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 9.5180 - val_loss: 9.3012\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 9.4845 - val_loss: 9.9255\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 9.4397 - val_loss: 9.3192\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 9.4070 - val_loss: 9.2733\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 9.3669 - val_loss: 9.1221\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 9.3423 - val_loss: 9.2423\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 9.2895 - val_loss: 9.0717\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 9.2441 - val_loss: 9.0728\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 9.2310 - val_loss: 9.0005\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 9.1809 - val_loss: 9.1662\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 9.1746 - val_loss: 8.9751\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 9.1457 - val_loss: 9.0149\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 9.1123 - val_loss: 8.9809\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 9.0687 - val_loss: 8.8844\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 9.0679 - val_loss: 8.9847\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 9.0239 - val_loss: 9.0273\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 9.0073 - val_loss: 8.9583\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 8.9955 - val_loss: 8.8256\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 8.9629 - val_loss: 8.9018\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 8.9532 - val_loss: 8.9884\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 8.8917 - val_loss: 8.8163\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 8.8990 - val_loss: 8.7977\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 8.8582 - val_loss: 8.8180\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 8.8773 - val_loss: 8.8069\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 8.8106 - val_loss: 8.7563\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 8.7990 - val_loss: 8.8010\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 8.7796 - val_loss: 8.8618\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 8.7593 - val_loss: 8.6640\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 8.7466 - val_loss: 8.7093\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 8.7632 - val_loss: 8.6168\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 8.7167 - val_loss: 8.7710\n",
      "Epoch 93/128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 64s 257ms/step - loss: 8.7032 - val_loss: 8.7733\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 8.6869 - val_loss: 8.8742\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 8.6453 - val_loss: 8.7372\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 8.6383 - val_loss: 8.6992\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 8.6047 - val_loss: 8.7069\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 8.5955 - val_loss: 8.8372\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 8.5677 - val_loss: 8.6169\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 8.5706 - val_loss: 8.5553\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 8.5413 - val_loss: 8.5975\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 8.5220 - val_loss: 8.6231\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 8.5298 - val_loss: 8.5203\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 8.4947 - val_loss: 8.6275\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 8.4964 - val_loss: 8.5573\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 8.4450 - val_loss: 8.6132\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 8.4502 - val_loss: 8.5408\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 8.4370 - val_loss: 8.8538\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 8.4377 - val_loss: 8.4847\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 8.3964 - val_loss: 8.5701\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 8.3824 - val_loss: 8.5481\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 8.3312 - val_loss: 8.8427\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 8.3511 - val_loss: 8.5773\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 8.3748 - val_loss: 8.6076\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 8.3219 - val_loss: 8.5090\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 8.3141 - val_loss: 8.6082\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 8.2787 - val_loss: 8.5203\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 8.2971 - val_loss: 8.6561\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 8.2763 - val_loss: 8.6037\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 8.2555 - val_loss: 8.8467\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 8.2453 - val_loss: 8.7190\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 8.2281 - val_loss: 8.6492\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 8.2285 - val_loss: 8.7866\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 8.2157 - val_loss: 8.4719\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 8.1703 - val_loss: 8.5852\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 8.1473 - val_loss: 8.7489\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 8.1747 - val_loss: 8.6056\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 8.1763 - val_loss: 8.4327\n",
      "\n",
      "Loss: 843.27%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 8)         80        \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 94, 94, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 94, 94, 8)         0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 8)         584       \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 92, 92, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 70)        5110      \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 90, 90, 70)       280       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 8)         5048      \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 88, 88, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 136)       9928      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21,130\n",
      "Trainable params: 20,940\n",
      "Non-trainable params: 190\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 7s 25ms/step - loss: 2733.4321 - val_loss: 2016.2991\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 1203.2599 - val_loss: 469.6011\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 219.5729 - val_loss: 64.9833\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 33.8671 - val_loss: 16.3204\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 12.7193 - val_loss: 10.5299\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 10.3066 - val_loss: 9.8978\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9966 - val_loss: 9.7620\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9566 - val_loss: 9.7669\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9476 - val_loss: 9.7302\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9568 - val_loss: 9.7896\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9489 - val_loss: 9.7516\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9577 - val_loss: 9.7403\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9571 - val_loss: 9.7724\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9583 - val_loss: 9.7261\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9454 - val_loss: 9.6996\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9432 - val_loss: 9.7051\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9554 - val_loss: 9.7463\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9426 - val_loss: 9.7444\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9392 - val_loss: 9.7654\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9492 - val_loss: 9.8220\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9457 - val_loss: 9.7408\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9454 - val_loss: 9.7031\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9589 - val_loss: 9.7779\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9500 - val_loss: 9.7280\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9461 - val_loss: 9.7450\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9586 - val_loss: 9.6914\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9567 - val_loss: 9.7080\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9548 - val_loss: 9.7218\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9483 - val_loss: 9.7652\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9463 - val_loss: 9.8025\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9593 - val_loss: 9.8020\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9417 - val_loss: 9.6886\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9443 - val_loss: 9.7039\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9391 - val_loss: 9.7734\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9565 - val_loss: 9.7440\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9467 - val_loss: 9.7047\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9394 - val_loss: 9.7449\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9458 - val_loss: 9.7414\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9403 - val_loss: 9.8761\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9541 - val_loss: 9.7886\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9492 - val_loss: 9.7567\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9519 - val_loss: 9.7436\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9557 - val_loss: 9.7185\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9340 - val_loss: 9.7085\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9511 - val_loss: 9.7217\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9518 - val_loss: 9.7556\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9505 - val_loss: 9.6758\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9495 - val_loss: 9.7488\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9401 - val_loss: 9.8332\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9445 - val_loss: 9.7515\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9452 - val_loss: 9.7359\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9423 - val_loss: 9.7426\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9647 - val_loss: 9.9041\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9655 - val_loss: 9.7824\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9422 - val_loss: 9.7641\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9409 - val_loss: 9.7058\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9536 - val_loss: 9.7037\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9506 - val_loss: 9.7271\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9464 - val_loss: 9.7140\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9459 - val_loss: 9.6962\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9349 - val_loss: 9.7715\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9425 - val_loss: 9.6886\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9377 - val_loss: 9.7047\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9426 - val_loss: 9.7615\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9382 - val_loss: 9.7840\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9462 - val_loss: 9.7389\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9548 - val_loss: 9.8192\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9402 - val_loss: 9.7133\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9586 - val_loss: 9.7564\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9514 - val_loss: 9.6908\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9503 - val_loss: 9.7231\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9377 - val_loss: 9.7347\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9424 - val_loss: 9.7022\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9381 - val_loss: 9.7300\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9354 - val_loss: 9.7148\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9490 - val_loss: 9.7385\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9459 - val_loss: 9.7140\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9462 - val_loss: 9.8205\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9402 - val_loss: 9.7476\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9479 - val_loss: 9.7169\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9406 - val_loss: 9.7650\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9312 - val_loss: 9.6940\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9389 - val_loss: 9.7706\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9441 - val_loss: 9.6815\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9395 - val_loss: 9.6770\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9385 - val_loss: 9.9226\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9364 - val_loss: 9.7613\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9353 - val_loss: 9.7454\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9412 - val_loss: 9.7631\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9344 - val_loss: 9.7196\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9392 - val_loss: 9.7064\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9441 - val_loss: 9.7028\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9488 - val_loss: 9.7789\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9480 - val_loss: 9.7137\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9379 - val_loss: 9.7459\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9350 - val_loss: 9.7457\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9493 - val_loss: 9.7195\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9349 - val_loss: 9.6852\n",
      "Epoch 99/128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9350 - val_loss: 9.7539\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9378 - val_loss: 9.7192\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9392 - val_loss: 9.7541\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9473 - val_loss: 9.6987\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9338 - val_loss: 9.6844\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9325 - val_loss: 9.7047\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9450 - val_loss: 9.7513\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9455 - val_loss: 9.7704\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9396 - val_loss: 9.8303\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9332 - val_loss: 9.7833\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9261 - val_loss: 9.7202\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9315 - val_loss: 9.6995\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9279 - val_loss: 9.7141\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9377 - val_loss: 9.7286\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9337 - val_loss: 9.7345\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9402 - val_loss: 9.6951\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9419 - val_loss: 9.7335\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9243 - val_loss: 9.8844\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9491 - val_loss: 9.7018\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9307 - val_loss: 9.7123\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9372 - val_loss: 9.6969\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9352 - val_loss: 9.7320\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9413 - val_loss: 9.6807\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9411 - val_loss: 9.7503\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9330 - val_loss: 9.7264\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9282 - val_loss: 9.7606\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9426 - val_loss: 9.7249\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9348 - val_loss: 9.7189\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9301 - val_loss: 9.6775\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 9.9353 - val_loss: 9.7680\n",
      "\n",
      "Loss: 976.80%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 256)       2560      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 94, 94, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 94, 94, 256)       0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 92, 92, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 92, 92, 256)       0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 8)         18440     \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 8)         584       \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 88, 88, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 88, 88, 8)         0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 8)         584       \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 84, 84, 8)         584       \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 84, 84, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 82, 82, 8)         584       \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 82, 82, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 80, 80, 8)         584       \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 80, 80, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 78, 78, 136)       9928      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 626,108\n",
      "Trainable params: 625,018\n",
      "Non-trainable params: 1,090\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 26s 100ms/step - loss: 2730.4460 - val_loss: 2253.8533\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 1200.5499 - val_loss: 533.9567\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 220.5039 - val_loss: 72.3466\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 34.1866 - val_loss: 16.8620\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 12.7255 - val_loss: 10.5408\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 10.2558 - val_loss: 9.8327\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9612 - val_loss: 9.7662\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9219 - val_loss: 9.7382\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9162 - val_loss: 9.6936\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9166 - val_loss: 9.7660\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9155 - val_loss: 9.6978\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9180 - val_loss: 9.6885\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9192 - val_loss: 9.7113\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9168 - val_loss: 9.7344\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9146 - val_loss: 9.6866\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9246 - val_loss: 9.7564\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9167 - val_loss: 9.7231\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9170 - val_loss: 9.7245\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9181 - val_loss: 9.7453\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9299 - val_loss: 9.6987\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9178 - val_loss: 9.7104\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9231 - val_loss: 9.7240\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9208 - val_loss: 9.7307\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9201 - val_loss: 9.7386\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9178 - val_loss: 9.6929\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9194 - val_loss: 9.6848\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9312 - val_loss: 9.6984\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9184 - val_loss: 9.6991\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9198 - val_loss: 9.7449\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9208 - val_loss: 9.7330\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9220 - val_loss: 9.7627\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9274 - val_loss: 9.7633\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9242 - val_loss: 9.7232\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9278 - val_loss: 9.7509\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9182 - val_loss: 9.7762\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9198 - val_loss: 9.7122\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9172 - val_loss: 9.8350\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9219 - val_loss: 9.8109\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9204 - val_loss: 9.7858\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9097 - val_loss: 9.8827\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9172 - val_loss: 9.9766\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9112 - val_loss: 9.8530\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9216 - val_loss: 9.8391\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9074 - val_loss: 9.8738\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9051 - val_loss: 10.0035\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9052 - val_loss: 9.8394\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9087 - val_loss: 9.8510\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9053 - val_loss: 10.0080\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9125 - val_loss: 10.0377\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9058 - val_loss: 9.9603\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9035 - val_loss: 9.9454\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9069 - val_loss: 10.0752\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9182 - val_loss: 10.0255\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9042 - val_loss: 10.0217\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8978 - val_loss: 10.0509\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9172 - val_loss: 10.1161\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9107 - val_loss: 9.8908\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8914 - val_loss: 10.0050\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9195 - val_loss: 10.1718\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8946 - val_loss: 10.0505\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8887 - val_loss: 9.8649\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9000 - val_loss: 9.9651\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8994 - val_loss: 10.0455\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8929 - val_loss: 9.9243\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8994 - val_loss: 9.9415\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8923 - val_loss: 10.0432\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8924 - val_loss: 10.1511\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8902 - val_loss: 10.0466\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8829 - val_loss: 9.9968\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8916 - val_loss: 10.0300\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8915 - val_loss: 10.0787\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8896 - val_loss: 9.9988\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8850 - val_loss: 10.0602\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8944 - val_loss: 9.8353\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8992 - val_loss: 9.8270\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8959 - val_loss: 9.9008\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8843 - val_loss: 10.0502\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8907 - val_loss: 10.0064\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8945 - val_loss: 10.0470\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8836 - val_loss: 9.9908\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8821 - val_loss: 10.2631\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8823 - val_loss: 10.0839\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8713 - val_loss: 10.1008\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8810 - val_loss: 9.9676\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8805 - val_loss: 9.9594\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8907 - val_loss: 9.9136\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8783 - val_loss: 10.1337\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8793 - val_loss: 10.1003\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8805 - val_loss: 9.9984\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8853 - val_loss: 10.0689\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8854 - val_loss: 9.9400\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8738 - val_loss: 9.8928\n",
      "Epoch 93/128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8761 - val_loss: 9.9343\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8822 - val_loss: 10.2142\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8773 - val_loss: 9.8811\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8772 - val_loss: 9.9240\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8663 - val_loss: 9.8597\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8641 - val_loss: 9.8860\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8726 - val_loss: 10.0413\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8692 - val_loss: 9.9035\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8700 - val_loss: 9.9670\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8775 - val_loss: 10.0034\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8672 - val_loss: 10.0155\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8663 - val_loss: 9.9666\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8670 - val_loss: 9.9268\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8681 - val_loss: 9.9460\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8727 - val_loss: 9.9292\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8606 - val_loss: 10.0308\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8762 - val_loss: 9.9179\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8691 - val_loss: 9.9501\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8651 - val_loss: 10.2048\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8646 - val_loss: 10.1794\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8646 - val_loss: 9.8184\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8691 - val_loss: 9.8402\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8561 - val_loss: 9.8607\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8561 - val_loss: 9.7381\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8481 - val_loss: 9.8082\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8522 - val_loss: 10.0540\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8546 - val_loss: 9.7985\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8550 - val_loss: 10.0948\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8692 - val_loss: 9.9031\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8492 - val_loss: 9.8187\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8468 - val_loss: 9.9873\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8526 - val_loss: 9.8878\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8601 - val_loss: 9.7720\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8551 - val_loss: 10.0973\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8506 - val_loss: 9.8958\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8518 - val_loss: 10.0279\n",
      "\n",
      "Loss: 1002.79%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 222)       2220      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 94, 94, 222)      888       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 94, 94, 222)       0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 256)       511744    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 92, 92, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 92, 92, 256)       0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 256)       590080    \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 88, 88, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 88, 88, 256)       0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 8)         18440     \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 84, 84, 8)         584       \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 84, 84, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 82, 82, 44)        3212      \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 82, 82, 44)       176       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 80, 80, 8)         3176      \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 80, 80, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 78, 78, 136)       9928      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,732,644\n",
      "Trainable params: 1,731,054\n",
      "Non-trainable params: 1,590\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 48s 186ms/step - loss: 2733.6624 - val_loss: 2354.5725\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 1202.7009 - val_loss: 716.3517\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 219.7110 - val_loss: 106.8757\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 34.1076 - val_loss: 19.9654\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 12.8371 - val_loss: 10.6893\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 10.3708 - val_loss: 10.1282\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 10.0817 - val_loss: 10.1841\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 10.0326 - val_loss: 9.9638\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 10.0072 - val_loss: 9.9590\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 10.0191 - val_loss: 10.0576\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 9.9985 - val_loss: 9.8547\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 10.0019 - val_loss: 10.2964\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 10.0196 - val_loss: 10.0369\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 10.0051 - val_loss: 9.7941\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 10.0003 - val_loss: 10.1023\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 10.0026 - val_loss: 9.8746\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 9.9912 - val_loss: 9.7495\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 10.0047 - val_loss: 9.8769\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 9.9900 - val_loss: 9.8874\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 9.9838 - val_loss: 9.7703\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 9.9824 - val_loss: 9.7305\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 9.9836 - val_loss: 9.8865\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 9.9890 - val_loss: 10.1253\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 9.9799 - val_loss: 9.9251\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 9.9965 - val_loss: 9.9883\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 9.9905 - val_loss: 9.7251\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 9.9841 - val_loss: 9.8909\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 9.9814 - val_loss: 9.7391\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 9.9738 - val_loss: 9.8745\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 9.9738 - val_loss: 9.8899\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 9.9724 - val_loss: 9.8512\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 9.9698 - val_loss: 9.7042\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 9.9577 - val_loss: 9.7219\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 9.9718 - val_loss: 9.8908\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 9.9680 - val_loss: 9.7660\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 9.9667 - val_loss: 9.7406\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 9.9675 - val_loss: 9.8122\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 9.9647 - val_loss: 9.8064\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 9.9707 - val_loss: 9.7461\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 9.9511 - val_loss: 9.7100\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 9.9741 - val_loss: 9.7003\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 9.9556 - val_loss: 9.7352\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 9.9850 - val_loss: 9.7534\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 9.9588 - val_loss: 9.8711\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 9.9499 - val_loss: 9.7406\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 9.9605 - val_loss: 9.7777\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 9.9598 - val_loss: 9.7181\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 9.9577 - val_loss: 9.7652\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 9.9524 - val_loss: 9.7589\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 9.9538 - val_loss: 9.7426\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 46s 185ms/step - loss: 9.9472 - val_loss: 9.9015\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.9511 - val_loss: 9.7663\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 47s 187ms/step - loss: 9.9482 - val_loss: 9.8372\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.9412 - val_loss: 9.6822\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.9479 - val_loss: 9.8385\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.9487 - val_loss: 9.7009\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.9379 - val_loss: 9.7287\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.9456 - val_loss: 9.7222\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.9421 - val_loss: 9.6960\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.9349 - val_loss: 9.6973\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.9481 - val_loss: 9.7120\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.9384 - val_loss: 9.7276\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.9450 - val_loss: 9.7096\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.9446 - val_loss: 9.6767\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.9383 - val_loss: 9.7251\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.9327 - val_loss: 9.7397\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.9313 - val_loss: 9.7762\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.9313 - val_loss: 9.6754\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.9354 - val_loss: 9.7384\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.9305 - val_loss: 9.6702\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.9303 - val_loss: 9.7955\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.9361 - val_loss: 9.6547\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.9205 - val_loss: 9.8382\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.9299 - val_loss: 9.7209\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.9328 - val_loss: 9.6979\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.9280 - val_loss: 9.7362\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.9257 - val_loss: 9.6852\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.9224 - val_loss: 9.7096\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.9318 - val_loss: 9.7591\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.9279 - val_loss: 9.6772\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.9260 - val_loss: 9.6746\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.9279 - val_loss: 9.7472\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.9417 - val_loss: 9.6803\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.9191 - val_loss: 9.6905\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.9153 - val_loss: 9.7454\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 46s 184ms/step - loss: 9.9061 - val_loss: 9.7031\n",
      "Epoch 87/128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 46s 183ms/step - loss: 9.9198 - val_loss: 9.7846\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 9.9175 - val_loss: 9.7439\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 9.9146 - val_loss: 9.6988\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 9.9171 - val_loss: 9.7216\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 9.9187 - val_loss: 9.8260\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 9.9185 - val_loss: 9.7982\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 9.9065 - val_loss: 9.7396\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 9.9057 - val_loss: 9.7818\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 9.9169 - val_loss: 9.7716\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 9.9129 - val_loss: 9.7174\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 9.9062 - val_loss: 9.7371\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 9.8979 - val_loss: 9.7182\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 9.9137 - val_loss: 9.7820\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 9.9179 - val_loss: 9.7683\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 9.8999 - val_loss: 9.7710\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 9.9015 - val_loss: 9.8759\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 9.9060 - val_loss: 9.9408\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 9.9022 - val_loss: 9.8253\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 9.8946 - val_loss: 9.8758\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 9.8941 - val_loss: 9.8149\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 9.8952 - val_loss: 9.7500\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 9.8816 - val_loss: 9.7385\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 9.8915 - val_loss: 9.8971\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 9.8849 - val_loss: 9.7030\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 9.8650 - val_loss: 9.6887\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 9.8847 - val_loss: 9.7566\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 9.8621 - val_loss: 9.7403\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 9.8598 - val_loss: 9.8360\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 9.8689 - val_loss: 9.7970\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 9.8356 - val_loss: 9.6599\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 9.8386 - val_loss: 9.6415\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 9.8213 - val_loss: 9.6831\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 9.8064 - val_loss: 9.6455\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 9.8011 - val_loss: 9.7481\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 9.8038 - val_loss: 9.7648\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 9.7848 - val_loss: 9.7183\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 9.7865 - val_loss: 9.6490\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 9.7655 - val_loss: 9.7283\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 9.7680 - val_loss: 9.8485\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 9.7528 - val_loss: 9.6682\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 9.7549 - val_loss: 9.6734\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 46s 183ms/step - loss: 9.7398 - val_loss: 9.6870\n",
      "\n",
      "Loss: 968.70%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 256)       2560      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 92, 92, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 92, 92, 256)       0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 8)         18440     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 90, 90, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 8)         584       \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 88, 88, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 136)       9928      \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 86, 86, 136)      544       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 84, 84, 136)       166600    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 789,828\n",
      "Trainable params: 789,010\n",
      "Non-trainable params: 818\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 27s 104ms/step - loss: 1171.3976 - val_loss: 31.0740\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.8377 - val_loss: 13.7191\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.2842 - val_loss: 13.0451\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.1709 - val_loss: 11.2012\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.1742 - val_loss: 10.3996\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.1484 - val_loss: 10.3917\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.1284 - val_loss: 10.5750\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.1072 - val_loss: 10.1197\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.1177 - val_loss: 10.2602\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0984 - val_loss: 9.9091\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.1252 - val_loss: 10.1957\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0972 - val_loss: 9.8775\n",
      "Epoch 13/128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 26s 103ms/step - loss: 10.1128 - val_loss: 10.0557\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.1048 - val_loss: 10.0483\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.1335 - val_loss: 10.1208\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.1208 - val_loss: 9.8431\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0963 - val_loss: 10.0489\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.1379 - val_loss: 9.7983\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.1021 - val_loss: 10.0185\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0879 - val_loss: 10.5742\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.1221 - val_loss: 10.0434\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.1127 - val_loss: 10.0213\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.1297 - val_loss: 10.0292\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.1232 - val_loss: 10.7151\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.1310 - val_loss: 9.8893\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.1188 - val_loss: 9.8801\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0904 - val_loss: 11.0678\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.1291 - val_loss: 9.8462\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0906 - val_loss: 9.9053\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.1066 - val_loss: 9.7680\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.1117 - val_loss: 9.7674\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.1022 - val_loss: 9.8431\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.1039 - val_loss: 9.8087\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.1205 - val_loss: 10.2167\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0951 - val_loss: 10.1216\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0964 - val_loss: 9.9682\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.1178 - val_loss: 10.4253\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.1035 - val_loss: 9.8468\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.1268 - val_loss: 10.3937\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0796 - val_loss: 10.0717\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.1097 - val_loss: 10.0279\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0960 - val_loss: 40.5638\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0800 - val_loss: 9.8153\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0658 - val_loss: 9.9618\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0716 - val_loss: 9.8269\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0673 - val_loss: 9.7599\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0822 - val_loss: 9.8326\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0870 - val_loss: 9.7579\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0909 - val_loss: 10.1445\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0679 - val_loss: 10.3490\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0838 - val_loss: 10.3172\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0787 - val_loss: 9.7793\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0914 - val_loss: 9.7619\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0845 - val_loss: 9.8799\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0709 - val_loss: 9.9226\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0569 - val_loss: 9.7727\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0562 - val_loss: 10.0512\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0651 - val_loss: 9.7744\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0526 - val_loss: 9.9753\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0723 - val_loss: 9.9562\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0827 - val_loss: 9.8452\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0527 - val_loss: 10.1423\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0602 - val_loss: 10.3592\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0857 - val_loss: 10.1057\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0426 - val_loss: 9.9603\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0487 - val_loss: 9.7999\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0361 - val_loss: 9.7075\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0467 - val_loss: 9.8286\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0658 - val_loss: 9.8702\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0623 - val_loss: 9.9120\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0577 - val_loss: 9.7699\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0637 - val_loss: 9.8531\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0491 - val_loss: 9.8584\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0338 - val_loss: 9.8752\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0395 - val_loss: 9.9902\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0610 - val_loss: 9.7483\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0596 - val_loss: 9.8115\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0693 - val_loss: 9.8422\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0292 - val_loss: 12.0979\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0236 - val_loss: 9.8573\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0381 - val_loss: 9.7987\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0311 - val_loss: 9.7321\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0094 - val_loss: 9.8102\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0158 - val_loss: 9.7483\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0572 - val_loss: 9.9869\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0223 - val_loss: 9.8842\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0288 - val_loss: 9.8228\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0472 - val_loss: 9.8802\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0562 - val_loss: 9.8899\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0295 - val_loss: 9.9895\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0502 - val_loss: 9.9772\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0154 - val_loss: 9.9721\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0170 - val_loss: 9.9895\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0253 - val_loss: 9.7351\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0341 - val_loss: 9.7125\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0013 - val_loss: 9.7722\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0182 - val_loss: 9.9714\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0235 - val_loss: 9.8564\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0071 - val_loss: 9.7508\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0103 - val_loss: 9.8759\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0400 - val_loss: 9.9128\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0235 - val_loss: 9.9018\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0004 - val_loss: 9.8228\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 9.9976 - val_loss: 9.8343\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 9.9986 - val_loss: 9.7643\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0078 - val_loss: 9.8593\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0228 - val_loss: 9.7938\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0112 - val_loss: 9.7294\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0032 - val_loss: 10.0081\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0043 - val_loss: 9.8792\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0103 - val_loss: 9.7763\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0148 - val_loss: 9.8689\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 9.9999 - val_loss: 9.8066\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0307 - val_loss: 9.7863\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0352 - val_loss: 9.8743\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 9.9969 - val_loss: 9.7759\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 9.9830 - val_loss: 9.7291\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0078 - val_loss: 9.7214\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0142 - val_loss: 10.1094\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 9.9854 - val_loss: 9.8421\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 9.9906 - val_loss: 9.8014\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0023 - val_loss: 9.7922\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0084 - val_loss: 9.7113\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 9.9975 - val_loss: 10.2282\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 9.9889 - val_loss: 9.7866\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 10.0112 - val_loss: 9.8322\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 9.9758 - val_loss: 9.7574\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 9.9790 - val_loss: 9.7673\n",
      "\n",
      "Loss: 976.73%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 256)       2560      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 94, 94, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 94, 94, 256)       0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 256)       590080    \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 90, 90, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 106)       244330    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 88, 88, 106)      424       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 136)       129880    \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 86, 86, 136)      544       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 84, 84, 136)       166600    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,726,550\n",
      "Trainable params: 1,725,040\n",
      "Non-trainable params: 1,510\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 45s 174ms/step - loss: 1188.7784 - val_loss: 54.7691\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 11.0279 - val_loss: 14.5438\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.4384 - val_loss: 11.7905\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.3012 - val_loss: 11.4622\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.2672 - val_loss: 10.3155\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.1939 - val_loss: 10.4520\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.1989 - val_loss: 10.3092\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.1852 - val_loss: 11.4365\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.2102 - val_loss: 10.2957\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.1459 - val_loss: 10.0349\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.1439 - val_loss: 9.9751\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.1519 - val_loss: 10.4293\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.1676 - val_loss: 10.4612\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.1731 - val_loss: 10.4304\n",
      "Epoch 15/128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 43s 171ms/step - loss: 10.1356 - val_loss: 10.2485\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.1612 - val_loss: 10.0162\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.1494 - val_loss: 9.8786\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.1544 - val_loss: 10.1604\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.1533 - val_loss: 10.4809\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.1451 - val_loss: 10.0213\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.1522 - val_loss: 10.0742\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.1451 - val_loss: 10.3143\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.1603 - val_loss: 10.4861\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.1412 - val_loss: 9.8716\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.1105 - val_loss: 9.8118\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.1221 - val_loss: 10.9796\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.1480 - val_loss: 10.3033\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.1011 - val_loss: 10.2184\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.1560 - val_loss: 9.7842\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.1292 - val_loss: 10.1595\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.1558 - val_loss: 10.2317\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.1078 - val_loss: 9.9055\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.1101 - val_loss: 9.7987\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.1395 - val_loss: 9.8818\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.1171 - val_loss: 10.1053\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.1308 - val_loss: 9.8744\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.1187 - val_loss: 10.0179\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.1151 - val_loss: 9.8512\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.0776 - val_loss: 9.8136\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.1085 - val_loss: 9.8523\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.0796 - val_loss: 9.9342\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.0980 - val_loss: 9.9503\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.1148 - val_loss: 9.9227\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.0916 - val_loss: 10.4080\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.1214 - val_loss: 10.1010\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.0871 - val_loss: 9.9158\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.1013 - val_loss: 9.9530\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.0922 - val_loss: 10.1214\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.1009 - val_loss: 9.8015\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.0768 - val_loss: 9.7417\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.0829 - val_loss: 10.1324\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.0875 - val_loss: 9.9600\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.0729 - val_loss: 9.8745\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.0808 - val_loss: 9.9054\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.0940 - val_loss: 9.8338\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.0946 - val_loss: 10.1223\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.0870 - val_loss: 9.9710\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.0769 - val_loss: 10.0684\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.0929 - val_loss: 9.8817\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.0790 - val_loss: 10.1892\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.0803 - val_loss: 9.9121\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.0520 - val_loss: 10.1949\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.0883 - val_loss: 9.7671\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.0836 - val_loss: 10.1530\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.0540 - val_loss: 10.0319\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.0733 - val_loss: 9.8997\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.0601 - val_loss: 9.9015\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.0539 - val_loss: 9.8360\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.0676 - val_loss: 10.0256\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.0739 - val_loss: 10.1755\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.0430 - val_loss: 9.8004\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.0608 - val_loss: 9.8684\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.0284 - val_loss: 9.9475\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.0335 - val_loss: 10.0356\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.0581 - val_loss: 9.8785\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.0452 - val_loss: 9.9384\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.0544 - val_loss: 9.9993\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.0195 - val_loss: 9.9574\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.0085 - val_loss: 10.1105\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.0041 - val_loss: 10.1658\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.0341 - val_loss: 9.7482\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 9.9888 - val_loss: 10.0489\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.0192 - val_loss: 10.0413\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.0059 - val_loss: 9.9491\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.0125 - val_loss: 9.8631\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.0223 - val_loss: 10.1142\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 9.9860 - val_loss: 9.8722\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 9.9718 - val_loss: 9.7062\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 10.0021 - val_loss: 9.9189\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 9.9761 - val_loss: 9.8911\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 9.9613 - val_loss: 9.8081\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 9.9584 - val_loss: 9.8620\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 9.9834 - val_loss: 9.9319\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 9.9701 - val_loss: 9.8380\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 9.9721 - val_loss: 10.0595\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 9.9717 - val_loss: 9.8670\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 9.9700 - val_loss: 9.8293\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 9.9439 - val_loss: 9.6853\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 9.9496 - val_loss: 9.7161\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 9.9383 - val_loss: 9.7312\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 9.9377 - val_loss: 9.6844\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 9.9436 - val_loss: 9.6662\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 9.9231 - val_loss: 9.6904\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 9.9290 - val_loss: 9.8071\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 9.9135 - val_loss: 9.9447\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 9.9262 - val_loss: 9.9782\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 9.9064 - val_loss: 9.7886\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 9.8962 - val_loss: 9.7049\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 9.8998 - val_loss: 9.7670\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 9.9033 - val_loss: 9.8010\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 9.8872 - val_loss: 9.6350\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 9.9013 - val_loss: 9.8533\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 9.8813 - val_loss: 9.8494\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 9.8858 - val_loss: 9.7391\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 9.8887 - val_loss: 9.9139\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 9.8728 - val_loss: 9.8394\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 9.8870 - val_loss: 9.8201\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 9.8594 - val_loss: 9.7628\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 9.8885 - val_loss: 9.8630\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 9.9066 - val_loss: 11.2554\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 9.8745 - val_loss: 9.9886\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 9.8412 - val_loss: 9.7678\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 9.8728 - val_loss: 9.7323\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 9.8615 - val_loss: 9.8288\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 9.8343 - val_loss: 9.8240\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 9.8346 - val_loss: 9.6343\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 9.8386 - val_loss: 9.8768\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 9.8155 - val_loss: 9.6815\n",
      "\n",
      "Loss: 968.15%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 256)       2560      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 92, 92, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 92, 92, 256)       0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 8)         18440     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 90, 90, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 90, 90, 8)         0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 8)         584       \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 88, 88, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 64)        4672      \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 86, 86, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 84, 84, 60)        34620     \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 84, 84, 60)       240       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 82, 82, 136)       73576     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 726,120\n",
      "Trainable params: 725,326\n",
      "Non-trainable params: 794\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 27s 101ms/step - loss: 1647.6604 - val_loss: 153.2488\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 27.9430 - val_loss: 37.2406\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 10.4232 - val_loss: 13.1455\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 10.2997 - val_loss: 12.4240\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 10.2950 - val_loss: 12.9956\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 10.2524 - val_loss: 11.6532\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 10.2586 - val_loss: 11.9273\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 10.2066 - val_loss: 12.0984\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 10.1626 - val_loss: 10.7309\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 10.1374 - val_loss: 10.5290\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 10.0994 - val_loss: 10.1589\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 10.1312 - val_loss: 10.8246\n",
      "Epoch 13/128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 25s 99ms/step - loss: 10.1505 - val_loss: 10.8601\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 10.1301 - val_loss: 10.2535\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 10.0998 - val_loss: 10.6731\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 10.0911 - val_loss: 10.0552\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 10.1071 - val_loss: 10.3846\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 10.1257 - val_loss: 10.6540\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 10.0745 - val_loss: 9.9419\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 10.0946 - val_loss: 10.1475\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 10.1029 - val_loss: 10.0478\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 10.0745 - val_loss: 9.8663\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 10.0981 - val_loss: 10.4829\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 10.0774 - val_loss: 9.8763\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 10.0800 - val_loss: 10.6322\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 10.1126 - val_loss: 10.2979\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 10.0920 - val_loss: 10.2475\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 10.0849 - val_loss: 10.4377\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 10.0941 - val_loss: 9.8542\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 10.0738 - val_loss: 10.2509\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 10.0898 - val_loss: 10.0201\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 10.0599 - val_loss: 9.9823\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 10.0800 - val_loss: 9.9537\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 10.0881 - val_loss: 9.8748\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 10.0640 - val_loss: 10.0851\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 10.0683 - val_loss: 9.9891\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 10.0742 - val_loss: 10.1820\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 10.0572 - val_loss: 9.9545\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 10.0608 - val_loss: 9.7641\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 10.0778 - val_loss: 9.7720\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 10.0452 - val_loss: 10.0001\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 10.0666 - val_loss: 9.8419\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 10.0387 - val_loss: 10.5808\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 10.0593 - val_loss: 10.0540\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 10.0611 - val_loss: 9.8897\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 10.0514 - val_loss: 9.8518\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 10.0695 - val_loss: 9.8381\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 10.0343 - val_loss: 9.8190\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 10.0520 - val_loss: 9.8876\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 10.0563 - val_loss: 10.0313\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 10.0461 - val_loss: 9.7729\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 10.0379 - val_loss: 9.8961\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 10.0102 - val_loss: 9.7601\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 10.0295 - val_loss: 10.1045\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 10.0267 - val_loss: 9.7648\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 10.0144 - val_loss: 9.7377\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 10.0314 - val_loss: 9.9648\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 10.0343 - val_loss: 9.8618\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 10.0314 - val_loss: 9.8521\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 10.0104 - val_loss: 9.7940\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 10.0293 - val_loss: 9.8436\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 10.0391 - val_loss: 10.0251\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9914 - val_loss: 9.8671\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 10.0224 - val_loss: 9.8203\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 10.0110 - val_loss: 9.7887\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 10.0180 - val_loss: 9.7534\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9946 - val_loss: 9.9121\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9914 - val_loss: 9.7963\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9955 - val_loss: 9.9264\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9863 - val_loss: 9.7898\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 10.0005 - val_loss: 9.9096\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9913 - val_loss: 9.7839\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 10.0161 - val_loss: 9.8854\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9850 - val_loss: 9.7183\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9922 - val_loss: 9.7135\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9695 - val_loss: 9.9254\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9855 - val_loss: 9.8026\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9809 - val_loss: 9.6808\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9666 - val_loss: 9.8796\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9772 - val_loss: 9.7175\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9680 - val_loss: 9.6736\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9569 - val_loss: 9.7199\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9553 - val_loss: 9.7313\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9606 - val_loss: 9.6895\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9669 - val_loss: 9.7231\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9657 - val_loss: 9.7726\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9666 - val_loss: 9.6958\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9736 - val_loss: 9.7339\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9690 - val_loss: 9.8537\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9636 - val_loss: 9.8019\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9358 - val_loss: 9.7417\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9409 - val_loss: 9.7952\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9195 - val_loss: 9.8577\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9541 - val_loss: 9.7717\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9388 - val_loss: 9.6453\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9369 - val_loss: 9.8969\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9575 - val_loss: 9.6895\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9221 - val_loss: 9.7534\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9195 - val_loss: 9.6898\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9294 - val_loss: 9.8390\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8947 - val_loss: 9.7105\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9296 - val_loss: 9.8144\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9131 - val_loss: 9.6815\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9066 - val_loss: 9.8594\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9238 - val_loss: 9.6675\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9166 - val_loss: 10.0367\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9036 - val_loss: 9.6362\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9024 - val_loss: 9.7218\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.9002 - val_loss: 9.6233\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8890 - val_loss: 9.6239\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8833 - val_loss: 9.6617\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8789 - val_loss: 9.6977\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8917 - val_loss: 9.8590\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8858 - val_loss: 9.7195\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8936 - val_loss: 9.7291\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8918 - val_loss: 9.8326\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8536 - val_loss: 9.9254\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8646 - val_loss: 9.7511\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8835 - val_loss: 9.6440\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8955 - val_loss: 9.5945\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8950 - val_loss: 9.6444\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8478 - val_loss: 9.7956\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8585 - val_loss: 9.5907\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8776 - val_loss: 9.6527\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8650 - val_loss: 9.6236\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8693 - val_loss: 9.6106\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8840 - val_loss: 9.6258\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 9.8513 - val_loss: 9.6510\n",
      "\n",
      "Loss: 965.10%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 256)       2560      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 94, 94, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 94, 94, 256)       0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 92, 92, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 92, 92, 256)       0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 256)       590080    \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 88, 88, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 8)         18440     \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 86, 86, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 84, 84, 105)       7665      \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 84, 84, 105)      420       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 82, 82, 136)       128656    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,931,089\n",
      "Trainable params: 1,929,325\n",
      "Non-trainable params: 1,764\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 48s 190ms/step - loss: 1316.0260 - val_loss: 96.2358\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 11.8752 - val_loss: 16.0320\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.1830 - val_loss: 13.9102\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.1434 - val_loss: 11.0949\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.1277 - val_loss: 11.3200\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.1105 - val_loss: 10.5673\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0847 - val_loss: 10.9253\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.1092 - val_loss: 10.8747\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.1027 - val_loss: 10.4168\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0991 - val_loss: 10.6773\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.1015 - val_loss: 10.2411\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0890 - val_loss: 10.3525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0951 - val_loss: 9.9894\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0944 - val_loss: 10.3950\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.1021 - val_loss: 10.0215\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0582 - val_loss: 9.9226\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0737 - val_loss: 10.7378\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.1045 - val_loss: 10.3131\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.1159 - val_loss: 10.5208\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0918 - val_loss: 9.9530\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.2266 - val_loss: 18.2819\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0995 - val_loss: 10.7968\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0983 - val_loss: 10.4589\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0784 - val_loss: 9.7904\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0860 - val_loss: 9.9532\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.1004 - val_loss: 10.1024\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.1019 - val_loss: 9.8228\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.1163 - val_loss: 9.9668\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.1384 - val_loss: 10.2952\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0703 - val_loss: 10.3326\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.1092 - val_loss: 9.9384\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0711 - val_loss: 10.4632\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0787 - val_loss: 190.2754\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0870 - val_loss: 9.9757\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.1122 - val_loss: 10.1616\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0989 - val_loss: 10.1574\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.1078 - val_loss: 9.9985\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0800 - val_loss: 9.8583\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.1020 - val_loss: 10.1986\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.1313 - val_loss: 9.9660\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.1042 - val_loss: 9.9009\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0557 - val_loss: 10.1017\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0715 - val_loss: 10.0849\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.1054 - val_loss: 9.8848\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.1014 - val_loss: 9.9530\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0807 - val_loss: 9.8728\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0611 - val_loss: 9.9072\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0620 - val_loss: 9.9805\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.1100 - val_loss: 9.8447\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0870 - val_loss: 9.9955\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0722 - val_loss: 9.7450\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.1029 - val_loss: 9.9442\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0850 - val_loss: 10.2410\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0695 - val_loss: 9.8819\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0725 - val_loss: 9.7810\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0850 - val_loss: 10.1355\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0550 - val_loss: 9.8039\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0438 - val_loss: 9.7572\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.1117 - val_loss: 9.7773\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0704 - val_loss: 10.0061\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0621 - val_loss: 9.9941\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0574 - val_loss: 9.9369\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0461 - val_loss: 9.8751\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0791 - val_loss: 9.7934\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0534 - val_loss: 9.9979\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.1075 - val_loss: 9.8574\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0721 - val_loss: 9.9675\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0707 - val_loss: 9.7390\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0858 - val_loss: 10.4205\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0690 - val_loss: 9.9780\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0752 - val_loss: 9.9096\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0783 - val_loss: 10.0286\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0792 - val_loss: 10.1596\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0509 - val_loss: 9.8784\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0615 - val_loss: 10.1155\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0935 - val_loss: 9.8359\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0557 - val_loss: 9.8847\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0474 - val_loss: 9.9840\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0517 - val_loss: 9.7953\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0623 - val_loss: 9.9703\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0567 - val_loss: 10.6744\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0568 - val_loss: 9.7270\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0726 - val_loss: 9.8120\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0666 - val_loss: 9.8733\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0508 - val_loss: 10.0680\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0556 - val_loss: 9.9485\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0518 - val_loss: 9.7661\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0729 - val_loss: 9.7703\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0531 - val_loss: 9.7797\n",
      "Epoch 90/128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0606 - val_loss: 9.8992\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0723 - val_loss: 9.7792\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0491 - val_loss: 9.7736\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0641 - val_loss: 9.9839\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0776 - val_loss: 9.7691\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0391 - val_loss: 9.9066\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0486 - val_loss: 9.7533\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0676 - val_loss: 9.9451\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0412 - val_loss: 9.8591\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0385 - val_loss: 9.8228\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0336 - val_loss: 9.9624\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0419 - val_loss: 9.8743\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0273 - val_loss: 9.8884\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0322 - val_loss: 10.1268\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0315 - val_loss: 9.8936\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0128 - val_loss: 9.8013\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0539 - val_loss: 10.9919\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0251 - val_loss: 9.9491\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0298 - val_loss: 9.8924\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 9.9905 - val_loss: 9.8680\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0165 - val_loss: 9.7901\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0173 - val_loss: 10.0233\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0073 - val_loss: 9.8317\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0270 - val_loss: 9.9235\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0190 - val_loss: 9.9540\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 9.9830 - val_loss: 10.0327\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 9.9920 - val_loss: 9.9805\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 9.9822 - val_loss: 9.8553\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 9.9891 - val_loss: 9.9495\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 9.9939 - val_loss: 9.9422\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0117 - val_loss: 9.8587\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 9.9955 - val_loss: 9.9328\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0128 - val_loss: 10.4442\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 9.9967 - val_loss: 10.0398\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 9.9796 - val_loss: 9.8140\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 9.9878 - val_loss: 10.2114\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 9.9965 - val_loss: 9.7887\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 9.9519 - val_loss: 10.4087\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 47s 188ms/step - loss: 10.0042 - val_loss: 10.4200\n",
      "\n",
      "Loss: 1042.00%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 8)         80        \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 8)         584       \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 92, 92, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 92, 92, 8)         0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 177)       12921     \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 177)       282138    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 88, 88, 177)      708       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 88, 88, 177)       0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 33)        52602     \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 84, 84, 33)        9834      \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 84, 84, 33)       132       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 82, 82, 76)        22648     \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 82, 82, 76)       304       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 80, 80, 87)        59595     \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 80, 80, 87)       348       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 78, 78, 136)       106624    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 548,554\n",
      "Trainable params: 547,790\n",
      "Non-trainable params: 764\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 27s 99ms/step - loss: 1425.2964 - val_loss: 167.2496\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 14.5645 - val_loss: 14.4617\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.6254 - val_loss: 11.7657\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.3904 - val_loss: 11.7249\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.2218 - val_loss: 10.9867\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.1890 - val_loss: 10.0456\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.3017 - val_loss: 10.8382\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.2442 - val_loss: 10.6105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.1626 - val_loss: 10.6080\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.2908 - val_loss: 10.1783\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.1625 - val_loss: 10.4366\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.1554 - val_loss: 11.0876\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.1556 - val_loss: 10.6100\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.1411 - val_loss: 10.3347\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.0914 - val_loss: 10.5298\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.1492 - val_loss: 10.2401\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.1511 - val_loss: 10.1542\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.1271 - val_loss: 10.2198\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.1806 - val_loss: 10.1975\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.1045 - val_loss: 10.6857\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.1298 - val_loss: 10.2555\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.1330 - val_loss: 10.5602\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.1497 - val_loss: 10.0635\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.1324 - val_loss: 10.0719\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.1051 - val_loss: 10.0952\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.1083 - val_loss: 9.9265\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.1245 - val_loss: 9.9991\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.0887 - val_loss: 9.8825\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.0764 - val_loss: 9.9593\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.0897 - val_loss: 9.9827\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.1229 - val_loss: 9.9331\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.1182 - val_loss: 10.0285\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.0890 - val_loss: 10.0083\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.0846 - val_loss: 10.1591\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.0733 - val_loss: 10.1663\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.1007 - val_loss: 9.9670\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.0913 - val_loss: 9.9092\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.0673 - val_loss: 9.9584\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.0596 - val_loss: 9.8560\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.0954 - val_loss: 9.9143\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.0499 - val_loss: 10.0181\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.0649 - val_loss: 10.2879\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.0631 - val_loss: 9.7776\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.0787 - val_loss: 10.1200\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.0893 - val_loss: 9.8591\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.0878 - val_loss: 9.9643\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.0627 - val_loss: 10.1546\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.0516 - val_loss: 9.7912\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.0819 - val_loss: 9.8379\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.0332 - val_loss: 9.8305\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.0530 - val_loss: 9.9365\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.0442 - val_loss: 9.8392\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.0752 - val_loss: 9.7929\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.0426 - val_loss: 9.7780\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.0427 - val_loss: 9.8691\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.0338 - val_loss: 9.9001\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.0293 - val_loss: 9.9826\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.0688 - val_loss: 9.8070\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.0454 - val_loss: 9.9206\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.0008 - val_loss: 9.8011\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.0569 - val_loss: 9.9362\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.0550 - val_loss: 10.0248\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.0375 - val_loss: 9.8801\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.0256 - val_loss: 9.9261\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.0412 - val_loss: 9.7940\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.0576 - val_loss: 9.8547\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.0089 - val_loss: 9.8868\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.0257 - val_loss: 9.8743\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.0252 - val_loss: 9.9798\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.0521 - val_loss: 9.8266\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.0144 - val_loss: 9.7879\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.0318 - val_loss: 10.5279\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.0001 - val_loss: 9.8841\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.0049 - val_loss: 9.9009\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.0306 - val_loss: 10.0067\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.0014 - val_loss: 9.9267\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.0428 - val_loss: 10.1924\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 9.9859 - val_loss: 9.7380\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.0177 - val_loss: 9.8351\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.0149 - val_loss: 9.7875\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 9.9915 - val_loss: 9.9112\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 9.9922 - val_loss: 9.8167\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 10.0206 - val_loss: 10.0896\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 9.9841 - val_loss: 9.8577\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 9.9945 - val_loss: 9.9210\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 9.9866 - val_loss: 10.1859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 9.9722 - val_loss: 10.0388\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 9.9829 - val_loss: 10.0523\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 9.9703 - val_loss: 9.9405\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 9.9673 - val_loss: 9.8249\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 9.9506 - val_loss: 10.0886\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 9.9576 - val_loss: 9.9214\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 9.9838 - val_loss: 10.0127\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 9.9856 - val_loss: 10.1316\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 9.9854 - val_loss: 9.9041\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 9.9692 - val_loss: 9.8953\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 9.9694 - val_loss: 10.3969\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 9.9558 - val_loss: 10.2755\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 9.9544 - val_loss: 10.5165\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 9.9638 - val_loss: 9.9720\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 9.9232 - val_loss: 10.5693\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 9.9521 - val_loss: 9.8472\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 9.9440 - val_loss: 10.0745\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 9.9328 - val_loss: 10.0588\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 9.9241 - val_loss: 11.0080\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 9.9212 - val_loss: 9.9360\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 9.9234 - val_loss: 10.9555\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 9.9267 - val_loss: 10.1611\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 9.9198 - val_loss: 10.2481\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 9.9091 - val_loss: 10.6824\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 9.9205 - val_loss: 10.8218\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 9.9017 - val_loss: 12.1417\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 9.8748 - val_loss: 10.2542\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 9.8983 - val_loss: 12.3131\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 9.8831 - val_loss: 9.8445\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 9.8592 - val_loss: 9.8418\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 9.8716 - val_loss: 12.3500\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 9.8888 - val_loss: 11.8134\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 9.8529 - val_loss: 10.7229\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 9.8706 - val_loss: 10.1793\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 9.8583 - val_loss: 10.3371\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 9.8608 - val_loss: 10.2017\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 9.8293 - val_loss: 10.1314\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 9.8401 - val_loss: 10.9831\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 9.8337 - val_loss: 10.3874\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 9.8404 - val_loss: 9.9339\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 9.8274 - val_loss: 10.1694\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 9.8246 - val_loss: 9.8880\n",
      "\n",
      "Loss: 988.80%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 256)       2560      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 94, 94, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 94, 94, 256)       0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 234)       539370    \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 234)       493038    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 90, 90, 234)      936       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 90, 90, 234)       0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 8)         16856     \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 8)         584       \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 86, 86, 8)        32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 84, 84, 33)        2409      \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 84, 84, 33)       132       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 82, 82, 103)       30694     \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 82, 82, 103)      412       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 80, 80, 136)       126208    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,214,259\n",
      "Trainable params: 1,212,989\n",
      "Non-trainable params: 1,270\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 43s 161ms/step - loss: 1326.7762 - val_loss: 38.7634\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 12.0939 - val_loss: 12.8988\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 10.3156 - val_loss: 11.7054\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 10.1556 - val_loss: 10.5744\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 10.1664 - val_loss: 10.0095\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 10.1561 - val_loss: 10.6512\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 10.1135 - val_loss: 11.7970\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 10.0862 - val_loss: 10.7200\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 10.0903 - val_loss: 10.4028\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 10.0434 - val_loss: 10.2717\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 10.0851 - val_loss: 12.4020\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 10.0659 - val_loss: 10.3244\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 10.0771 - val_loss: 10.4024\n",
      "Epoch 14/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 10.0676 - val_loss: 10.6128\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 10.0753 - val_loss: 10.8817\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 10.0766 - val_loss: 10.1164\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 10.0506 - val_loss: 10.2694\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 10.0927 - val_loss: 11.3406\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 10.0962 - val_loss: 10.7638\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 10.1140 - val_loss: 12.5429\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 10.1085 - val_loss: 10.1047\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 10.0739 - val_loss: 10.1350\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 10.0771 - val_loss: 10.1116\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 10.0781 - val_loss: 10.1124\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 10.0981 - val_loss: 12.8397\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 10.1027 - val_loss: 10.3960\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 10.0628 - val_loss: 10.9938\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 10.0979 - val_loss: 10.0626\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 10.1187 - val_loss: 21.1237\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 10.0719 - val_loss: 10.0741\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 10.0804 - val_loss: 10.2665\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 10.0733 - val_loss: 10.4173\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 10.0751 - val_loss: 10.0516\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 10.0533 - val_loss: 10.6453\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 10.0762 - val_loss: 10.5017\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 10.0713 - val_loss: 10.0726\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 10.1038 - val_loss: 10.0697\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 10.0711 - val_loss: 10.0506\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 10.0831 - val_loss: 10.5977\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 10.1046 - val_loss: 10.1485\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 10.0568 - val_loss: 10.1436\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 10.0939 - val_loss: 10.0634\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 10.0764 - val_loss: 11.3595\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 10.0676 - val_loss: 9.9587\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 10.1108 - val_loss: 10.1542\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 10.0611 - val_loss: 10.1684\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 10.0494 - val_loss: 11.3099\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 10.0646 - val_loss: 10.1800\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 10.0464 - val_loss: 10.4771\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 10.0417 - val_loss: 10.2328\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 10.0296 - val_loss: 9.7625\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 10.0803 - val_loss: 10.1733\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 10.0473 - val_loss: 9.9542\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 10.0429 - val_loss: 10.1105\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 10.0704 - val_loss: 10.0317\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 10.0409 - val_loss: 10.2500\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 10.0518 - val_loss: 11.9826\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 10.0705 - val_loss: 11.3404\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 10.0496 - val_loss: 10.7398\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 10.0414 - val_loss: 10.2982\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 10.0545 - val_loss: 10.3684\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 10.0448 - val_loss: 10.0274\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 10.0736 - val_loss: 11.3801\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 10.0203 - val_loss: 10.4111\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 10.0243 - val_loss: 9.8448\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 10.0362 - val_loss: 10.6630\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 10.0352 - val_loss: 10.3722\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 10.0336 - val_loss: 11.8442\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 10.0648 - val_loss: 10.0203\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 10.0043 - val_loss: 10.3665\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 10.0255 - val_loss: 10.3456\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 10.0271 - val_loss: 10.8757\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 10.0116 - val_loss: 10.0352\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 10.0127 - val_loss: 10.7481\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 10.0079 - val_loss: 10.0458\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 9.9953 - val_loss: 10.1586\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 10.0112 - val_loss: 10.1135\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 9.9828 - val_loss: 9.8915\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 9.9885 - val_loss: 10.0779\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 9.9732 - val_loss: 10.3511\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 9.9823 - val_loss: 9.7406\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 9.9821 - val_loss: 10.2473\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 9.9766 - val_loss: 10.4583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 9.9829 - val_loss: 9.7839\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 9.9694 - val_loss: 9.7049\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 9.9686 - val_loss: 9.9767\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 9.9464 - val_loss: 9.7191\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 9.9336 - val_loss: 10.0624\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 9.9402 - val_loss: 9.7981\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 9.9334 - val_loss: 9.9232\n",
      "Epoch 91/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 9.9669 - val_loss: 10.2986\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 9.9535 - val_loss: 10.9359\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 9.9182 - val_loss: 10.0169\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 9.9214 - val_loss: 11.0210\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 9.9202 - val_loss: 9.8484\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 9.9038 - val_loss: 10.2017\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 9.9308 - val_loss: 10.0027\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 9.9078 - val_loss: 10.5412\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 9.9195 - val_loss: 9.9326\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 9.9173 - val_loss: 9.8415\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 9.8992 - val_loss: 9.8698\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 9.8984 - val_loss: 9.8089\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 9.9026 - val_loss: 10.0288\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 9.8864 - val_loss: 10.1269\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 9.9113 - val_loss: 9.9143\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 9.8816 - val_loss: 10.0874\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 9.8934 - val_loss: 10.1626\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 9.8871 - val_loss: 9.8487\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 9.8844 - val_loss: 10.9798\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 9.8673 - val_loss: 10.3771\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 9.8724 - val_loss: 10.1756\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 9.8912 - val_loss: 9.8316\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 9.8748 - val_loss: 10.1198\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 9.8698 - val_loss: 9.8206\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 9.8585 - val_loss: 9.8975\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 9.8344 - val_loss: 9.9783\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 9.8522 - val_loss: 10.1116\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 9.8587 - val_loss: 9.9031\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 9.8754 - val_loss: 9.8592\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 9.8606 - val_loss: 10.3235\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 9.8429 - val_loss: 10.1168\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 9.8497 - val_loss: 10.5834\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 9.8505 - val_loss: 9.9883\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 9.8607 - val_loss: 10.6608\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 9.8306 - val_loss: 9.7416\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 9.8415 - val_loss: 9.9058\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 9.8265 - val_loss: 10.0393\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 9.8116 - val_loss: 10.0312\n",
      "\n",
      "Loss: 1003.12%\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 96, 96, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 96, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 256)       2560      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 92, 92, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 92, 92, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 90, 90, 43)        99115     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 90, 90, 43)       172       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 88, 88, 136)       52768     \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 88, 88, 136)      544       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 86, 86, 136)       166600    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 912,867\n",
      "Trainable params: 911,995\n",
      "Non-trainable params: 872\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "250/250 [==============================] - 29s 111ms/step - loss: 1187.2271 - val_loss: 46.4470\n",
      "Epoch 2/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.7056 - val_loss: 12.3729\n",
      "Epoch 3/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.1551 - val_loss: 10.3311\n",
      "Epoch 4/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.1193 - val_loss: 10.4380\n",
      "Epoch 5/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.1272 - val_loss: 10.4050\n",
      "Epoch 6/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.1057 - val_loss: 10.2480\n",
      "Epoch 7/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.1043 - val_loss: 9.7591\n",
      "Epoch 8/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0918 - val_loss: 9.9961\n",
      "Epoch 9/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0738 - val_loss: 10.2658\n",
      "Epoch 10/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.1177 - val_loss: 10.0080\n",
      "Epoch 11/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0872 - val_loss: 9.9024\n",
      "Epoch 12/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.1284 - val_loss: 9.9665\n",
      "Epoch 13/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0994 - val_loss: 9.8201\n",
      "Epoch 14/128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 27s 108ms/step - loss: 10.1039 - val_loss: 9.8894\n",
      "Epoch 15/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.1069 - val_loss: 11.0441\n",
      "Epoch 16/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0921 - val_loss: 10.8322\n",
      "Epoch 17/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0661 - val_loss: 10.1221\n",
      "Epoch 18/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.1073 - val_loss: 10.3002\n",
      "Epoch 19/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.1046 - val_loss: 10.2054\n",
      "Epoch 20/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0838 - val_loss: 10.4461\n",
      "Epoch 21/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0917 - val_loss: 10.0684\n",
      "Epoch 22/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0779 - val_loss: 9.9122\n",
      "Epoch 23/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0802 - val_loss: 10.0004\n",
      "Epoch 24/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0834 - val_loss: 10.0181\n",
      "Epoch 25/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.1108 - val_loss: 10.0183\n",
      "Epoch 26/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.1068 - val_loss: 9.9527\n",
      "Epoch 27/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0756 - val_loss: 10.0470\n",
      "Epoch 28/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.1143 - val_loss: 9.8048\n",
      "Epoch 29/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0896 - val_loss: 10.0279\n",
      "Epoch 30/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0954 - val_loss: 10.1729\n",
      "Epoch 31/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0883 - val_loss: 9.8970\n",
      "Epoch 32/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.1243 - val_loss: 127.3001\n",
      "Epoch 33/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.1778 - val_loss: 10.6145\n",
      "Epoch 34/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.1010 - val_loss: 10.1347\n",
      "Epoch 35/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.1395 - val_loss: 10.2121\n",
      "Epoch 36/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0892 - val_loss: 9.8913\n",
      "Epoch 37/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.1158 - val_loss: 9.8589\n",
      "Epoch 38/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0950 - val_loss: 9.8524\n",
      "Epoch 39/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.1281 - val_loss: 9.9051\n",
      "Epoch 40/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0916 - val_loss: 9.9230\n",
      "Epoch 41/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0864 - val_loss: 9.9099\n",
      "Epoch 42/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.1309 - val_loss: 9.9069\n",
      "Epoch 43/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0928 - val_loss: 9.9977\n",
      "Epoch 44/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.1142 - val_loss: 9.9160\n",
      "Epoch 45/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0826 - val_loss: 9.9156\n",
      "Epoch 46/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0880 - val_loss: 9.8616\n",
      "Epoch 47/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0925 - val_loss: 9.9525\n",
      "Epoch 48/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0835 - val_loss: 9.8092\n",
      "Epoch 49/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0868 - val_loss: 9.8224\n",
      "Epoch 50/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0820 - val_loss: 9.7445\n",
      "Epoch 51/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0765 - val_loss: 10.0928\n",
      "Epoch 52/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0739 - val_loss: 10.2824\n",
      "Epoch 53/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0549 - val_loss: 9.7783\n",
      "Epoch 54/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0808 - val_loss: 9.9481\n",
      "Epoch 55/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0674 - val_loss: 9.9208\n",
      "Epoch 56/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0480 - val_loss: 9.8088\n",
      "Epoch 57/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0436 - val_loss: 10.4137\n",
      "Epoch 58/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0891 - val_loss: 10.2181\n",
      "Epoch 59/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0738 - val_loss: 9.7943\n",
      "Epoch 60/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0414 - val_loss: 9.8443\n",
      "Epoch 61/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0893 - val_loss: 9.7635\n",
      "Epoch 62/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0405 - val_loss: 10.1695\n",
      "Epoch 63/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0425 - val_loss: 9.9092\n",
      "Epoch 64/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0596 - val_loss: 9.8658\n",
      "Epoch 65/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0720 - val_loss: 10.0715\n",
      "Epoch 66/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.1169 - val_loss: 14.8506\n",
      "Epoch 67/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0740 - val_loss: 10.1256\n",
      "Epoch 68/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0625 - val_loss: 9.9314\n",
      "Epoch 69/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0557 - val_loss: 9.7780\n",
      "Epoch 70/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0425 - val_loss: 10.0795\n",
      "Epoch 71/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0548 - val_loss: 9.7950\n",
      "Epoch 72/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0402 - val_loss: 10.0673\n",
      "Epoch 73/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0447 - val_loss: 9.8913\n",
      "Epoch 74/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0536 - val_loss: 10.1924\n",
      "Epoch 75/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0527 - val_loss: 9.9112\n",
      "Epoch 76/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0666 - val_loss: 9.8566\n",
      "Epoch 77/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0285 - val_loss: 9.8538\n",
      "Epoch 78/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0422 - val_loss: 9.9967\n",
      "Epoch 79/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0255 - val_loss: 9.9094\n",
      "Epoch 80/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0176 - val_loss: 10.0042\n",
      "Epoch 81/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0481 - val_loss: 9.8787\n",
      "Epoch 82/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0562 - val_loss: 9.7905\n",
      "Epoch 83/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0354 - val_loss: 9.9988\n",
      "Epoch 84/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0202 - val_loss: 9.8149\n",
      "Epoch 85/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0539 - val_loss: 10.0353\n",
      "Epoch 86/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0402 - val_loss: 9.9425\n",
      "Epoch 87/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0222 - val_loss: 9.9820\n",
      "Epoch 88/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0183 - val_loss: 9.9968\n",
      "Epoch 89/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 9.9944 - val_loss: 9.7538\n",
      "Epoch 90/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0092 - val_loss: 10.3324\n",
      "Epoch 91/128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0265 - val_loss: 9.9174\n",
      "Epoch 92/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0415 - val_loss: 9.8182\n",
      "Epoch 93/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0159 - val_loss: 10.0200\n",
      "Epoch 94/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0005 - val_loss: 10.2034\n",
      "Epoch 95/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0041 - val_loss: 10.1943\n",
      "Epoch 96/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0341 - val_loss: 10.0286\n",
      "Epoch 97/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0400 - val_loss: 9.9478\n",
      "Epoch 98/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0534 - val_loss: 9.7464\n",
      "Epoch 99/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0372 - val_loss: 9.8607\n",
      "Epoch 100/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0048 - val_loss: 9.8481\n",
      "Epoch 101/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0127 - val_loss: 9.7436\n",
      "Epoch 102/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0238 - val_loss: 9.8050\n",
      "Epoch 103/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0181 - val_loss: 9.7634\n",
      "Epoch 104/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0001 - val_loss: 9.8274\n",
      "Epoch 105/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0061 - val_loss: 9.7974\n",
      "Epoch 106/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0129 - val_loss: 9.8361\n",
      "Epoch 107/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0207 - val_loss: 10.0808\n",
      "Epoch 108/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 9.9996 - val_loss: 9.8390\n",
      "Epoch 109/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0105 - val_loss: 9.8316\n",
      "Epoch 110/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0004 - val_loss: 9.7200\n",
      "Epoch 111/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0078 - val_loss: 9.8936\n",
      "Epoch 112/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 9.9842 - val_loss: 9.7106\n",
      "Epoch 113/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0026 - val_loss: 9.8768\n",
      "Epoch 114/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 9.9737 - val_loss: 9.7555\n",
      "Epoch 115/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0120 - val_loss: 9.8026\n",
      "Epoch 116/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 9.9998 - val_loss: 9.8238\n",
      "Epoch 117/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 9.9947 - val_loss: 9.8574\n",
      "Epoch 118/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0262 - val_loss: 9.7989\n",
      "Epoch 119/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 9.9861 - val_loss: 9.8664\n",
      "Epoch 120/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0059 - val_loss: 9.7118\n",
      "Epoch 121/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 9.9806 - val_loss: 9.8023\n",
      "Epoch 122/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 9.9960 - val_loss: 9.7223\n",
      "Epoch 123/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 9.9892 - val_loss: 9.7329\n",
      "Epoch 124/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0043 - val_loss: 9.8187\n",
      "Epoch 125/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 9.9863 - val_loss: 9.7567\n",
      "Epoch 126/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 10.0041 - val_loss: 9.7728\n",
      "Epoch 127/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 9.9978 - val_loss: 9.8297\n",
      "Epoch 128/128\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 9.9824 - val_loss: 10.1048\n",
      "\n",
      "Loss: 1010.48%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "best_accuracy=-1\n",
    "dim_num_neurons_1 = Integer(low=8, high=256, name='neurons_1')\n",
    "dim_has_layer_1=Categorical([True,False],name='has_layer_1')\n",
    "dim_has_2_layer_1=Categorical([True,False],name='has_2_conv_layer_1')\n",
    "\n",
    "dim_num_neurons_2 = Integer(low=8, high=256, name='neurons_2')\n",
    "dim_has_layer_2=Categorical([True,False],name='has_layer_2')\n",
    "dim_has_2_layer_2=Categorical([True,False],name='has_2_conv_layer_2')\n",
    "\n",
    "dim_num_neurons_3= Integer(low=8, high=256, name='neurons_3')\n",
    "dim_has_layer_3=Categorical([True,False],name='has_layer_3')\n",
    "dim_has_2_layer_3=Categorical([True,False],name='has_2_conv_layer_3')\n",
    "\n",
    "dim_num_neurons_4 = Integer(low=8, high=256, name='neurons_4')\n",
    "dim_has_layer_4=Categorical([True,False],name='has_layer_4')\n",
    "dim_has_2_layer_4=Categorical([True,False],name='has_2_conv_layer_4')\n",
    "\n",
    "dim_num_neurons_5 = Integer(low=8, high=256, name='neurons_5')\n",
    "dim_has_layer_5=Categorical([True,False],name='has_layer_5')\n",
    "dim_has_2_layer_5=Categorical([True,False],name='has_2_conv_layer_5')\n",
    "\n",
    "dim_num_dropout_1=Real(low=0.1,high=0.6,name='drop_1')\n",
    "dim_num_dropout_2=Real(low=0.1,high=0.6,name='drop_2')\n",
    "dim_num_dropout_3=Real(low=0.1,high=0.6,name='drop_3')\n",
    "dim_num_dropout_4=Real(low=0.1,high=0.6,name='drop_4')\n",
    "dim_num_dropout_5=Real(low=0.1,high=0.6,name='drop_5')\n",
    "\n",
    "dim_num_dense_1=Integer(low=8,high=output_shape,name='dense_1')\n",
    "dim_num_dense_2=Integer(low=8,high=output_shape,name='dense_2')\n",
    "\n",
    "\n",
    "dimensions2 = [dim_num_neurons_1,\n",
    "                dim_has_layer_1,\n",
    "                dim_has_2_layer_1,\n",
    "\n",
    "                dim_num_neurons_2,\n",
    "                dim_has_layer_2,\n",
    "                dim_has_2_layer_2,\n",
    "\n",
    "                dim_num_neurons_3,\n",
    "                dim_has_layer_3,\n",
    "                dim_has_2_layer_3,\n",
    "\n",
    "                dim_num_neurons_4,\n",
    "                dim_has_layer_4,\n",
    "                dim_has_2_layer_4,\n",
    "               \n",
    "                dim_num_neurons_5,\n",
    "                dim_has_layer_5,\n",
    "                dim_has_2_layer_5,\n",
    "\n",
    "                dim_num_dropout_1,\n",
    "                dim_num_dropout_2,\n",
    "                dim_num_dropout_3,\n",
    "                dim_num_dropout_4,\n",
    "                dim_num_dropout_5,\n",
    "                dim_num_dense_1,\n",
    "                dim_num_dense_2\n",
    "             ]\n",
    "\n",
    "\n",
    "\n",
    "default_parameters2 = [8,False,False,8,False,False,8,False,False,8,False,False,8,True,False,0.1,0.1,0.1,0.1,0.1,8,8]\n",
    "@use_named_args(dimensions=dimensions2)\n",
    "def fitness2(neurons_1=256,has_layer_1=True, has_2_conv_layer_1=True,neurons_2=16,has_layer_2=True, has_2_conv_layer_2=True,neurons_3=256,has_layer_3=True, has_2_conv_layer_3=True,neurons_4=64,has_layer_4=True, has_2_conv_layer_4=True,neurons_5=16,has_layer_5=True, has_2_conv_layer_5=True,drop_1=0.5,drop_2=0.4,drop_3=0.3,drop_4=0.2,drop_5=0.1,dense_1=0.7*output_shape,dense_2=0.5*output_shape):\n",
    "    loss=9999\n",
    "    try:\n",
    "        model = create_model_2(neurons_1=neurons_1,\n",
    "                               has_layer_1=has_layer_1,\n",
    "                               has_2_conv_layer_1=has_2_conv_layer_1,\n",
    "                             neurons_2=neurons_2,\n",
    "                               has_layer_2=has_layer_2,\n",
    "                               has_2_conv_layer_2=has_2_conv_layer_2,\n",
    "                             neurons_3=neurons_3,\n",
    "                               has_layer_3=has_layer_3,\n",
    "                               has_2_conv_layer_3=has_2_conv_layer_3,\n",
    "                             neurons_4=neurons_4,\n",
    "                               has_layer_4=has_layer_4,\n",
    "                               has_2_conv_layer_4=has_2_conv_layer_4,\n",
    "                             neurons_5=neurons_5,\n",
    "                               has_layer_5=has_layer_5,\n",
    "                               has_2_conv_layer_5=has_2_conv_layer_5,\n",
    "                             drop_1=drop_1,\n",
    "                             drop_2=drop_2,\n",
    "                             drop_3=drop_3,\n",
    "                             drop_4=drop_4,\n",
    "                             drop_5=drop_5,\n",
    "                             dense_1=dense_1,\n",
    "                             dense_2=dense_2\n",
    "                            )\n",
    "        model.summary()\n",
    "\n",
    "        history = model.fit(x=X_train,\n",
    "                            y=y_train,\n",
    "                            epochs=128,\n",
    "                            batch_size=16,\n",
    "                            validation_data=(X_test,y_test),\n",
    "                            callbacks=[callback]\n",
    "                           )\n",
    "        loss = history.history['val_loss'][-1]\n",
    "\n",
    "        print()\n",
    "        print(\"Loss: {0:.2%}\".format(loss))\n",
    "        print()\n",
    "\n",
    "        del model\n",
    "    except NameError:\n",
    "        print(\"NameError\")\n",
    "    finally:\n",
    "        \n",
    "        clear_session()\n",
    "        return loss\n",
    "search_result = gp_minimize(func=fitness2,\n",
    "                            dimensions=dimensions2,\n",
    "                            acq_func='EI', # Expected Improvement.\n",
    "                            n_calls=100,\n",
    "                            x0=default_parameters2,\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bfb3a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9d9066d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 172, 172, 1)      4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 85, 85, 32)        320       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 42, 42, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 21, 21, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 21, 21, 32)        0         \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 21, 21, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 19, 19, 64)        18496     \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 17, 17, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 8, 8, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 8, 8, 64)          0         \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 8, 8, 64)         256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 6, 6, 128)         73856     \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 4, 4, 128)         147584    \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 2, 2, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 2, 2, 128)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 512)               0         \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 512)              2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense (Dense)               (None, 136)               69768     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 358,636\n",
      "Trainable params: 357,418\n",
      "Non-trainable params: 1,218\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "2000/2000 [==============================] - 12s 5ms/step - loss: 914.3050 - mae: 14.7384 - val_loss: 35.0090 - val_mae: 4.3033 - lr: 0.0010\n",
      "Epoch 2/128\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 35.6158 - mae: 4.3607 - val_loss: 33.1011 - val_mae: 4.0999 - lr: 0.0010\n",
      "Epoch 3/128\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 33.5382 - mae: 4.1884 - val_loss: 31.9366 - val_mae: 4.0620 - lr: 0.0010\n",
      "Epoch 4/128\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 32.6000 - mae: 4.1080 - val_loss: 32.3788 - val_mae: 4.0953 - lr: 0.0010\n",
      "Epoch 5/128\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 32.2241 - mae: 4.0768 - val_loss: 31.5295 - val_mae: 4.0182 - lr: 0.0010\n",
      "Epoch 6/128\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 32.1218 - mae: 4.0702 - val_loss: 32.5149 - val_mae: 4.0567 - lr: 0.0010\n",
      "Epoch 7/128\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 32.0964 - mae: 4.0714 - val_loss: 31.6051 - val_mae: 4.0330 - lr: 0.0010\n",
      "Epoch 8/128\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 32.1357 - mae: 4.0717 - val_loss: 31.5027 - val_mae: 4.0229 - lr: 0.0010\n",
      "Epoch 9/128\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 32.1570 - mae: 4.0718 - val_loss: 31.7689 - val_mae: 4.0616 - lr: 0.0010\n",
      "Epoch 10/128\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 32.1027 - mae: 4.0710 - val_loss: 31.4926 - val_mae: 4.0282 - lr: 0.0010\n",
      "Epoch 11/128\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 32.1327 - mae: 4.0711 - val_loss: 31.5857 - val_mae: 4.0301 - lr: 0.0010\n",
      "Epoch 12/128\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 32.1192 - mae: 4.0709 - val_loss: 31.8490 - val_mae: 4.0258 - lr: 0.0010\n",
      "Epoch 13/128\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 32.0787 - mae: 4.0667 - val_loss: 32.1948 - val_mae: 4.0963 - lr: 0.0010\n",
      "Epoch 14/128\n",
      "2000/2000 [==============================] - 10s 5ms/step - loss: 32.1130 - mae: 4.0693 - val_loss: 31.5741 - val_mae: 4.0256 - lr: 0.0010\n",
      "Epoch 15/128\n",
      "2000/2000 [==============================] - 11s 5ms/step - loss: 32.1357 - mae: 4.0713 - val_loss: 31.7640 - val_mae: 4.0393 - lr: 0.0010\n",
      "Epoch 16/128\n",
      "1700/2000 [========================>.....] - ETA: 1s - loss: 32.1037 - mae: 4.0664"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1620\\580277792.py\u001b[0m in \u001b[0;36m<cell line: 45>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m model.fit(x=X_train,\n\u001b[0m\u001b[0;32m     46\u001b[0m                 \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m                 \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1382\u001b[0m                 _r=1):\n\u001b[0;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1384\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1385\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2954\u001b[0m       (graph_function,\n\u001b[0;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2958\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1852\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1854\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "clear_session()\n",
    "# model_final=tf.keras.models.load_model('./best_model/')\n",
    "# model_final.summary()\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Convolutional layers\n",
    "model.add(BatchNormalization(input_shape=(172, 172, 1)))\n",
    "model.add(Conv2D(filters=32, kernel_size=(3,3),strides=2, activation='relu'))\n",
    "model.add(Conv2D(filters=32, kernel_size=(3,3),strides=2, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters=64, kernel_size=(3,3), strides=1,activation='relu'))\n",
    "model.add(Conv2D(filters=64, kernel_size=(3,3), strides=1,activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters=128, kernel_size=(3,3),strides=1,activation='relu'))\n",
    "model.add(Conv2D(filters=128, kernel_size=(3,3),strides=1,activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Conv2D(filters=256, kernel_size=(3,3),strides=1, activation='relu'))\n",
    "# model.add(Conv2D(filters=256, kernel_size=(3,3),strides=1, activation='relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Flatten layer\n",
    "model.add(Flatten())\n",
    "\n",
    "# Dense layers for regression\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(units=136))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae'])\n",
    "model.summary()\n",
    "\n",
    "model.fit(x=X_train,\n",
    "                y=y_train,\n",
    "                epochs=128,\n",
    "                batch_size=4,\n",
    "                validation_data=(X_test,y_test),\n",
    "                callbacks=[callback,reduce_lr]\n",
    "                           )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a45a4463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/256\n",
      "1998/2000 [============================>.] - ETA: 0s - loss: 31.9446 - mae: 4.0560"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1620\\296991305.py\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# model=tf.keras.models.load_model(r\"D:\\Desktop\\cours\\3I\\DetectionVisage\\model_best_loss.h5\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m model.fit(x=X_train,\n\u001b[0m\u001b[0;32m      3\u001b[0m                 \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                 \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                 \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    100\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m   \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
     ]
    }
   ],
   "source": [
    "# model=tf.keras.models.load_model(r\"D:\\Desktop\\cours\\3I\\DetectionVisage\\model_best_loss.h5\")\n",
    "model.fit(x=X_train,\n",
    "                y=y_train,\n",
    "                epochs=256,\n",
    "                batch_size=4,\n",
    "                validation_data=(X_test,y_test),\n",
    "                callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "85c29255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 12384x12384 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# model_final=tf.keras.models.load_model(r\"D:\\Desktop\\cours\\3I\\DetectionVisage\\model_best_loss.h5\")\n",
    "fig = plt.figure(figsize=( 172 , 172 ))\n",
    "for i in range( 1 , 6 ):\n",
    "    sample_image = X_test[i]\n",
    "    landmarks=model_final.predict(X_test)[i][0][0]\n",
    "    fig.add_subplot( 1 , 10 , i )\n",
    "    plt.imshow( sample_image , cmap='gray' )\n",
    "    for j in range(0,136,2):\n",
    "        plt.scatter( landmarks[j],landmarks[j+1],s=100, c='red' )\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2fb3e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 172, 172, 1)      4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 84, 84, 256)       6656      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 40, 40, 256)       1638656   \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 40, 40, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 36, 36, 128)       819328    \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 32, 32, 128)       409728    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 32, 32, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 28, 28, 128)       409728    \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 24, 24, 128)       409728    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 24, 24, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 20, 20, 64)        204864    \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 16, 16, 64)        102464    \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 16, 16, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 14, 14, 32)        18464     \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 12, 12, 32)        9248      \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 12, 12, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 10, 10, 64)        18496     \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 8, 8, 128)         73856     \n",
      "                                                                 \n",
      " conv2d_12 (Conv2D)          (None, 6, 6, 136)         156808    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,280,460\n",
      "Trainable params: 4,279,242\n",
      "Non-trainable params: 1,218\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chene\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/128\n",
      "2000/2000 [==============================] - 74s 33ms/step - loss: 316.7341 - mse: 316.7334 - val_loss: 34.3581 - val_mse: 34.3581 - lr: 1.0000e-04\n",
      "Epoch 2/128\n",
      "2000/2000 [==============================] - 66s 33ms/step - loss: 31.7511 - mse: 31.7511 - val_loss: 33.6153 - val_mse: 33.6153 - lr: 1.0000e-04\n",
      "Epoch 3/128\n",
      "2000/2000 [==============================] - 66s 33ms/step - loss: 25.4541 - mse: 25.4541 - val_loss: 29.8150 - val_mse: 29.8150 - lr: 1.0000e-04\n",
      "Epoch 4/128\n",
      "2000/2000 [==============================] - 65s 33ms/step - loss: 22.3132 - mse: 22.3132 - val_loss: 26.8108 - val_mse: 26.8109 - lr: 1.0000e-04\n",
      "Epoch 5/128\n",
      "2000/2000 [==============================] - 65s 33ms/step - loss: 20.6229 - mse: 20.6229 - val_loss: 20.0250 - val_mse: 20.0250 - lr: 1.0000e-04\n",
      "Epoch 6/128\n",
      "2000/2000 [==============================] - 65s 33ms/step - loss: 17.1053 - mse: 17.1054 - val_loss: 13.4015 - val_mse: 13.4015 - lr: 1.0000e-04\n",
      "Epoch 7/128\n",
      "2000/2000 [==============================] - 65s 33ms/step - loss: 10.7732 - mse: 10.7732 - val_loss: 10.3898 - val_mse: 10.3898 - lr: 1.0000e-04\n",
      "Epoch 8/128\n",
      "2000/2000 [==============================] - 65s 33ms/step - loss: 8.6357 - mse: 8.6357 - val_loss: 8.3497 - val_mse: 8.3497 - lr: 1.0000e-04\n",
      "Epoch 9/128\n",
      "2000/2000 [==============================] - 65s 33ms/step - loss: 7.9266 - mse: 7.9266 - val_loss: 7.7332 - val_mse: 7.7332 - lr: 1.0000e-04\n",
      "Epoch 10/128\n",
      "2000/2000 [==============================] - 65s 33ms/step - loss: 6.9991 - mse: 6.9991 - val_loss: 8.3231 - val_mse: 8.3231 - lr: 1.0000e-04\n",
      "Epoch 11/128\n",
      "2000/2000 [==============================] - 65s 33ms/step - loss: 6.2362 - mse: 6.2362 - val_loss: 7.0795 - val_mse: 7.0795 - lr: 1.0000e-04\n",
      "Epoch 12/128\n",
      "2000/2000 [==============================] - 65s 33ms/step - loss: 5.4905 - mse: 5.4905 - val_loss: 6.2715 - val_mse: 6.2715 - lr: 1.0000e-04\n",
      "Epoch 13/128\n",
      "2000/2000 [==============================] - 65s 33ms/step - loss: 4.8811 - mse: 4.8811 - val_loss: 6.8312 - val_mse: 6.8312 - lr: 1.0000e-04\n",
      "Epoch 14/128\n",
      "2000/2000 [==============================] - 65s 33ms/step - loss: 4.5600 - mse: 4.5600 - val_loss: 4.8761 - val_mse: 4.8761 - lr: 1.0000e-04\n",
      "Epoch 15/128\n",
      "2000/2000 [==============================] - 65s 33ms/step - loss: 3.9171 - mse: 3.9171 - val_loss: 5.5401 - val_mse: 5.5401 - lr: 1.0000e-04\n",
      "Epoch 16/128\n",
      "2000/2000 [==============================] - 65s 33ms/step - loss: 4.1682 - mse: 4.1682 - val_loss: 5.0002 - val_mse: 5.0002 - lr: 1.0000e-04\n",
      "Epoch 17/128\n",
      "2000/2000 [==============================] - 65s 33ms/step - loss: 3.4230 - mse: 3.4230 - val_loss: 4.6204 - val_mse: 4.6204 - lr: 1.0000e-04\n",
      "Epoch 18/128\n",
      "2000/2000 [==============================] - 65s 33ms/step - loss: 3.2436 - mse: 3.2436 - val_loss: 3.8629 - val_mse: 3.8629 - lr: 1.0000e-04\n",
      "Epoch 19/128\n",
      "2000/2000 [==============================] - 65s 33ms/step - loss: 3.1239 - mse: 3.1239 - val_loss: 3.6255 - val_mse: 3.6255 - lr: 1.0000e-04\n",
      "Epoch 20/128\n",
      "2000/2000 [==============================] - 65s 33ms/step - loss: 3.0565 - mse: 3.0565 - val_loss: 3.7027 - val_mse: 3.7027 - lr: 1.0000e-04\n",
      "Epoch 21/128\n",
      "2000/2000 [==============================] - 65s 33ms/step - loss: 2.8620 - mse: 2.8620 - val_loss: 3.5246 - val_mse: 3.5246 - lr: 1.0000e-04\n",
      "Epoch 22/128\n",
      "2000/2000 [==============================] - 65s 33ms/step - loss: 2.6923 - mse: 2.6923 - val_loss: 3.3519 - val_mse: 3.3519 - lr: 1.0000e-04\n",
      "Epoch 23/128\n",
      "2000/2000 [==============================] - 65s 33ms/step - loss: 2.6425 - mse: 2.6425 - val_loss: 3.2664 - val_mse: 3.2664 - lr: 1.0000e-04\n",
      "Epoch 24/128\n",
      "2000/2000 [==============================] - 65s 33ms/step - loss: 2.4723 - mse: 2.4723 - val_loss: 3.1592 - val_mse: 3.1592 - lr: 1.0000e-04\n",
      "Epoch 25/128\n",
      "2000/2000 [==============================] - 65s 33ms/step - loss: 2.3356 - mse: 2.3356 - val_loss: 2.8963 - val_mse: 2.8963 - lr: 1.0000e-04\n",
      "Epoch 26/128\n",
      "2000/2000 [==============================] - 65s 33ms/step - loss: 2.1993 - mse: 2.1993 - val_loss: 3.0013 - val_mse: 3.0013 - lr: 1.0000e-04\n",
      "Epoch 27/128\n",
      "2000/2000 [==============================] - 65s 33ms/step - loss: 2.1126 - mse: 2.1126 - val_loss: 2.9032 - val_mse: 2.9032 - lr: 1.0000e-04\n",
      "Epoch 28/128\n",
      "2000/2000 [==============================] - 67s 34ms/step - loss: 2.0651 - mse: 2.0651 - val_loss: 2.8430 - val_mse: 2.8430 - lr: 1.0000e-04\n",
      "Epoch 29/128\n",
      "2000/2000 [==============================] - 68s 34ms/step - loss: 1.9871 - mse: 1.9870 - val_loss: 3.0136 - val_mse: 3.0136 - lr: 1.0000e-04\n",
      "Epoch 30/128\n",
      "2000/2000 [==============================] - 66s 33ms/step - loss: 1.9235 - mse: 1.9235 - val_loss: 3.1328 - val_mse: 3.1328 - lr: 1.0000e-04\n",
      "Epoch 31/128\n",
      "2000/2000 [==============================] - 66s 33ms/step - loss: 1.8525 - mse: 1.8525 - val_loss: 2.7360 - val_mse: 2.7360 - lr: 1.0000e-04\n",
      "Epoch 32/128\n",
      "2000/2000 [==============================] - 66s 33ms/step - loss: 1.8479 - mse: 1.8479 - val_loss: 2.6347 - val_mse: 2.6347 - lr: 1.0000e-04\n",
      "Epoch 33/128\n",
      "2000/2000 [==============================] - 66s 33ms/step - loss: 1.7831 - mse: 1.7831 - val_loss: 2.6908 - val_mse: 2.6908 - lr: 1.0000e-04\n",
      "Epoch 34/128\n",
      "2000/2000 [==============================] - 66s 33ms/step - loss: 1.7376 - mse: 1.7376 - val_loss: 2.7696 - val_mse: 2.7696 - lr: 1.0000e-04\n",
      "Epoch 35/128\n",
      "2000/2000 [==============================] - 66s 33ms/step - loss: 1.6845 - mse: 1.6845 - val_loss: 2.5536 - val_mse: 2.5536 - lr: 1.0000e-04\n",
      "Epoch 36/128\n",
      "2000/2000 [==============================] - 66s 33ms/step - loss: 1.6566 - mse: 1.6566 - val_loss: 2.4806 - val_mse: 2.4806 - lr: 1.0000e-04\n",
      "Epoch 37/128\n",
      "2000/2000 [==============================] - 66s 33ms/step - loss: 1.6158 - mse: 1.6158 - val_loss: 2.4440 - val_mse: 2.4440 - lr: 1.0000e-04\n",
      "Epoch 38/128\n",
      "2000/2000 [==============================] - 67s 34ms/step - loss: 1.5810 - mse: 1.5810 - val_loss: 2.3549 - val_mse: 2.3549 - lr: 1.0000e-04\n",
      "Epoch 39/128\n",
      "2000/2000 [==============================] - 68s 34ms/step - loss: 1.5283 - mse: 1.5283 - val_loss: 2.4591 - val_mse: 2.4591 - lr: 1.0000e-04\n",
      "Epoch 40/128\n",
      "2000/2000 [==============================] - 68s 34ms/step - loss: 1.4765 - mse: 1.4765 - val_loss: 2.4022 - val_mse: 2.4022 - lr: 1.0000e-04\n",
      "Epoch 41/128\n",
      "2000/2000 [==============================] - 68s 34ms/step - loss: 1.4458 - mse: 1.4458 - val_loss: 2.4053 - val_mse: 2.4053 - lr: 1.0000e-04\n",
      "Epoch 42/128\n",
      "2000/2000 [==============================] - 68s 34ms/step - loss: 1.4270 - mse: 1.4270 - val_loss: 2.3820 - val_mse: 2.3820 - lr: 1.0000e-04\n",
      "Epoch 43/128\n",
      "2000/2000 [==============================] - 68s 34ms/step - loss: 1.3784 - mse: 1.3784 - val_loss: 2.1285 - val_mse: 2.1285 - lr: 1.0000e-04\n",
      "Epoch 44/128\n",
      "2000/2000 [==============================] - 68s 34ms/step - loss: 1.3223 - mse: 1.3223 - val_loss: 2.2309 - val_mse: 2.2309 - lr: 1.0000e-04\n",
      "Epoch 45/128\n",
      "2000/2000 [==============================] - 68s 34ms/step - loss: 1.3202 - mse: 1.3202 - val_loss: 2.3875 - val_mse: 2.3875 - lr: 1.0000e-04\n",
      "Epoch 46/128\n",
      "2000/2000 [==============================] - 68s 34ms/step - loss: 1.2822 - mse: 1.2822 - val_loss: 2.2789 - val_mse: 2.2789 - lr: 1.0000e-04\n",
      "Epoch 47/128\n",
      "2000/2000 [==============================] - 68s 34ms/step - loss: 1.2579 - mse: 1.2579 - val_loss: 2.0713 - val_mse: 2.0713 - lr: 1.0000e-04\n",
      "Epoch 48/128\n",
      "2000/2000 [==============================] - 68s 34ms/step - loss: 1.2307 - mse: 1.2307 - val_loss: 2.0411 - val_mse: 2.0411 - lr: 1.0000e-04\n",
      "Epoch 49/128\n",
      "2000/2000 [==============================] - 68s 34ms/step - loss: 1.2176 - mse: 1.2176 - val_loss: 2.1994 - val_mse: 2.1994 - lr: 1.0000e-04\n",
      "Epoch 50/128\n",
      "2000/2000 [==============================] - 68s 34ms/step - loss: 1.2017 - mse: 1.2017 - val_loss: 2.2988 - val_mse: 2.2988 - lr: 1.0000e-04\n",
      "Epoch 51/128\n",
      "2000/2000 [==============================] - 68s 34ms/step - loss: 1.1841 - mse: 1.1841 - val_loss: 2.2031 - val_mse: 2.2031 - lr: 1.0000e-04\n",
      "Epoch 52/128\n",
      "2000/2000 [==============================] - 68s 34ms/step - loss: 1.1492 - mse: 1.1492 - val_loss: 2.0033 - val_mse: 2.0033 - lr: 1.0000e-04\n",
      "Epoch 53/128\n",
      "2000/2000 [==============================] - 68s 34ms/step - loss: 1.1451 - mse: 1.1451 - val_loss: 1.9562 - val_mse: 1.9562 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/128\n",
      "2000/2000 [==============================] - 67s 33ms/step - loss: 1.1327 - mse: 1.1327 - val_loss: 1.9648 - val_mse: 1.9648 - lr: 1.0000e-04\n",
      "Epoch 55/128\n",
      "2000/2000 [==============================] - 66s 33ms/step - loss: 1.1311 - mse: 1.1311 - val_loss: 2.1244 - val_mse: 2.1244 - lr: 1.0000e-04\n",
      "Epoch 56/128\n",
      "2000/2000 [==============================] - 67s 33ms/step - loss: 1.1158 - mse: 1.1158 - val_loss: 2.0807 - val_mse: 2.0807 - lr: 1.0000e-04\n",
      "Epoch 57/128\n",
      "2000/2000 [==============================] - 66s 33ms/step - loss: 1.0966 - mse: 1.0966 - val_loss: 2.0668 - val_mse: 2.0668 - lr: 1.0000e-04\n",
      "Epoch 58/128\n",
      "2000/2000 [==============================] - 66s 33ms/step - loss: 1.0850 - mse: 1.0850 - val_loss: 2.1187 - val_mse: 2.1187 - lr: 1.0000e-04\n",
      "Epoch 59/128\n",
      "2000/2000 [==============================] - 66s 33ms/step - loss: 1.0606 - mse: 1.0606 - val_loss: 2.0770 - val_mse: 2.0770 - lr: 1.0000e-04\n",
      "Epoch 60/128\n",
      "2000/2000 [==============================] - 66s 33ms/step - loss: 1.0525 - mse: 1.0525 - val_loss: 2.0234 - val_mse: 2.0234 - lr: 1.0000e-04\n",
      "Epoch 61/128\n",
      "2000/2000 [==============================] - 66s 33ms/step - loss: 1.0501 - mse: 1.0501 - val_loss: 1.9854 - val_mse: 1.9854 - lr: 1.0000e-04\n",
      "Epoch 62/128\n",
      "2000/2000 [==============================] - 66s 33ms/step - loss: 1.0438 - mse: 1.0438 - val_loss: 2.1065 - val_mse: 2.1065 - lr: 1.0000e-04\n",
      "Epoch 63/128\n",
      "2000/2000 [==============================] - 66s 33ms/step - loss: 1.0292 - mse: 1.0292 - val_loss: 1.9708 - val_mse: 1.9708 - lr: 1.0000e-04\n",
      "Epoch 64/128\n",
      "2000/2000 [==============================] - 66s 33ms/step - loss: 1.0135 - mse: 1.0135 - val_loss: 1.9821 - val_mse: 1.9821 - lr: 1.0000e-04\n",
      "Epoch 65/128\n",
      "2000/2000 [==============================] - 66s 33ms/step - loss: 1.0143 - mse: 1.0143 - val_loss: 1.9496 - val_mse: 1.9496 - lr: 1.0000e-04\n",
      "Epoch 66/128\n",
      "2000/2000 [==============================] - 66s 33ms/step - loss: 1.0002 - mse: 1.0001 - val_loss: 1.9492 - val_mse: 1.9492 - lr: 1.0000e-04\n",
      "Epoch 67/128\n",
      "2000/2000 [==============================] - 66s 33ms/step - loss: 1.0219 - mse: 1.0219 - val_loss: 1.9732 - val_mse: 1.9732 - lr: 1.0000e-04\n",
      "Epoch 68/128\n",
      "2000/2000 [==============================] - 66s 33ms/step - loss: 0.9985 - mse: 0.9985 - val_loss: 2.5071 - val_mse: 2.5071 - lr: 1.0000e-04\n",
      "Epoch 69/128\n",
      "2000/2000 [==============================] - 66s 33ms/step - loss: 0.9829 - mse: 0.9829 - val_loss: 1.8910 - val_mse: 1.8910 - lr: 1.0000e-04\n",
      "Epoch 70/128\n",
      "2000/2000 [==============================] - 66s 33ms/step - loss: 0.9746 - mse: 0.9746 - val_loss: 1.9830 - val_mse: 1.9830 - lr: 1.0000e-04\n",
      "Epoch 71/128\n",
      "2000/2000 [==============================] - 66s 33ms/step - loss: 0.9601 - mse: 0.9601 - val_loss: 2.1419 - val_mse: 2.1419 - lr: 1.0000e-04\n",
      "Epoch 72/128\n",
      "2000/2000 [==============================] - 66s 33ms/step - loss: 0.9605 - mse: 0.9605 - val_loss: 1.9571 - val_mse: 1.9571 - lr: 1.0000e-04\n",
      "Epoch 73/128\n",
      "2000/2000 [==============================] - 66s 33ms/step - loss: 0.9471 - mse: 0.9471 - val_loss: 1.9466 - val_mse: 1.9466 - lr: 1.0000e-04\n",
      "Epoch 74/128\n",
      "2000/2000 [==============================] - 66s 33ms/step - loss: 0.9443 - mse: 0.9443 - val_loss: 1.8742 - val_mse: 1.8742 - lr: 1.0000e-04\n",
      "Epoch 75/128\n",
      "2000/2000 [==============================] - 66s 33ms/step - loss: 0.9366 - mse: 0.9366 - val_loss: 1.8911 - val_mse: 1.8911 - lr: 1.0000e-04\n",
      "Epoch 76/128\n",
      "2000/2000 [==============================] - 68s 34ms/step - loss: 0.9299 - mse: 0.9299 - val_loss: 1.8029 - val_mse: 1.8029 - lr: 1.0000e-04\n",
      "Epoch 77/128\n",
      "2000/2000 [==============================] - 68s 34ms/step - loss: 0.9102 - mse: 0.9102 - val_loss: 1.8423 - val_mse: 1.8423 - lr: 1.0000e-04\n",
      "Epoch 78/128\n",
      "2000/2000 [==============================] - 68s 34ms/step - loss: 0.9179 - mse: 0.9179 - val_loss: 1.8814 - val_mse: 1.8814 - lr: 1.0000e-04\n",
      "Epoch 79/128\n",
      "2000/2000 [==============================] - 68s 34ms/step - loss: 0.8987 - mse: 0.8987 - val_loss: 1.8145 - val_mse: 1.8145 - lr: 1.0000e-04\n",
      "Epoch 80/128\n",
      "2000/2000 [==============================] - 68s 34ms/step - loss: 0.8923 - mse: 0.8923 - val_loss: 1.8467 - val_mse: 1.8467 - lr: 1.0000e-04\n",
      "Epoch 81/128\n",
      "2000/2000 [==============================] - 68s 34ms/step - loss: 0.8840 - mse: 0.8840 - val_loss: 1.7870 - val_mse: 1.7870 - lr: 1.0000e-04\n",
      "Epoch 82/128\n",
      "2000/2000 [==============================] - 68s 34ms/step - loss: 0.8843 - mse: 0.8843 - val_loss: 1.7970 - val_mse: 1.7970 - lr: 1.0000e-04\n",
      "Epoch 83/128\n",
      "2000/2000 [==============================] - 68s 34ms/step - loss: 0.8658 - mse: 0.8658 - val_loss: 2.0647 - val_mse: 2.0647 - lr: 1.0000e-04\n",
      "Epoch 84/128\n",
      "2000/2000 [==============================] - 68s 34ms/step - loss: 0.8526 - mse: 0.8526 - val_loss: 2.0039 - val_mse: 2.0039 - lr: 1.0000e-04\n",
      "Epoch 85/128\n",
      "2000/2000 [==============================] - 75s 38ms/step - loss: 0.8480 - mse: 0.8480 - val_loss: 1.8705 - val_mse: 1.8705 - lr: 1.0000e-04\n",
      "Epoch 86/128\n",
      "2000/2000 [==============================] - 93s 47ms/step - loss: 0.8291 - mse: 0.8291 - val_loss: 2.1704 - val_mse: 2.1704 - lr: 1.0000e-04\n",
      "Epoch 87/128\n",
      "2000/2000 [==============================] - 93s 47ms/step - loss: 0.8305 - mse: 0.8305 - val_loss: 1.9294 - val_mse: 1.9294 - lr: 1.0000e-04\n",
      "Epoch 88/128\n",
      "2000/2000 [==============================] - 93s 47ms/step - loss: 0.8273 - mse: 0.8273 - val_loss: 1.8392 - val_mse: 1.8392 - lr: 1.0000e-04\n",
      "Epoch 89/128\n",
      "2000/2000 [==============================] - 93s 47ms/step - loss: 0.8257 - mse: 0.8257 - val_loss: 1.8127 - val_mse: 1.8127 - lr: 1.0000e-04\n",
      "Epoch 90/128\n",
      "2000/2000 [==============================] - 93s 47ms/step - loss: 0.8215 - mse: 0.8215 - val_loss: 1.7897 - val_mse: 1.7897 - lr: 1.0000e-04\n",
      "Epoch 91/128\n",
      "2000/2000 [==============================] - 93s 47ms/step - loss: 0.8039 - mse: 0.8039 - val_loss: 1.9026 - val_mse: 1.9026 - lr: 1.0000e-04\n",
      "Epoch 92/128\n",
      "2000/2000 [==============================] - 93s 47ms/step - loss: 0.7959 - mse: 0.7959 - val_loss: 1.8203 - val_mse: 1.8203 - lr: 1.0000e-04\n",
      "Epoch 93/128\n",
      "2000/2000 [==============================] - 93s 47ms/step - loss: 0.7854 - mse: 0.7854 - val_loss: 1.8074 - val_mse: 1.8074 - lr: 1.0000e-04\n",
      "Epoch 94/128\n",
      "2000/2000 [==============================] - 93s 47ms/step - loss: 0.8025 - mse: 0.8025 - val_loss: 1.9055 - val_mse: 1.9055 - lr: 1.0000e-04\n",
      "Epoch 95/128\n",
      "2000/2000 [==============================] - 93s 47ms/step - loss: 0.7812 - mse: 0.7812 - val_loss: 1.8877 - val_mse: 1.8877 - lr: 1.0000e-04\n",
      "Epoch 96/128\n",
      "2000/2000 [==============================] - 93s 47ms/step - loss: 0.7829 - mse: 0.7829 - val_loss: 1.8570 - val_mse: 1.8570 - lr: 1.0000e-04\n",
      "Epoch 97/128\n",
      "2000/2000 [==============================] - 93s 47ms/step - loss: 0.7593 - mse: 0.7593 - val_loss: 1.7523 - val_mse: 1.7523 - lr: 1.0000e-04\n",
      "Epoch 98/128\n",
      "2000/2000 [==============================] - 93s 47ms/step - loss: 0.7646 - mse: 0.7646 - val_loss: 1.8287 - val_mse: 1.8287 - lr: 1.0000e-04\n",
      "Epoch 99/128\n",
      "2000/2000 [==============================] - 93s 47ms/step - loss: 0.7648 - mse: 0.7648 - val_loss: 1.7989 - val_mse: 1.7989 - lr: 1.0000e-04\n",
      "Epoch 100/128\n",
      "2000/2000 [==============================] - 93s 47ms/step - loss: 0.7642 - mse: 0.7642 - val_loss: 1.8555 - val_mse: 1.8555 - lr: 1.0000e-04\n",
      "Epoch 101/128\n",
      "2000/2000 [==============================] - 93s 47ms/step - loss: 0.7632 - mse: 0.7632 - val_loss: 1.8569 - val_mse: 1.8569 - lr: 1.0000e-04\n",
      "Epoch 102/128\n",
      "2000/2000 [==============================] - 93s 47ms/step - loss: 0.7435 - mse: 0.7435 - val_loss: 1.8576 - val_mse: 1.8576 - lr: 1.0000e-04\n",
      "Epoch 103/128\n",
      "2000/2000 [==============================] - 93s 47ms/step - loss: 0.7374 - mse: 0.7374 - val_loss: 1.7696 - val_mse: 1.7696 - lr: 1.0000e-04\n",
      "Epoch 104/128\n",
      "2000/2000 [==============================] - 93s 47ms/step - loss: 0.7355 - mse: 0.7355 - val_loss: 1.8855 - val_mse: 1.8855 - lr: 1.0000e-04\n",
      "Epoch 105/128\n",
      "2000/2000 [==============================] - 93s 47ms/step - loss: 0.7269 - mse: 0.7269 - val_loss: 1.8185 - val_mse: 1.8185 - lr: 1.0000e-04\n",
      "Epoch 106/128\n",
      "2000/2000 [==============================] - 93s 47ms/step - loss: 0.7239 - mse: 0.7239 - val_loss: 1.9615 - val_mse: 1.9615 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107/128\n",
      "2000/2000 [==============================] - 93s 47ms/step - loss: 0.7208 - mse: 0.7208 - val_loss: 1.8496 - val_mse: 1.8496 - lr: 1.0000e-04\n",
      "Epoch 108/128\n",
      "2000/2000 [==============================] - 93s 47ms/step - loss: 0.7178 - mse: 0.7178 - val_loss: 1.7413 - val_mse: 1.7413 - lr: 1.0000e-04\n",
      "Epoch 109/128\n",
      "2000/2000 [==============================] - 93s 47ms/step - loss: 0.7100 - mse: 0.7100 - val_loss: 1.7832 - val_mse: 1.7832 - lr: 1.0000e-04\n",
      "Epoch 110/128\n",
      "2000/2000 [==============================] - 93s 47ms/step - loss: 0.6987 - mse: 0.6987 - val_loss: 1.7963 - val_mse: 1.7963 - lr: 1.0000e-04\n",
      "Epoch 111/128\n",
      "2000/2000 [==============================] - 93s 47ms/step - loss: 0.7211 - mse: 0.7211 - val_loss: 1.8866 - val_mse: 1.8866 - lr: 1.0000e-04\n",
      "Epoch 112/128\n",
      "2000/2000 [==============================] - 93s 47ms/step - loss: 0.7070 - mse: 0.7070 - val_loss: 1.7641 - val_mse: 1.7641 - lr: 1.0000e-04\n",
      "Epoch 113/128\n",
      "2000/2000 [==============================] - 93s 47ms/step - loss: 0.6953 - mse: 0.6953 - val_loss: 1.8055 - val_mse: 1.8055 - lr: 1.0000e-04\n",
      "Epoch 114/128\n",
      "2000/2000 [==============================] - 90s 45ms/step - loss: 0.6915 - mse: 0.6915 - val_loss: 1.8431 - val_mse: 1.8431 - lr: 1.0000e-04\n",
      "Epoch 115/128\n",
      "2000/2000 [==============================] - 88s 44ms/step - loss: 0.6965 - mse: 0.6965 - val_loss: 1.7609 - val_mse: 1.7609 - lr: 1.0000e-04\n",
      "Epoch 116/128\n",
      "2000/2000 [==============================] - 93s 47ms/step - loss: 0.6759 - mse: 0.6759 - val_loss: 1.9058 - val_mse: 1.9058 - lr: 1.0000e-04\n",
      "Epoch 117/128\n",
      "2000/2000 [==============================] - 93s 47ms/step - loss: 0.6798 - mse: 0.6798 - val_loss: 1.7888 - val_mse: 1.7888 - lr: 1.0000e-04\n",
      "Epoch 118/128\n",
      "2000/2000 [==============================] - 93s 47ms/step - loss: 0.6729 - mse: 0.6729 - val_loss: 1.8922 - val_mse: 1.8922 - lr: 1.0000e-04\n",
      "Epoch 119/128\n",
      "2000/2000 [==============================] - 93s 47ms/step - loss: 0.6723 - mse: 0.6723 - val_loss: 1.9022 - val_mse: 1.9022 - lr: 1.0000e-04\n",
      "Epoch 120/128\n",
      "2000/2000 [==============================] - 93s 47ms/step - loss: 0.6694 - mse: 0.6694 - val_loss: 1.8130 - val_mse: 1.8130 - lr: 1.0000e-04\n",
      "Epoch 121/128\n",
      "2000/2000 [==============================] - 93s 47ms/step - loss: 0.6686 - mse: 0.6686 - val_loss: 1.7633 - val_mse: 1.7633 - lr: 1.0000e-04\n",
      "Epoch 122/128\n",
      "2000/2000 [==============================] - 93s 47ms/step - loss: 0.6582 - mse: 0.6582 - val_loss: 1.7934 - val_mse: 1.7934 - lr: 1.0000e-04\n",
      "Epoch 123/128\n",
      "2000/2000 [==============================] - 93s 47ms/step - loss: 0.6483 - mse: 0.6483 - val_loss: 1.7864 - val_mse: 1.7864 - lr: 1.0000e-04\n",
      "Epoch 124/128\n",
      "2000/2000 [==============================] - 93s 47ms/step - loss: 0.6448 - mse: 0.6448 - val_loss: 1.8464 - val_mse: 1.8464 - lr: 1.0000e-04\n",
      "Epoch 125/128\n",
      "2000/2000 [==============================] - 93s 47ms/step - loss: 0.6644 - mse: 0.6644 - val_loss: 1.7748 - val_mse: 1.7748 - lr: 1.0000e-04\n",
      "Epoch 126/128\n",
      "2000/2000 [==============================] - 93s 47ms/step - loss: 0.6484 - mse: 0.6484 - val_loss: 1.7421 - val_mse: 1.7421 - lr: 1.0000e-04\n",
      "Epoch 127/128\n",
      "2000/2000 [==============================] - 93s 47ms/step - loss: 0.6415 - mse: 0.6415 - val_loss: 1.7567 - val_mse: 1.7567 - lr: 1.0000e-04\n",
      "Epoch 128/128\n",
      "2000/2000 [==============================] - 93s 47ms/step - loss: 0.6445 - mse: 0.6445 - val_loss: 2.1311 - val_mse: 2.1311 - lr: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "model_layers = [\n",
    "    tf.keras.layers.BatchNormalization( input_shape=( 172 , 172 , 1 ) ),\n",
    "    tf.keras.layers.Conv2D( 256 , kernel_size=(5,5 ) , strides=2 , activation='relu' ), \n",
    "    tf.keras.layers.Conv2D( 256 , kernel_size=( 5,5) , strides=2 , activation='relu' ),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "\n",
    "    tf.keras.layers.Conv2D( 128 , kernel_size=(5,5) , strides=1 , activation='relu' ),\n",
    "    tf.keras.layers.Conv2D( 128 , kernel_size=( 5,5 ) , strides=1 , activation='relu' ),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "\n",
    "    tf.keras.layers.Conv2D( 128 , kernel_size=( 5,5 ) , strides=1 , activation='relu' ),\n",
    "    tf.keras.layers.Conv2D( 128 , kernel_size=( 5,5) , strides=1 , activation='relu' ),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "\n",
    "    tf.keras.layers.Conv2D( 64 , kernel_size=( 5,5 ) , strides=1 , activation='relu' ),\n",
    "    tf.keras.layers.Conv2D( 64 , kernel_size=( 5,5 ) , strides=1 , activation='relu' ),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "\n",
    "    tf.keras.layers.Conv2D( 32 , kernel_size=( 3 , 3 ) , strides=1 , activation='relu'),\n",
    "    tf.keras.layers.Conv2D( 32 , kernel_size=( 3 , 3 ) , strides=1 , activation='relu' ),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "\n",
    "    tf.keras.layers.Conv2D( 64 , kernel_size=( 3 , 3 ) , strides=1 , activation='relu', ),\n",
    "    tf.keras.layers.Conv2D( 128 , kernel_size=( 3 , 3 ) , strides=1 , activation='relu', ),\n",
    "    tf.keras.layers.Conv2D( 136 , kernel_size=( 3 , 3 ) , strides=1 ),\n",
    "\n",
    "]\n",
    "model = tf.keras.Sequential( model_layers )\n",
    "model.compile( loss=tf.keras.losses.mean_squared_error , optimizer=tf.keras.optimizers.Adam( lr=0.0001 ) , metrics=[ 'mse' ] )\n",
    "model.summary()\n",
    "history=model.fit(x=X_train,\n",
    "                y=y_train,\n",
    "                epochs=128,\n",
    "                batch_size=4,\n",
    "                validation_data=(X_test,y_test),\n",
    "                callbacks=[callback,reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ec967b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 10s 36ms/step - loss: 0.8263 - mse: 0.8263\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 1.7414 - mse: 1.7414\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.7413865327835083, 1.7413859367370605]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_final=tf.keras.models.load_model(r\"D:\\Desktop\\cours\\3I\\DetectionVisage\\model_best_loss.h5\")\n",
    "model_final.evaluate(X_train,y_train)\n",
    "model_final.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a7d29f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17534 files belonging to 105 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.540114900097251"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=r\"C:\\Users\\Authentification\\PycharmProjects\\ProjectFinalDetectionVisage\\DetectionVisage\\FaceDetectionTheo\\Original Images\\Dataset2\"\n",
    "ds=tf.keras.utils.image_dataset_from_directory(\n",
    "    data,\n",
    "    color_mode='grayscale',\n",
    "    batch_size=4,\n",
    "    image_size=(172, 172),\n",
    ")\n",
    "model_final=tf.keras.models.load_model(r\"D:\\Desktop\\cours\\3I\\DetectionVisage\\model_best_loss.h5\")\n",
    "\n",
    "img=ds.take(1)\n",
    "init=time.perf_counter()\n",
    "for i in range(100):\n",
    "    model_final.predict(img)\n",
    "time.perf_counter()-init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e8df9d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_graph(history):\n",
    "    # summarize history for accuracy\n",
    "#     plt.plot(history['accuracy'])\n",
    "#     plt.plot(history['val_accuracy'])\n",
    "#     plt.title('model accuracy')\n",
    "#     plt.ylabel('accuracy')\n",
    "#     plt.xlabel('epoch')\n",
    "#     plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(history['loss'])\n",
    "    plt.plot(history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.ylim([0,40])\n",
    "    plt.show()\n",
    "plot_graph(history.history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
